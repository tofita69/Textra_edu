{
    "original_text_chunks": [
        "HAL open science Thermodynamics-based Artificial Neural Networks for constitutive modeling Filippo Masi, Ioannis Stefanou, Paolo Vannucci, Victor Maffi-Berthier",
        "To cite this version: Filippo Masi, Ioannis Stefanou, Paolo Vannucci, Victor Maffi-Berthier. Themmodynamic-based Ar- tificial Neural Networks for constitutive modeling. Journal of the Mechanics and Physics of Solids, 2021, 147, pp.104277. 10,06/ApaDTT. hal-03079127 HAL Id: hal-03079127",
        "https//halscience/la-03079127V1 Submitted on 17 Dec 2020 HAL is a multi-disciplinary open access L'archive ouverte pluridisciplinaire HAL, est archive for the deposit and dissemination of sci- destinée au dépôt et à la diffusion de documents",
        "entific research documents, whether they are pub- scientifiques de niveau recherche, publiés ou non, lished or not. The documents may come from émanant des établissements d'enseignement et de teaching and research institutions in France or recherche français ou étrangers, des laboratoires abroad, or from public or private research centers. publics ou privés. Thermodynamics-based Artificial Neural Networks for constitutive",
        "modeling Filippo Masia,b, Ioannis Stefanou**, Paolo Vannuccir, Victor Maffi-Berthierb aInstitut de Recherche en Génie Civil et Mécanique, UMR 6183, CNRS, Ecole Centrale de Nantes, Université de Nantes, 1 rue de la Noe, F-44300, Nantes, France.",
        "bIngérop Conseil et Ingénierie, 18 rue des Deux Gares, F-92500, Rueil-Malmaison, France. PLMV, UMR 8100, Université de Versailles et Saint-Quentin, 55 avenue de Paris, F-78035, Versailles, France. Abstract",
        "Machine Learning methods and, in particular, Artificial Neural Networks (ANNs) have demonstrated promising capabilities in material constitutive modeling. One of the main drawbacks of such approaches is the lack of a rigorous frame based on the laws of physics. This may render physically inconsistent the predictions of a trained network, which can be even dangerous for real applications. Here we propose a new class of data-driven, physics-based, neural networks for constitutive",
        "modeling of strain rate independent processes at the material point level, which we define as Thermodynamic-based Artificial Neural Networks (TANNs). The two basic principles of thermo- dynamics are encoded in the network's architecture by taking advantage of automatic differentiation to compute the numerical derivatives of a network with respect to its inputs. In this way, derivatives of the free-energy, the dissipation rate and their relation with the stress and internal state variables",
        "are hardwired in the architecture of TANNS. Consequently, our approach does not have to identify the underlying pattern of thermodynamic laws during training, reducing the need of large data- sets. Moreover the training is more efficient and robust, and the predictions more accurate. Finally and more important, the predictions remain thermodynamicaly consistent, even for unseen data. Based on these features, TANNS are a starting point for data-driven, physics-based constitutive",
        "modeling with neural networks. We demonstrate the wide applicability of TANNS for modeling elasto-plastic materials, using both hyper- and hypo-plasticity models. Strain hardening and softening are also considered for the hyper-plastic scenario. Detailed comparisons show that the predictions of TANNS outperform those of standard ANNS.",
        "Finally, we demonstrate that the implementation of the laws of thermodynamics confers to TANNS high degrees of robustness to the presence of noise in the training data, compared to standard approaches. TANNS ' architecture is general, enabling applications to materials with different or more complex behavior, without any modification.",
        "Keywords: Data-driven modeling; Machine learning; Artificial neural network; Thermodynamics; Constitutive model. Masi, et al. (2020) preprint, Journal of the Mechanics and Physics of Solids. doi: 101016/mpa202A.0E7 1. Introduction A large spectrum of constitutive models have been proposed in the literature, based on",
        "observations and experimental testing. Existing constitutive laws can account for phenomena taking place at various length scales. This is achieved either through heuristic approaches and assumptions or through asymptotic approximations and averaging (e.g. Lloberas Valls et al., 2019; Nitka et al., 2011; Feyel, 2003; Bakhvalov and Panasenko, 1989). The history and the state of a material is commonly taken into account through ad hoc enrichment of simpler",
        "constitutive laws and extensive calibration. For this purpose, the laws of thermodynamics offer a useful framework for deriving more sophisticated laws, by intrinsically respecting the energy balance and the entropy production requirements (see e.g. Houlsby and Puzrin, 2000; Einav et al., 2007; Houlsby and Puzrin, 2007; Einav, 2012, among others). An important limitation in constitutive modeling is the availability of data at different",
        "time- and length-scales. However, with the increase of computational power, it is nowadays possible to foresee micromechanical simulations that can account for realistic physics and explore stress paths and non-linear phenomena, which are experimentally inaccessible with the current methods. Of course, some constitutive assumptions will be always necessary, but these might be at a smaller scale, where the material properties are measurable and",
        "easier to identify. This scale is for instance the scale of the microstructure of a material (e.g. the scale of sand grains, crystals, alloys' grains, composites' fibers, masonry bricks' etc. including their topological configuration). However, it is likely that the existing constitutive models might not be sufficient for describing complex material behaviors emerging from the microstructure. Therefore, calibration",
        "(parameter fitting) of known constitutive descriptors might be insufficient for representing the full space of material response, provided by sophisticated micromechanical simulations. Moreover, micromechanical simulations have currently a tremendous calculation cost, which is impossible to afford in large-scale, non-linear, incremental simulations (e.g. Finite Elements) that are usually needed in applications (cf. Masi et al., 2020, 2018; Rattez et al., 2018a,b;",
        "Collins-Craft et al., 2020; Lloberas Valls et al., 2019; Nitka et al., 2011; Eijnden et al., 2016; Feyel, 2003). A promising solution to this issue seems to be Machine Learning. According to Geron (2015), \"Machine Learning is the science (and art) of programming computers SO they can learn from data\" \" In the context of computer programming, learning is defined by Mitchell",
        "et al. (1997) as follows: \"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience\". In the frame of constitutive modeling, a Machine Learning program can learn the stress-strain behavior of a material, given examples of stress-strain increments, which are either determined experimentally or through detailed micromechanical",
        "simulations. The data that the system uses to learn are called the training data-set and \"Corresponding author. Email addresses: lippo.masilec-mantes.fr (Filippo Masi), ioannis.stefanougec-nantes.fr (Ioannis Stefanou), paolovammuciduwsg.tr (Paolo Vannucci), victor.mafi-berthiertherdingerop.com (Victor Maffi-Berthier)",
        "2 each training example is called a training instance (or sample). In our case, the task T, for instance, can be the prediction of the stress for a given strain increment and internal state of the material. The experience E is the training data-set and the performance measure P can be the prediction error. Machine Learning is a general term to describe a large spectrum",
        "of numerical methods. Some of them offer very rich interpolation spaces, which, in theory, could be used for approximating complicated functions belonging to uncommon spaces. Here we focus on the method of Artificial Neural Networks (ANNs), which is considered to be a sub-class of Machine Learning methods. According to Cybenko (1989) and Chen and Chen (1995), ANNs have proved to be universal approximators, due to their rich interpolation",
        "space. Therefore, they seem to be a useful and promising tool for approximating constitutive laws of many materials (e.g. sand, masonry, alloys, ceramics, composites etc.). Recognizing this potential, there is an increasing amount of new literature employing ANNS successfully in constitutive modeling of non-linear materials from model identification based on experiments and detailed numerical simulations. Starting form the seminal work",
        "of Ghaboussi et al. (1991) and without being exhaustive, we refer to Ghaboussi and Sidarta (1998); Lefik and Schrefler (2003); Jung and Ghaboussi (2006); Settgast et al. (2019); Liu and Wu (2019); Lu et al. (2019); Xu et al. (2020); Huang et al. (2020); Liu and Wu (2019); Gajek et al. (2020); Gorji et al. (2020) and references therein. The main idea in these works is to appropriately train ANNs, feeding them with material data, and predict the material",
        "response at the material point level. In this sense ANNs can be seen as rich interpolation spaces, able to represent complex material behavior. For instance, we record the works of Heider et al. (2020); Ghavamian and Simone (2019); Mozaffar et al. (2019); Frankel et al. (2019); Gonzalez et al. (2019); Gorji et al. (2020), who demonstrated that Recurrent Neural Networks (RNNs), an extension of neural networks, can be particularly useful for modeling",
        "path-dependent plasticity models. RNNs, differently from ANNS, process time sequences. As suggested by Gorji et al. (2020), the history-dependent variables of RNNs can potentially mimic the role of physical quantities. The Boundary Value Problem (BVP), set to determine the behavior of a solid under mechanical and/or multiphysics couplings, is then solved by replacing the standard constitutive equations",
        "or algorithms by the trained ANN. This replacement is straightforward and non-intrusive in Finite Element (FE) codes. We record, without being exhaustive, the successful embedding of ANNs as material description subroutines in FE codes by Lefik and Schrefler (2003); Jung and Ghaboussi (2006); Lefik et al. (2009); Settgast et al. (2019). Ghavamian and Simone (2019) further implemented ANNs in a FE2 scheme for accelerating multiscale FE",
        "simulations for materials displaying strain softening, with Perzyna viscoplaticity model. It is worth emphasizing that the aforementioned data-driven approaches are different from another promising data-driven method (i.e., data driven computing Kirchdoerfer and Ortiz, 2016) in which the BVP is solved directly from experimental material data (measurements), bypassing the empirical material modeling step, involving the calibration of constitutive",
        "parameters (Kirchdoerfer and Ortiz, 2016; Ibanez et al., 2017; Kirchdoerfer and Ortiz, 2018,?; Ibanez et al., 2018; Eggersmann et al., 2019). While data-driven computing can be extremely powerful in many applications (Eggersmann et al., 2019), the first class of methods above-mentioned (based on the constitutive behavior at the material point level) 3",
        "can be advantageous when modeling complex and abstract constitutive behaviors, which are not a priori known. Moreover, they can be used even if the BVP does not have a unique solution due to important non-linearities and bifurcation phenomena (e.g. loss of uniqueness, strain localization at the length of interest, multiphysics, runway instabilities etc.). Nevertheless, until now ANNs for constitutive modeling are mainly used as a 'black-box'",
        "mathematical operator, which once trained on available data-sets, does not embody the basic laws of thermodynamics. As a result, vast amount of high quality data (e.g. with reduced noise and free of outliers) are needed to enable ANNs to identify and learn the underlying thermodynamic laws. Moreover, nothing guarantees that the predictions of trained ANNs will be thermodynamically consistent, especially for unseen data.",
        "In this paper, we encode the two basic laws of thermodynamics in the architecture of neural networks. This assures thermodynamically consistent predictions, even for unseen data (which can exceed the range of training data-sets). Therefore, we assure thermodynamicaly consistent network's predictions, both for seen and unseen data (which can exceed the range of the training data-sets). Moreover, our network does not have to identify/learn the",
        "underlying pattern of thermodynamical laws. Consequently, smaller data-sets are needed in principle, the training is more efficient and the accuracy of the predictions higher. The price to pay, in comparison with existing approaches, is the need of two additional scalar functions (outputs) in the training data-set. These are the free-energy and the dissipation rate. However, these quantities are easily accessible in micromechanical simulations (e.g.",
        "Nitka et al., 2011; Eijnden et al., 2016; Feyel, 2003) and can also be obtained experimentally in some cases. Then, based on classical derivations in thermodynamics (e.g. Houlsby and Puzrin, 2007; Einav, 2012) specific interconnections are programmed inside our ANN architecture to impose the necessary thermodynamic restrictions. These thermodynamic restrictions concern the stresses and internal state variables and their relation with the free-",
        "energy and the dissipation rate. Our approach is inspired by the so-called Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019), in which reverse-mode autodiff (Baydin et al., 2017) is used, allowing the numerical calculation of the derivatives of an ANN with respect to its inputs. The calculation of these derivatives, imposes some numerical requirements regarding",
        "the mathematical class of the activation functions to be used. More specifically, the internal ANN restrictions, derived from the first law of thermodynamics, require activation functions whose second gradient does not vanish. Otherwise, the problem of second-order vanishing gradients, as it is called here (cf. classical vanishing gradients problem in ANNs, e.g. Geron, 2015), can inhibit back-propagation and make training to fail. This new problem and its",
        "remedy is extensively explored and discussed herein. For the sake of simplicity and for distinguishing our approach from existing ones, we call the proposed ANN architecture Thermodynamics-based Artificial Neural Networks (TANNs). In our opinion TANNS should be the starting point for data-driven and physics-based constitutive modeling at the material point level.",
        "The paper is structured as follows. Section 2 presents a brief summary of the theoretical background of thermodynamics. In Section 3 an overview of the methodology proposed and architecture of TANNs is given. The main differences with classical, standard ANNs for 4 material constitutive modeling are also discussed. Particular attention is given to the choice",
        "of activation functions and the issue of second-order vanishing gradient is investigated in detail. Generation of material data-sets, with which the training of ANNs is performed, is presented in Section 4. In a first phase, we apply TANNs for the constitutive modeling of three-dimensional elasto-plastic material models, Section 5. In particular, we consider both hyper-plasticity models and smoother hypo-plasticity ones. Extensive comparisons with",
        "standard ANNs, which are not based on thermodynamics, are also presented. In a second phase, we investigate the performance and robustness of TANNS with the presence of noise in the training data, Section 6. This is achieved by generating a set of pseudo-experimental data, adding several levels of artificial noise. Supplementary figures and data are available in Supplementary data file. For the implementation of Artificial Neural Networks and",
        "Thermodynamic-based Artificial Neural Networks, we leverage Tensorflow v2.0. All code accompanying this manuscript is available upon request. 2. Thermodynamics principles: energy conservation and dissipation inequality 2.1. Energy conservation A convenient way to express the (local) energy conservation is",
        "pè = a VSymy divq + ph, (1) with p being the material density; e the specific internal energy (per unit mass); a the Cauchy stress tensor; Vv the spatial velocity gradient tensor; 9 the rate of heat flux per unit area; h the specific energy source (supply) per unit mass, and \".\" denotes contraction",
        "of adjacent indices. 2.2. Second principle The second law of thermodynamics can be formulated in terms of the local Clausius- Duhem inequality ph",
        "4 ps N div (2) 8",
        "A with S being the specific (per unit mass) entropy; h/0 and -(q n)/0 the rate of entropy supply and flux, respectively. By removing the heat supply h between the energy equation (1) and the entropy inequality (2) leads to q VO",
        "p(es - é) + a VSymy N 0, (3) 8 where the first two terms represent the rate of mechanical dissipation D = p(0s = è) +",
        "a . VSymy and the latter the thermal dissipation rate, i.e., Dth = 9-V8 The thermal dissipation is non-negative because heat only flows from regions of higher temperature to lower temperature-that is, the heat flux 9 is always in the direction of the negative thermal gradient. As it follows we argue that the mechanical dissipation rate must itself be non- negative (point-wise), i.e., D N 0.",
        "5 2.3. Dissipation function The definition of the (mechanical) dissipation rate D leads to pé = pes + 0. VSymy - D. (4)",
        "Let us define the specific (per unit volume) internal energy E = pe and entropy S = ps and further assume constant material density, i.e., dp = 0-that is, É = pè and $ = ps. We shall assume a small strain regime, i.e., Vu A 1, with 8 : VSymu the small strain tensor, where u is the displacement vector field, and & : VSymy its rate of change. Equation (4) hence becomes",
        "É eS + a -D. (5) Let assume a strain-rate independent material such that the energy potential is E:=E(S,,Z), (6)",
        "and the dissipation rate, being a first-order homogeneous function of Z, is D:=D(S.6,Z.Z). (7) where Z = (Gis 5N) denotes a set of N (additional) internal state variables, Sis i = 1, N. We define here (thermodynamic) state variables those macroscopic quantities",
        "characterizing the state of a system, see e.g. Maugin and Muschik (1994). The physical representation of Si is not a priori prescribed. For instance, in the case of isotropic damage, 5 is a scalar; in anistotropic damage, a tensor; in the case of elasto-plasticity, a second order tensor, etc. The generalization to a finite-strain formulation can be achieved by considering the deformation gradient, F, and the first Piola-Kirchhoff tensor, P, as strain",
        "and stress measures, respectively (see e.g. Mariano and Galano, 2015; Anand et al., 2012). Nevertheless, as it would presented in Section 3, an incremental formulation of the material response is herein adopted. Therefore, the hypothesis of a small strain regime is usually realistic, at least for a large class of materials and an updated Lagrangian scheme. Time differentiation of the internal energy gives",
        "OE QE OE É = & +",
        "(8) as ds 1=1 05 which is equal to (5) and, grouping terms, it leads to",
        "QE OE OE + + =",
        "(9) as dE as The arbitrariness of $, é, and 5 leads to the following relations",
        "OE 8 = (10a) as QE",
        "a (10b) ds N OE >",
        "5i+D=0. (10c) i=1 a5; 6 Further introducing the thermodynamic stress, conjugate to Si X = (X1, ,XN), with",
        "OE Xi : Vie[1,N], (11) asi",
        "we obtain the following, alternative definition of the dissipation D=2x5 (12) i=1 2.4. Isothermal processes",
        "In the case of isothermal process, the (specific) Helmholtz free-energy, F : E - Se = F(,,Z), which is the Legendre transform conjugate of e, is preferable. In this case, the dissipation rate is such that D : D(,,Z,2). The equations presented above (9-30) still hold (by replacing E with F) OF",
        "OF OF S = a X5",
        "(13) 80 ds a5i 3. Thermodynamic-based Artificial Neural Networks",
        "Within the framework of ANN or RNN material models, we can distinguish two main classes. The first consists of direct, so-called \"black-box\", approaches, where the information flow passes through the machine learning tool which operates as a mere regression operator, see e.g. Ghaboussi et al. (1991); Lefik and Schrefler (2003). The second class coincides with ANN and/or RNN models incorporating some knowledge in an informed, guided graph",
        "with intermediate history-dependent variables or detecting history-dependent features, see Heider et al. (2020); Mozaffar et al. (2019); Gorji et al. (2020), among others. Whilst the latter case has demonstrated to be extremely successful for path-dependent plasticity models, both classes are affected by the lack of physics, being the predictions not always compatible with thermodynamic principles (at least). Figure la depicts the direct approach",
        "AE Aa AE Aa T",
        "0 Da I Qt AE",
        "(b) informed neural network (i- (c) informed neural network (i- (a) black-box (BB) network. NN1). NN2). Figure 1: Examples of direct, black-box (BB) (a) and informed (b, c) neural networks for material laws modeling. Inputs are highlighted in gray (0), outputs in black (0)",
        "7 (BB), in which ANNs, usually Feed-Forward Neural Networks (FFNNs), are used to predict the stress increment, (output, 0) O = Ao = o+Ar a', from the input I = (E,AE), being g' the precedent strain and As its increment. In concise form, we write O = BB@ I. In this scheme, g' and As can be regarded as the state variables, namely the ANN state variables",
        "(not necessarily coinciding with those introduced in Sect. 2), on which the updated material stress depends on. Two examples of guided, informed ANNS, either FFNNS or RNNS, are illustrated in Figures 1b and lc. In both cases, the neural network intrinsically accounts for path-dependency, see e.g. Heider et al. (2020), making sequence of predictions of the main output. The network i-NN1 makes use of the last predicted output, i.e., a', to make",
        "predictions of the next output, O = Ao. The inputs are hence I = (E,AE, a'). We shall notice that, differently from BB, the stress at the precedent state, d', is also considered to be an ANN state variable. Other alternatives exist in the selection of the ANN variables of state. One may chose, as we shall see in Section 5, thermodynamic) state variables to be ANN state variables.",
        "In the case of temperature-dependent material response, the second case (1-NN2) allows to make predictions that depend on the precedent temperature state, 8, namely O = i-NN2@ I, with I = (E,AE, a',0) and O = (Ac, A8). The main aim of this work is to change the classical paradigm of data-driven ANN material modeling into physics-based ANN material modeling. We propose a new class",
        "of ANNs based on thermodynamics, which are Thermodynamic-based Artificial Neural Networks (TANNs). By exploiting the theoretical background presented in Section 2, we propose neural networks which, by definition, respect the thermodynamic principles, holding true for any class of material. In this framework, TANNS posses the special feature that the entire constitutive response of a material can be derived from definition of only",
        "two (pseudo-) potential functions: an energy potential and a dissipation pseudo-potential (Houlsby and Puzrin, 2007). TANNS are fed with thermodynamics \"information\" by relying on the automatic differentiation technique (Baydin et al., 2017) to differentiate neural networks outputs with respect to their inputs. This strategy allows to construct a general framework of neural networks material models which, in principle, can be exploited to",
        "predict the behavior of any material and assure that the predictions of TANNS will be thermodynamically consistent even for inputs that exceed the training range of data. In this paper, we only focus on strain-rate independent processes. Moreover, our approach can be extended, following the developments in Houlsby and Puzrin (2000), to materials showing viscosity and strain-rate dependency.",
        "The model relies on an incremental formulation and can be used in existing Finite Element formulations (among others), see e.g. Lefik and Schrefler (2003). Figure 2 illustrates the scheme of TANNS. The model inputs are the strain increment, the previous material state at time t, which is identified herein through the material stress, a', temperature, e, and the internal state variables, 5 as well as the time increment At, namely I = (,A6,0,0.6,A0).",
        "The primary outputs, O1, are internal variables increment, AKi, the temperature increment, AB, and the energy potential at time f+At, F'+Ar, i.e. Oi = (AKi, AB, FA). Secondary outputs, O2-that is, outputs computed by differentiation of the neural network with respect to the 8 inputs-are the stress increment, Ao, and the dissipation rate, D'Ar, 7 which we denote as",
        "O2 = VIOi = (Ao, D'+A). The class of neural networks we propose differs from the previous ones by the fact that the quantity of main interest, i.e., the stress increment, is obtained as a derived one, which intrinsically satisfies the first principle of thermodynamics (and, as we shall see, the second principle, as well). In the following, we briefly recall the basic concepts of artificial neural",
        "networks (paragraph 3.1), we then focus on the issue of the second-order vanishing gradients that may afflict the training and the performance of an ANN model (paragraph 3.2). In particular, it is shown that, in the framework of Thermodynamic-based Artificial Neural Networks, particular attention has to be paid to the selection of activation functions. Finally, we present in detail the architecture of our model (paragraph 3.3).",
        "At AE Aa 9 Figure 2: Schematic architecture of TANN. Inputs are highlighted in gray (C); outputs in black, (e) and",
        "and intermediate quantities in white (O). Dashed lines represent definitions, while arrows are used to denote neural network links. 3.1. Artificial neural networks overview We give herein a brief overview of the basic concepts of ANNs and in particular FFNNS. For more details, we refer to Hu and Hwang (2002) and Géron (2019). ANNs can be",
        "regarded as non-linear operators, composed of an assembly of mutually connected processing units-nodes-, which take an input signal I and return the output 0, namely O = ANN@I. (14) ANNs consist of at least three types of layers: input, output and hidden layers, with equal",
        "or different number of nodes. Figure 3 depicts a network composed of one hidden layer, with 3 nodes, an input layer with 2 inputs, and an output layer with 1 node. When an ANN has two or more hidden layers, it is called a deep neural network (Géron, 2019). Denoting the input array with I = (i), with t = 1,2...,n1 (nr is the number of inputs), and the outputs with O = 0;) with j = 1,2.. - ,no (no is the number of outputs), the signal flows from layer",
        "(1- 1) to layer (I) according to (I-1) PP A) Z) with k wip (-1) + (15)",
        "9 where PP are the outputs of node k, at layer (I); S is the activation function of layer (); ne\" is the number of neurons in layer (1 - 1); wp are the weights between the s-th node in layer (1-1 1) and the k-th node in layer (); and b) are the biases of layer (I). With reference to Figure 3, the output is given by",
        "0= (0) z(o)) with z(o) = Zr w/2) (1) + b(2) p with Zr (1) Xt + where the activation function of the output layer, A(out), is a linear function, as in most of",
        "the cases for regression problems. The weights and biases of interconnections are adjusted, in an iterative procedure (gradient descent algorithm Géron, 2019), to minimize the error between the benchmark, 0, and prediction, O, that is measured by a loss function, L. In the following, the Mean (over a set of N samples) Absolute Error (MAE) is used as loss function, i.e.,",
        "EAI 1Oi-O: L= (16) N where i = 1,2,...N. The errors related to each node of the output layer are hence back-",
        "propagated to the nodes in the hidden layers and used to calculate the gradients of the loss function, namely af a1-me aplm aL awkm aw/\" at-m) apl-m) I-m+I)",
        "(17) aL az-mD) ap-meD) aL apl-m) j=1 apl-m) atmD ap-mep which are then used to update weights and biases, and force the minimization of the loss",
        "function values, i.e. af wP-Rw = wp E (18) àw/p'",
        "where E is the so-called learning rate. The weights and biases updating, the so-called training process, is performed on a subset of the input-output data-set, defined as training set, known from experimental tests or numerical simulations of the phenomenon investigated. The ANN is trained. The training process is stopped as the loss function is below a specific tolerance. Then a test set, a subset of the input-output data-set different to the training set, is used",
        "to check the error of the network predictions. Once the ANN is trained, it is used in recall mode to obtain the output of the problem at hand. Due to their rich interpolation space, ANNs have proved to be universal approximators, see e.g. Cybenko (1989); Chen and Chen (1995), although the choice of hyper-parameters, such as the number of neurons, the network topology, the weights, etc. are problem-dependent.",
        "The same stands for the activation functions, which may be chosen to have some desirable properties of non-linearity, differentiation, monotonicity, etc. Most of these properties stem 10 P 1",
        "output input layer layer hidden",
        "layer (1) Figure 3: Graph illustration of an ANN structure with two inputs, one output, and one hidden layer with three nodes. from issues related to the gradient descent algorithm and the so-called (first-order) vanishing gradient problem. As it follows, we briefly present this well-known issue and we further give",
        "insights in a variation of it: the second-order vanishing gradient. 3.2. First- and second-order vanishing gradients During the training process, if the gradient of the loss function with respect to a certain weight tends to zero-that is, see Eq. (18), when  = ap? /az & 0 (with A' the first- derivative of the activation function with respect to its arguments)-the update operation can",
        "fail, and weights and biases are not updated. In this case, we have the so-called first-order vanishing gradient (Géron, 2019). Figure 4 displays some of the most common activation A() A() A()",
        "A() tanh(:) max(0,-) A(: 0) A(: 20)",
        "A'() A'() A'() A' (z) 5 cosk-()",
        "05 max(0,1) A'(x<0)=e (20)=1 Figure 4: Some of the most common activation functions and their first-order gradient. From left to right: the",
        "logistic (sigmoid) function, the hyperbolic tangent, the Rectified Linear Unit (ReLU), and the Exponential Linear Unit (ELU). functions and their derivatives-that is, the logistic (sigmoid) function, the hyperbolic tangent, the Rectified Linear Unit (ReLU), and the Exponential Linear Unit (ELU). The sigmoid function is S-shaped, continuous, differentiable, its output values range from 0 to 1, and its",
        "first-order gradient (derivative) assumes values much smaller than 1. When inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close 11 to 0. Thus when backpropagation kicks in, it has virtually no gradient to propagate back through the network, which is problematic for training. The hyperbolic tangent activation",
        "function is very similar to the sigmoid, but it is centered at zero allowing to maintain the output values within a normalized range (between -1 and 1). Nevertheless, it suffers from saturated gradients (at Z = 0, for Z <K -1 and Z >> 1). ReLU is continuous but not differentiable at Z = 0. Nevertheless it is an unsaturated activation function for positive values of Z (its gradient has no maximum) and, therefore, it allows to avoid vanishing gradient",
        "issues for Z > 0. Nevertheless, it suffers from a problem known as the dying ReLUs: during training, some neurons are effectively deactivated, meaning they stop outputting anything other than 0 (for Z < 0). To this purpose many variants exist. The ELU activation, for instance, takes on negative values when Z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradient problem, as discussed earlier.",
        "Second, it has a nonzero gradient for Z < 0, which avoids the dying units issue. Finally, the function is smooth everywhere, including Z = 0, which helps speed up gradient descent. When dealing with TANNS, second-order vanishing gradients can appear. This is a new concept and, in order to illustrate it, we will use a simple example. Assume an ANN which takes as input some I = x and returns (a) O1 = x2 and (b) its derivative with respect to the",
        "input, i.e., O2 = VIOi = 2x (see Figure 5). Let us consider one hidden layer, with activation function A and N, nodes. The activation function of the single output layer, which returns 3?, is assumed to be linear. In this case, the output (a) is given by 01 =plo) = S(0) e)) 01",
        "o) + b(o) (19) 01 + 50",
        "The derivatives of the outputs with respect to the inputs can be easily computed, in this simple example, by taking advantage of the automatic (numerical) differentiation (Baydin et al., 2017). Output (b) is hence computed by the ANN as a01 àp(o) az(o) ap azP",
        "02 = V,Oi 5 aI àz(0) ap\" az\" aI (20)",
        "801 aI Consider the following loss function L= WoLo + Wviolv,0, where Lo and Lv,o are the loss functions corresponding to output Oi and O2 = Vy01,",
        "respectively. Regularized weights, Wo and WVy can be used to obtain comparable order 12 2r A() Figure 5: ANN which takes as input x and returns (a) O1 = x2 and (b) its derivative with respect to the",
        "input, i.e., ViOi = 2x, with one hidden layer whose activation function is A. of magnitude of the two loss functions. During training, weights and biases are updated according to Eq. (18) where the computed gradients are aL AL +",
        "Ev,o (2la) awlo) aL =",
        "LVio (21b) aw aL L",
        "(21c) àb(o) aL - Ev,o (21d)",
        "ab\" It follows, from relations (21b) and (21d), that the gradient descent algorithm needs the computation of both first- and second-order gradients of the activation function A. This particular result is a direct consequence of the minimization of the error between the gradient of the outputs with respect to the inputs, i.e. O2 = V/01, and the corresponding benchmark",
        "values, 2x. This is what we call second-order vanishing gradient problem. It is tantamount to the first-order variant, but it involves the second derivatives (and not only the first) of the activation functions in an ANN. With reference to Figure 4, none of the depicted, classical activation functions is suitable for such class of problems. Consequently, care must be taken in selecting activation functions that do not have second-order vanishing gradients. To this",
        "purpose, Appendix A presents an example illustrating the issue of second-order vanishing gradients and proper solutions are given to this problem. 3.3. Architecture of Thermodynamic-based Artificial Neural Networks Herein we detail the architecture and the internal steps/definitions TANNS are relying on. The architecture is detailed in Figure 6. The input vector is I = (,A6,0,0.4,A), the",
        "primary and secondary outputs are O = (AKi, AB, FHA) and VIo = (Ac, D'+A), respectively. TANN involves the following steps: 1. computation of the updated strain (definition): g'+Ar  g' + As 2. prediction of the kinematic variables and temperature increments with two sub-ANNs: AE = SNN@ (W,AE,d,8.)",
        "and AB = SNNA@ (gAr A6,c,8,5) 13 3. computation of (a) the updated kinematic variables rates (backward finite difference approximation):",
        "+1 AL At (b) the updated kinematic variables (definition): $+1:=5+ AE (c) the updated temperature (definition): 0+1 e' + AB 4. prediction of the updated energy potential:",
        "FAr = SNNP@(g+Ar S'Ar 8'A) 5. computation of the updated dissipation rate (definition, Eq. (13)): D'+Ar : aFA agAr k+Ar 6. computation of (a) the updated stress (definition, Eq. (13)): o1+Ar aFAr ag'HAT (b) the stress increment (definition): Ao : o+Ar a",
        "At At t+At +A) AE",
        "tA 6+At At at 4 SNNF",
        "Aa AE SNN, Ot 1+At) SNNF",
        "Aa AB Q++A) SNNS SNN, $",
        "(a) non-isothermal processes. (b) isothermal processes. Figure 6: Architecture of TANNS: general case (a) and for isothermal processes (b). Inputs are highlighted in gray (0); outputs in black, C for direct ANN predictions and for derived outputs; and intermediate quantities (definitions) are in white (0) and (0). Relationships obtained from definitions are represented",
        "with dashed lines, while arrows denote ANNS. TANNS are thus composed of three sub-ANNs; SNNy predicts the internal variables increment, SNNe predicts the temperature increment (note that in case of the isothermal conditions, this component can be removed from the architecture, see Fig. 6b), and SNNF predicts the Helmholtz free-energy. The main output, the increment in stress, is computed",
        "according to expression (13), which stems from thermodynamic requirements. By virtue of the fact that the entire constitutive response of a material can be derived from definition of only two pseudo-)potential functions, the model is able to predict the stress increment from the knowledge of the energy potential (and the internal variables 5i). It is worth noticing that, differently from common approaches (cf. Sect. 3), the sub-network SNNF is required to",
        "learn a scalar quantity-that is, the Helmholtz free-energy potential. This offer compelling advantages. When dealing with ANNS, the curse of dimensions (increasing effort in training 14 and large amount of training data required) is an important issue when the studied problem passes to higher dimensions, see e.g. Bessa et al. (2017). Passing from 1D to 3D, for instance,",
        "increases the number of variables the ANNs need to learn. For stresses, from one single scalar value, in 1D, we pass to a vector with six-components, in 3D. The computational effort is thus not trivial. Nevertheless, TANNS are, in principle, less affected by these issues as the two pseude-)potentials, on which the entire set of predictions relies on, are scalar functions. The computation of dissipation, from expression (13), plays a double role. First, it assures",
        "thermodynamic consistency of the predictions of TANNS (first law). Second, it brings the information to distinguish between reversible and irreversible processes, e.g. elasticity from plasticity/damage, etc., and it is trained to be positive or zero (second law). It is worth noticing that further improvements of the performance of TANNS may be obtained, as suggested in the work of Karpatne et al. (2017), by adding a physical inconsistency",
        "term to the loss functions (e.g., with respect to dissipation). 4. Generation of data We present the procedures used to generate material data TANNS are trained with in the following applications (see Sect. 5). We distinguish two different strategies. The first one, based on the numerical integration of an incremental form of the constitutive",
        "relations, is used to generate data for an hyper-plastic von Mises constitutive model with kinematic hardening (Houlsby and Puzrin, 2000, 2007). A different procedure is instead used to generate data for von Mises hypo-plasticity (Einav, 2012). In the case of hyper-plasticity models, we assume the Ziegler's orthogonality condition (see paragraph 4.1 and Ziegler, 2012; Houlsby and Puzrin, 2000, 2007), which, in general,",
        "it is not a strict requirement. Nevertheless, it is worth noticing that this restriction applies only on the generated data, and not on the ANN class here proposed. More precisely, TANN architecture still holds even for materials for which the Ziegler's normality condition does not apply. We shall recall that the aim is to demonstrate the advantages of thermodynamics- based neural networks with respect to classical approaches. Hence the restrictions, imposed",
        "by the orthogonality hypothesis for the generation of data, are expected not to affect the comparisons presented in Section 5. Hypo-plasticity is here used to show that the framework of thermodynamics encoded in TANNs is general and does not depend on restrictive assumptions such as the Ziegler's orthogonality condition afflicting hyper-plasticity. Furthermore, we consider the hypo-plastic",
        "material case to test TANNS against materials with a smooth response, which is more representative of realistic materials. 4.1. Incremental formulation 4.1.1. Hyper-plasticity Following the hyper-plasticity framework proposed in Einav et al. (2007), the thermo-",
        "mechanical, non-linear, incremental constitutive relation for strain-rate independent materials, 15 undergoing infinitesimal strains, is here derived in the framework of isothermal processes (0 = cost). By differentiating the energy expressions (13) and rearranging the terms, we obtain the following non-linear incremental relations",
        "d=0eF-ét ZaF-5 (22a) -Xi = OyeF.ê+ > Ou4F. (22b) where the following notation is adopted",
        "aF aF a?F OEsF = deF =",
        "OuAF = OEiyOEu dEiyask a4a5 Further, introducing the thermodynamic dissipative stresses xt = (X1, .. XN) and assuming",
        "the Ziegler's orthogonality condition (Ziegler, 2012), the following non-linear, incremental constitutive relation can be found Mly-o . & ify=0 E = (23)",
        "Mlyso . & else with OesF 2k ELF.(9 y à axk dy",
        "OgeF ZKOLAF. e OEsF] aXk E = Mly-0 =",
        "CE and Mlyco = OdeF (24) 0 B",
        "Vg 0 B and . denotes the contraction of adjacent indices. In the above relations (23-24), whose derivation is presented in Appendix B, y = J(E, Z,X) is the yield function, 0 denotes a",
        "quantity (scalar or tensorial, depending on the dimensionality of the internal variable set) equal to zero, and dy dy CE",
        "OyeF, ds ax; i=1 y",
        "y dy B = OyeF i=1 a5i ax; i=1 aX; -",
        "aXk 4.1.2. Hypo-plasticity The theoretical framework used here to generate the hypo-plastic data can be found in Einav (2012). Einav (2012) proposed a new theoretical model, called hplastic, unifying hypo- and hyper-plasticity models. In particular, compared to standard hypo-plasticity, the",
        "incremental material formulation can be derived from (pseudo-) potentials. The h'plastic 16 model allows ease integration of the incremental constitutive equations, i.e., Eq. (5.15a) in Einav (2012). Here we use the following incremental equations (Eq.s 7.14 and 7.15 in Einav, 2012) for",
        "the relaxation strain rate (z) and stress increment (d), according to von Mises model:  a - z =",
        "(25a) 2k2 k 2 5- -",
        "- à = Kép + 2G (25b) k 2",
        "where k represents the elastic limit in simple shear; S is a material parameter (s > 0); K and G are, respectively, the bulk and shear moduli; Ep is the mean strain rate; à and z are, respectively, the deviatoric total and relaxation strain rate tensors; and a' is the deviatoric stress. 4.2. Data generation",
        "Data are generated in a Python environment, where SymPy and SciPy libraries are used for symbolic calculations and numerical integration. The accuracy of the generation process is 10-6 for strains and 10-4 MPa for stresses. For the case of hyer-plasticity, data are generated by identifying an initial state for the material at time t,",
        "state at time t : E' = and g', Si 0",
        "and a given strain increment é, assuming constant and unitary time increment At = 1 (8 = Ag'). Numerical integration of the ordinary differential equations (23) is performed with an explicit solver (Bogacki and Shampine, 1989) to obtain the state at the new time t + At, i.e., o1+Ar",
        "-XHAr state at time t + At : E'+Ar = 4HAr 2+At",
        "For hypo-plasticity, data are generated similarly but only internal variables Si, deformation 8, and stress a are used to represent the material state at time t and t+ At, through numerical resolution of Eq.s (25a) and (25b). The training data play a crucial role for both the accuracy of the predictions and the generalization with respect to the ANN state variables, e.g., strain increments. The",
        "generalization capability of a network is here defined as the ability to make predictions for loading paths different from those used in the training operation. Nevertheless, a significant 17 dependency on the ANN state variables is usually observed. This may result in a poor network generalization. In Lefik and Schrefler (2003), an improvement of the generalization",
        "capability of ANNs is proposed. Artificial sub-sets of data, with zero strain increments, are added in the set of training data to force the network in learning that to zero input increments correspond zero output increments. In the available literature, strain-stress loading paths are commonly used in training. If recursive neural networks are used, feeding them with history variables (loading paths) is",
        "the only possible solution (see e.g. Mozaffar et al., 2019). Nevertheless, ANNs do not necessary need the data-sets to be (historical) paths. Herein, we generate data randomly. Conversely, this allows us to (1) improve the representativeness: of the material data and (2) improve the generalization of the network on the strain increments. For the hyper-plastic material model, the initial state, E and g, and",
        "the strain increment, AE, are randomly generated from standard distributions with mean value equal to zero and standard deviation equal to Emax, Emaxi and AEmax respectively. The Cauchy and thermodynamic stresses, a' and X, as well as the internal variables 5i are then calculated to satisfy the constraint y' S 0. This incremental procedure is repeated for Nsamples, resulting in a set of Nsamples ordered pairs (E,8,Ae; EA), from which the corresponding",
        "energy potential and dissipation rate at time t + At are evaluated. For the case of hypo- plasticity, data are generated by random loading paths as the procedure aforementioned for hyper-plasticity is not applicable to the theoretical framework in Einav (2012), as no definition of yield surface is needed for the derivation of the incremental material constitutive law. Figure 7 depicts the sampling for one of the investigated applications (see paragraph",
        "5.1). 18 1500 - Perain 100001 = derain",
        "1250 - Fval 8000 - dval 1000 D Perain",
        "D dtrain 6000 750 J Poa",
        "D 4vr J 500 4000 250",
        "2000 0 0 -3000-2000-1000 0 1000 2000 3000 0 25 50 75 100 125 150 175 200",
        "p.pAr (MPa) q.q\"ar (MPa) 800 - Strain Arain 600-",
        "- Sval Kval 600 D Str 400",
        "D Arain J 400 p.val J Va",
        "200 200 0 -0.02 -0.01 0.00 0.01 0.02 0.00 0.01 0.02 0.03 0.04",
        "6,.6,a () d,ear (-) 800 = Fptrain - rain 600-",
        "- fpval - Eval 600 J D FAte ain 400-",
        "- rain 400 E J fpval D LVay",
        "200 200 0- -0.02 -0.01 0.00 0.01 0.02 0.00 0.01 0.02 0.03 0.04",
        "5-4ar () z',\"A(-) - Flrain 4000 - Dn",
        "6000 - FVy 3000 DVy 4000",
        "€ 2000 2000 1000 20 25 35",
        "0.0 0.5 1.0 1.5 2.0 2.5 prar (N-mm) DAr (N-mm/s) Figure 7: Sampling for material case H-1 (cf. Table 1). From top to bottom: mean and deviatoric stress (p and q); mean and deviatoric total deformation (Ep and e); mean and deviatoric plastic deformation (Sp and",
        "z); energy (F) and dissipation rate (D). Training and validation data-sets are also distinguished. 19 5. Applications Herein we use TANNS to the modeling of multi-dimensional elasto-plastic materials and demonstrate their wide applicability and effectiveness. It is worth noticing that, even though",
        "the applications here investigated consist of elasto-plastic materials, the proposed class of neural networks can be successfully applied (without any modification) to materials with different or more complex behavior, accounting e.g. for damage and/or other non-linearities (in the framework of strain-rate independent processes). In paragraph 5.1, von Mises hyper- plasticity is accounted for, considering perfect-plasticity, hardening and softening behaviors.",
        "In paragraph 5.1, we further investigate hypo-plastic material models. In the examples presented herein, reference dependent variables, such as the total plastic strain, were considered. However, the internal state variables set Z, see Eq. (7), and, consequently, our approach are not limited to this kind of state variables. As it follows, the hyper-parameters (i.e., number of hidden layers, neurons, activation",
        "functions, etc.) of the networks are selected to give the best predictions, while requiring minimum number of hidden layers and nodes per layer. This is accomplished by comparing the learning error on the set of test patterns, per each trial choice of the hyper-parameters. In each training process, we use early-stopping. In other words, training is stopped as the error of a validation set starts to increase while the learning error still decreases (Géron,",
        "2019). The validation set is used to avoid over-fitting of the training data. Throughout this Section relatively simple deep feed-forward neural networks architectures are used (with, at maximum, two hidden layers) and no additional regularization techniques are employed (e.g., L1/L2 penalties, dropout, etc.). Each numerical example is accompanied with a detailed discussion about the network architecture.",
        "5.1. von Mises hyper-plasticity In order to illustrate the performance of TANNs, we use the simple von Mises elasto- plastic model with kinematic hardening and softening. The model can be derived from the following expressions of the energy potential and dissipation rate 9K",
        "F= p)-(Ep-5p)+ 2 H + G(e- ) (e +",
        "Z, 2 D=kV2 Vz-:, where k represents the elastic limit in simple shear; K and G are the bulk and shear moduli; H the hardening (softening) parameter; Ep and 5p are, respectively, the mean total and",
        "plastic deformation; and e and Z are, respectively, the total and plastic deviatoric strain tensors. The yield surface can be derived as shown in Appendix B (Houlsby and Puzrin, 2007) and is defined as y=D-X'-z= VX'.X - V2k S 0, (26)",
        "with Xij = 2G eij Zij) + Hzij- 20 Table 1: Material parameters for 3D elasto-plastic von Mises material. case K G",
        "k H (GPa) (GPa) (MPa) (GPa) H-1 167 77 140",
        "0 H-2 167 77 140 -10 H-3 167 77 140",
        "10 5.1.1. Training Data are generated as detailed in Section 4. A total of 6000 data with random increments of deformation are generated. In order to improve the performance of the network in recall mode, additional sampling with random uni-axial and bi-axial loading paths are also used.",
        "The samples are split into training (50%), validation (25%), and test (25%) sets. The sampling in terms of the mean and deviatoric stresses, P and 4, and deformations, Ep and e, is presented in Figure 7 for material case H-1 (perfect plasticity). We distinguish between training and validation sets. For the sake of simplicity, stress and deformation are converted in the principal axes frame of reference. Table 2 shows the mean, standard deviation, and",
        "maximum values of the training data-sets. Adam optimizer with Nesterov's acceleration gradient (Dozat, 2016) is selected and a batch size of 10 samples is used. Data are normalized between -1 and 1. We use the Mean Absolute Error (MAE), and not the Mean Square Error (MSE), as loss function for each output in order to assure the same precision between data of low and high",
        "numerical values. Regularized weights are used to have consistent order of magnitude of different quantities involved in the loss functions. The network architecture is adapted to the size of the inputs and outputs, with respect to the mono-dimensional case. In particular, the sub-network SNN, consists of two hidden layers, with 48 neurons (leaky ReLU activation function), and three output layers, one per each",
        "(principal) component of (increment of) 5. The sub-network S-NNF has one hidden layer with 36 neurons (activation ELU2). The output layers for both sub-networks have linear activation functions and biases set to zero. The resulting number of hyper-parameters is R 3000 (cf. Ghaboussi and Sidarta, 1998; ?; Lefik et al., 2009; Mozaffar et al., 2019).. Figure 8 displays the loss functions of each output as the training is performed, for material case",
        "H-1 (perfect plasticity). The early stopping rule assures convergence, after approximately 1000 epochs, with MAEs of the same order of magnitude for the 4 outputs, AK, F'+Ar, 1 Ac, and D'+A The adimensional MAE is approximately equal to 1 X 10-4 for all outputs at the end of the training. Similar behaviors are also recovered for cases H-2 (softening), H-3 (hardening).",
        "5.1.2. Predictions in recall mode Once the network has been trained, it is used, in recall mode, to make predictions. We briefly present the performance of TANNs in predicting the material response for a random loading path. Figure 9 depicts the comparison with the target material model for material case H-1. The network displays extremely good performance and the ability to predict",
        "21 Table 2: Mean (y), standard deviation (st), and maximum values of the training data-sets. data H st",
        "max Ei () 8x 10-5 0.010 0.041 Asi () 2x 10-5 0.003 0.014",
        "5i () 8x 10-5 0.010 0.041 AG () 0 1x: 10-4 0.0011 ai (MPa)",
        "-1.4 143 544 Ac, (MPa) 123",
        "7577 36744 F'Ar (N-mm) 1.82 2.72 37.9 D'+Ar (N-mm/s) 0.41 0.38 2.61",
        "le-1 Afitrain Afival le-2 Ftrain",
        "Fval - le-3 Aditrain Adival le-4",
        "Dtrain Dval 10 100 1000",
        "epochs Figure 8: Errors in terms of the adimensional Mean Absolute Error (MAE) of the predictions of TANN (loss functions), as the training is being performed, evaluated with respect to the training (train) and validation (val) sets. Weights and biases update are computed only on the training set. random loading path.",
        "5.1.3. TANN VS standard ANN. Generalization of the network Herein we investigate the performance of TANNS with respect to the classical approach of ANNs (Ghaboussi et al., 1991; Lefik and Schrefler, 2003), as well as the sensitivity with respect to the input variables range. Figure 10 displays the architecture of the network, ANN, with inputs I = (E, AEi,C, 5) and output O = (ALi, Aci), with i = 1,2,3 denoting the",
        "principal components. The architecture is selected to give the best performance, preserving the same number of hyper-parameters between TANN and standard ANN. The network, ANN, consists of the two sub-networks, aNNy and aNNa, with two hidden layers, each one, leaky ReLU activation functions, and number of neurons per layer equal to 48. As for SNNy and SNNG, in aNNy and aNN three output layers (1 neuron each) are used, with linear",
        "activation functions and zero biases. In Figure 11 we present the comparison of the MAE of the network predictions with respect to the target values (training and validation data-sets). It is worth emphasizing that both ANNs and TANNs are dependent on the choice of the user, concerning, for instance, the hyper-parameters. Moreover, the actual configurations",
        "of both networks may benefit of ltemative/extensions, such as RNNs. Nevertheless, the 22 le-2 0.5 0.0",
        "3 A a -0.5 -1.0 -1.5",
        "20 40 60 80 Increments () (a) random loading path. le3 5.0 le-3",
        "model model TANN 2.5 TANN € 0",
        "0.0 -1 a 6 G-5.0",
        "-2 -7.5 -3 -1.5 -1.0 -0.5 0.0 0.5 -1.5 -10 -0.5 0.0 0.5 E1 ()",
        "le-2 61 (-) le-2 (b) stress and internal variable. lel",
        "1.5 model TANN 8. model TANN 6 1.0",
        "0.5. A .0 5p (-) le-3",
        "Luky () le-3 (c) energy and dissipation rate. Figure 9: Predictions of TANN for a uni-axial random loading path, compared with the target constitutive model, case H-1, perfect plasticity: (a) loading path; (b) principal stress, 01, and internal variable, 51,",
        "predictions; (c) energy and dissipation rate predictions. following comparisons show the added value of our approach compared to standard ones that do not explicitly contain physics, as TANNS. We first compare the performance of both networks, TANNS and standard ANNS, in predicting the material response for cyclic isotropic loading paths (material case H-1, cf.",
        "Table 1). A linear elastic material response is expected and retrieved. Figure 12 displays the stress predictions of TANNS and ANNs, compared with the target values, for different strain increments. It is worth mentioning that the standard approach of ANNs does not succeed in accurately predicting the elastic deformation range. Moreover, contrary to TANNS, the stress predictions of standard ANNS, depend strongly on the cyclic loading. As the network",
        "is used recursively, in recall mode, the stress predictions rapidly become less and less precise, due to error accumulation. The performance of both networks is further compared for the following tri-axial loading 23 1+At",
        "Ae Ae Ac aNN, Aa a'",
        "ANN. +At $ - (a) ANN scheme.",
        "(b) ANN architecture. Figure 10: Schematic (a) and full architecture (b) of the network, not based on thermodynamics, standard ANNS. Inputs are highlighted in gray (C), outputs in black (e) le-1 le-1",
        "TANN Ak,train TANN Abival ANN Aftrain le-2 le-2",
        "ANN Abival 14 le-3 TANN Acitrain le-34 TANN Adiyal",
        "ANN Acitrain le-4 ANN Adival 10 100 1000 10 100 1000",
        "epochs epochs (a) mean absolute error of AE prediction (b) mean absolute error of Aci prediction Figure 11: Training of ANNs compared with TANNS evaluated with respect to the training (train) and",
        "validation (val) sets. path nT ni Agi = As sgn cos",
        "AE2 - AE3 = As sgn sin (27) 2N 2N with N = Emax/Ae, Emax = 2 X 10-3 + 1, and As = 1 X 10-5 + 1 X 10-1.",
        "Figures 13 and 14 display the material response in terms of the principal stresses, 01 and 03, and inelastic strains, 51 and 53, respectively. We show in Figure 15 the energy and dissipation rate predicted by TANNS with those computed, with standard ANNs, directly using the corresponding definitions for the free-energy and dissipation rate, Eq. (5.1). The predictions of TANNS are in good agreement with the constitutive model, independently",
        "from the strain increment, which exceeds considerably the training range. Nevertheless, the performance of ANNs is found to be strongly affected by the values of As. For strain increments well inside the training range, i.e., As = 1 X 10-3, standard ANNs are well predict the material response. In particular, computing, through Eq. (5.1, the dissipation rate and energy from the ANNs' predictions reveals that ANNs can successfully predict",
        "output respecting the requirements of the thermodynamics. The first and second principles of thermodynamics are indirectly learned during training (on thermodynamic consistent data). However, standard ANNs perform poorly for strain increments smaller and larger than the ones at which it was trained (As = 1 X 103, cf. Table 2). And in these cases, 24",
        "standard ANNs predict thermodynamically inconsistent outputs. The predictions of TANNS are, instead, always thermodynamically consistent. Moreover, the quantities of primary interest, such as the stress, the internal state variable, and the energy are in extremely good agreement with the reference model. The same stands also for the dissipation rate. We notice, once more, that its values are always positive, even when",
        "the network is used for predictions beyond the training range. Figure $3 displays the predictions for very small strain increments, i.e. As = 1x10-5. TANNS successfully still predict the response in this limiting case, while ANNs do not. Indeed, the training data were generated guaranteeing an accuracy of the order of 10-6 in terms of strains and such small strain increments are at the margin of the computing precision.",
        "In the Supplementary Material, we present the results of a uni-axial loading scenario, in Figures S1 and S2, for material case H-1 (perfect plasticity). Kinematic hardening and softening material cases and the predictions of TANNS and ANNs are shown in Figures $4-S9. It is worth noticing that in all the cases, even for very large strain increments-for which the predictions of the network in terms of dissipation rate differ from the target values-,",
        "TANNS successfully predict the Jacobian, i.e., aci dej (i,j = 1,2,3), in very good agreement with the reference model. This is of particular importance for numerical simulations with implicit algorithms. Therefore, TANNS can successfully replace complicated constitutive models or multiscale approaches, but considerably and safely decreasing the calculation cost, even when the requested increments are outside the training range.",
        "We emphasize that the performance of TANNs and standard ANNs can be improved by increasing the dimension of the training data-sets, the number of the hyper-parameters (e.g. numbers of hidden layers, etc.). Nevertheless, the fundamental gap between the two approaches in assuring thermodynamically consistent quantities still persist. 25",
        "le3 e-3 1.0 model 3.0 model",
        "TANN TANN ANN 2.5 ANN",
        "0.5 2.0 € 0.0 1.5 5.0.5",
        "1.0 d 0.5 model A -1.0",
        "0.0 TANN ANN 51 () le-3 Ep 6p (-)",
        "le-3 Suku (-) le-5 (a) strain increment As = 1 X 10-3. le4",
        "le3 el model TANN ANN",
        "3 0- 4 -2 model",
        "A - model -4 TANN 5 TANN",
        "0 ANN ANN -1.0 -0.5 0.0 0.5 1e1 1.0 -0.5 0.0 0.5 1.0 -6",
        "E1 (-) Ep- 5p (-) le-1 Liku () le-3",
        "(b) strain increment As = 1 X 10-2. le5 le5 le4 model",
        "0.00 4 TANN ANN -0.25 E 2",
        "0 -0.50 6 2-0.75 model",
        "- model -4 A-1.00 0 TANN TANN",
        "-6 ANN -1.25 ANN -1.0 -0.5 0.0 0.5 1.0 1.0 -0.5 0.0 0.5 1.0",
        "() Ep 6p (-) Sysu (-) le-2 (c) strain increment As = 1 X 10-1.",
        "Figure 12: Comparison of the stress, energy, and dissipation predictions of TANNS and standard ANNs. Energy and dissipation for ANNS are computed according to Eq. (5.1), for the cyclic, isotropic loading path AEi = AE, = AE = As sgn (cos EN) -with N = Emax/AE, Emax = 2 X 10-3 (a), Emax = x10-1 (b), and Emax = 1 (c), for material case H-1 (perfect plasticity). Each row represents the prediction at different As increments. 26",
        "le3 le3 1.25 model 1.5 model TANN",
        "TANN 1.00 ANN ANN 1.0 *",
        "0.5 E 50.25 0.00 0.0",
        "E1 () le-3 63 (-) le-3 (a) strain increment As = 1 X 10-4.",
        "le3 le3 1.25 model 1.5 model 1.00 TANN",
        "TANN ANN 1.0 ANN Ea 0.5",
        "50.25 6 0.00 0.0 -1 0",
        "le-3 le-3 E1 () 53 (-) (b) strain increment As = 1 X 10-3.",
        "le4 le4 model model 6 TANN",
        "6. TANN ANN ANN a 4",
        "41  6 2- 6 0",
        "-1.0 -0.5 0.0 0.5 1e1 0.0 0.5 1.0 1.5 E1 (-) (-) 2e1",
        "E3 (c) strain increment As = 1 X 10-2. le5 le5 model",
        "model 6 TANN 6- TANN ANN ANN",
        "4 a 6 2 6 -1.0 -0.5 0.0 0.5 1.0",
        "0.0 0.5 1.0 1.5 2.0 E1 () 3 (-) (a) strain increment As = 1 X 10-1. Figure 13: Comparison of the stress predictions of TANNS and standard ANNs with respect to the target",
        "values, for the tri-axial cyclic loading path, Eq. (27), for material case H-1 (perfect plasticity). Each row represents the prediction at different As increments. 27 le-3 1.0 le-3",
        "0.01 model TANN 0.8 -0.5 ANN 3",
        "3 0.6 a1.01 G0.4 model TANN",
        "-1.54 0.2 ANN USUUEU 0.0",
        "-2.0- le-3 3 E1 (-) 63()",
        "le-3 (a) strain increment As = 1 X 10-4. le-3 1.0 le-3 0.01 model",
        "TANN 0.8 -0.5 ANN 0.6 3",
        "C 1.01 0.4 model TANN",
        "-1.5 0.2 ANN 0.0 -2.0 -2 -1 0",
        "2 3 51 () le-3 (-)",
        "le-3 (b) strain increment As = 1 X 10-3. le-1 le-2 model",
        "model 0.04 TANN TANN ANN ANN",
        "3o.5 C a  2 -1.0",
        "0 -1.5 2 -1.0 -0.5 0.0 0.5 1e1 0.0 0.5 1.0 1.5 251",
        "EI () 63 () (c) strain increment As = 1 X 10-2. le-1 model",
        "model 0.0 * TANN TANN ANN ANN",
        "30.5 C 2  3 -1.0",
        "0 -2 -1.5 -1.0 -0.5 0.0 0.5 1.0 0.0 0.5 1.0 1.5 2.0",
        "E1 (-) 53 (-) (a) strain increment As = 1 X 10-1. Figure 14: Comparison of the internal variable predictions of TANNS and standard ANNs with respect to the target values, for the tri-axial cyclic loading path, Eq. (27), for material case H-1 (perfect plasticity).",
        "Each row represents the prediction at different AE increments. 28 le-2 6 model model",
        "TANN 3 TANN & ANN ANN",
        "I 3 2 2 & A",
        "0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 Ep 5p () 3 Susu ()",
        "le-4 (a) strain increment As = 1 X 10-4. le-1 model model",
        "5 TANN 3 TANN ANN a ANN",
        "3 2 2 A A",
        "0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 Ep-5p (-) le-3",
        "byky (-) le-3 (b) strain increment As = 1 X 10-3. 1.50 le4 lel",
        "model L model 1.25 TANN TANN 1.00 ANN",
        "0 ANN  I 0.75 0.50 A 0.25",
        "A 0.00 0.00 0.25 0.50 0.75 1.00 0.0 0.5 1.0 1.5 Ep- -6p (-) 155150",
        "Liky () le-2 (c) strain increment As = 1 X 10-2. le6 le3",
        "1.50 model 1.25 TANN  a 1.00 ANN 5 0.75",
        "-2 50.50 & 0.25 A model",
        "TANN 0.00 -6 ANN 0.00 0.25 0.50 0.75 1.00 1.25 1.50 0.0 0.5 1.0 1.5",
        "Ep 6p (-) hibuy (-) le-1 (a) strain increment As = 1 x 10-1. Figure 15: Comparison of the energy and dissipation rate predictions of TANNS and computation according",
        "to Eq. (5.1) for standard ANNS with respect to the target values, for the tri-axial cyclic loading path, Eq. (27), for material case H-1 (perfect plasticity). Each rOW represents the prediction at different As increments. 29 5.2. von Mises hypo-plasticity We illustrate the performance of TANNS in predicting smooth material behaviors as well,",
        "modeled here using the hypo-plasticity model, presented in Einav (2012) and paragraph 4.1.2. The energy potential and dissipation rate are given by 9K F (Ep- Sp)+",
        "2 +Gle-2)-e-2, D=d.: with z being defined in Eq. (25a). We consider K = 167 GPa, G = 77 GPa, and S = 1, see Eq.s (25a) and (25b).",
        "Data are generated as detailed in Section 4. 8000 are data generated through random loading paths. As in the case of hyper-plasticity, additional sampling with random uni-axial and bi-axial random loading paths are also used. The samples are split into training (50%), validation (25%), and test (25%) sets. The sampling in terms of the mean and deviatoric stresses, p and 9, and deformations, Ep and e, is presented in Figure $10.",
        "The architecture and hyper-parameters of TANNS are maintained equal to the hyper- plastic case (see paragraph 5.1). The internal variables Si are selected to coincide with the inelastic strain. We emphasize that this particular choice does not affect the results of TANNS. As extensively discussed in Einav (2012), an alternative choice to the selection of the inelastic strain as internal variable can be the material porosity.",
        "The early stopping rule assures convergence, after approximately 1000 epochs, with MAEs of the same order of magnitude for the 4 outputs, AK, F'+Ar, Ac, and D'+Ar The (adimensional) MAE is approximately equal to 1 X 10-4 for all outputs at the end of the training. 5.2.1. TANN VS standard ANN. Generalization of the network",
        "As for the hyper-plastic cases, we investigate the performance of TANNS with respect to standard ANNs through illustrative examples. The architecture and hyper-parameters of ANNs are maintained equal to the hyper-plastic case (see paragraph 5.1.3). Figure 16 shows the predictions of both networks for the following bi-axial loading path ni",
        "AEi = -AE2 = As sgn COs AE3 = 0, 2N with N = Emax/AE, Emax = 2 X 10-3 + 1, and As = 2 X 10-4,2x 10-3. TANNs' predictions are in excellent agreement with the target model. The smoother material response, with",
        "respect to the hyper-plastic scenario, is well captured by the networks. Standard ANNs clearly underperform. 30 le2 3 le-3",
        "model model TANN 2 TANN ANN",
        "ANN  0 € 6 -1  -1",
        "d & -2 -2 2 -3 2",
        "E1 (-) le-3 (-) le-3 (a) strain increment As = 2 X 10-4.",
        "le2 le-2 model 2 ANN TANN ",
        "model € € 01 TANN 0",
        "6 ANN 4 1 2",
        "oota 2 61 (-) le-2 E1 (-)",
        "le-2 (b) strain increment As = 2 X 10-3. Figure 16: Comparison of the stress and internal variable predictions of TANNS and standard ANNs with respect to the target values, for a bi-axial cyclic loading path, AE2 = -AE1, with AEi as in Eq. (5.2.1), for a perfect hypo-plastic material. Each row represents the predictions at different As increments.",
        "Additional demonstration of the performance of TANNS is given in Figure 17, for a bi- axial loading path with strain-controlled ratcheting. Ratcheting is a well known phenomenon shown by many materials during cyclic loading, which has been modeled here with the h?plasticity framework (Einav, 2012). In particular, we show that TANNS, contrary to ANNs, successfully predict principal stresses, inelastic strains, energy potential, and dissipation",
        "rate. 31 le2 le2 model",
        "TANN ANN / 0 01",
        "- - € € -1",
        "6 -1 model 6 TANN -2",
        "-2 ANN -3 -6 51 (-)",
        "le-3 2 (-) le-3 (a) principal stresses. le-3",
        "le-3 model model 61 TANN 2 TANN",
        "ANN ANN 0 E € -2",
        "a U S 4 -2 -6 8",
        "6 -6 4 E1 (-) le-3 2 ()",
        "le-3 (b) principal internal variables. le-1 le-2 2.0",
        "- model 6 TANN 1.5 ANN a",
        "1.0  & 0.5 model A 2 0.0 TANN",
        "a ANN 3 -2 Ep-6p () le-4",
        "Syku (-) le-4 (c) energy and dissipation rate. Figure 17: Comparison of the predictions of TANNS and standard ANNs with respect to the target values, for the bi-axial loading path with strain-controlled ratcheting, for a perfect hypo-plastic material.",
        "32 6. Noise in training data and robustness of predictions After having demonstrated the performance of TANNS and their superiority to standard approaches in modeling path-dependent material behaviors, we investigate the effect of noise in the measurements of the data used to train artificial neural networks. This is achieved",
        "by training TANNS (and ANNs) using the previously generated data and adding, in the training and validation sets, artificial noise. For sake of clarity, we consider a perfectly plastic material (case H-1, cf. Tab. 1). The additive noise, ns, is based on a normal distribution with standard deviation (sd) equal to 10% of the mean value of the clean data. In particular, we consider the following scenarios, independently:",
        "(1) noise in 4Ar, 7 i.e., nsg, with sd = 10% of the mean value of 5A, (2) noise in CAr, 7 i.e., nso, with sd = 10% of the mean value of CAr, (3) noise in F+Ar 1 i.e., nsF, with sd = 10% of the mean value of Fl+Ar, and (4) noise in D'+Ar, 1 i.e., nsF, with sd = 10% of the mean value of D'+A We emphasize that the aforementioned noise levels were chosen to demonstrate the performance",
        "of TANNS and generally lower levels of noise are expected in practical applications. However, we examine each scenario independently in order to explore better the effect of noise on training and on the accuracy of the predictions. In cases (1) and (2), once the noised quantities are computed (denoted with aAr and Z+A), the increments, i.e., Adi and ASi are re-evaluated as Adi = a#Ar ai and ASi = gear - 5, respectively.",
        "The architecture and hyper-parameters of the neural networks, both TANNs and ANNs, designated in this study are the same as those used in Section 5. It should be noticed that, for each case (1-4), the data used to train the networks are not respecting the thermodynamics requirements due to the added noise, i.e., Eq. (13). The addition of noise can have an impact on the training of the networks and their",
        "predictions. We first focus on the former. Figure 18 displays the loss functions of each output as the training of TANNs is performed, for noise added in stresses, case (2). The MAE is evaluated between the TANNs' predictions and the (noised) training and validation data- sets. Table 3 shows the MAEs of the predictions of TANNS with respect to the validation data-sets, for each level of noise, at the end of the training. Although the earlystopping",
        "technique is used, training is accomplished, in all cases of noise, after approximately 1000 epochs. By comparing the training using the original, un-noised data (Fig. 8) and that using the noised ones (Fig. 18), we can observe that TANNS are unable to learn the noised signal, i.e., Ad,. This is a direct consequence of the fact that the network evaluates the stress increments",
        "from the knowledge of the stress state at time t and the energy potential predictions. When noise is added, the first law of thermodynamics is violated and the training operation with noised data is unsuccessful, with respect to the noised training and validation data-sets. However this is not a drawback of our approach. On the contrary, it is an indication of the 33",
        "le-1 Afitrain Alival le-2 Ftrain",
        "Fyal 3 le-3 Aditrain Adival le-4",
        "Dtrain Dval 10 100 1000",
        "epochs Figure 18: Errors of the predictions of TANN, as the training is being performed, evaluated with respect to the training (train) and validation (val) sets. Noise is added in stress to both training and validation data-sets, case (2). Table 3: MAEs of the predictions of TANNS with respect to the validation data-sets, for the original, un-",
        "noised data and each level of noise, at the end of the training, Early-stopping is used and the training is always completed at approximately 1000 epochs. Mean Absolute Error (le-4) ALi Aci Fl+Ar ADI+A:",
        "un-noised3.2 4.1 0.8 7.5 nsy 324 2.5 0.9 5.9 nsa 3.3 39 1.0 5.4",
        "nsF 3.3 3.1 19 7.3 nsp 3.4 4.9 0.9",
        "57 quality of the data, which in this case they don't respect the laws of thermodynamics due to measurement noise. Notice that the values of the training error is consistent with Eq. (13), the expression given in Section 3.4 and the magnitude of the noise. The implementation of the laws of thermodynamics in the network's architecture shields the learning process and",
        "prohibits learning of inconsistent data. For instance, with reference to Table 3, we can see that for case (2), nsa, the MAEs in the predictions of the inelastic strains, energy and dissipation rate approximately coincide with those obtained with the un-noised data. The aforementioned behavior is not observed in standard ANNS. As an example, we show",
        "in Table 4 the MAEs of the predictions of standard ANNs with respect to the validation data-sets, for noised stresses. In this case, we can see that the network, unaware of the requirements of the thermodynamics, learns successfully the noised outputs. This means that, once standard ANNs are asked to make predictions, in recall mode, the outputs will be affected by the noisy training in an unpredicted way. For the levels of noise cases (1),",
        "(3), and (4), similar results are obtained. 34 Table 4: MAES of the predictions of standard ANNs with respect to the validation data-sets, for the original, un-noised data and noise on stresses, at the end of the training (approximately 1000 epochs). Mean Absolute Error",
        "(le-4) ALi Aci un-noised5.8 4.2 nsa 5.9 4.2",
        "In Figure 19 compared the predictions in recall mode of both networks based on noise data. The predictions of the training with clean (un-noised) data is also presented for helping the comparison. We notice that TANNS, whilst trained on data with relatively large levels of noise, successfully predict the material response and perform more or less as when trained on data free of noise. On the contrary, standard ANNs are strongly affected by",
        "the large levels of noise of the data used to train the network. Similar results are found in presence of noise in the training and validation data of the internal variable, Si, see Figure S11, in the Supplementary Material. It should be noticed that, in this case, ANNs do not manage to successfully minimize the loss function of ALi, with the selected number of hyper- parameters. This is the consequence of the ANN architecture which have been chosen to",
        "achieve the best performance with thermodynamic consistent (clean of noise) data. However, we emphasize that, if the number of hyper-parameters of the ANN model were increased to achieve convergence with respect to the noised data, then ANNs would learn the noised material response, resulting to be highly affected by noise measurements. Consequently, we can state that TANNs show high degree of robustness to noise, when",
        "compared to ANNs. 7. Concluding remarks A new class of artificial neural networks models to replace constitutive laws and predict the material response at the material point level was proposed. The two basic laws of thermodynamics were directly encoded in the architecture of the model, which we refer to",
        "as Thermodynamic-lased Neural Network (TANN). Our approach was inspired by the SO- called Physics-Informed Neural Networks (PINNS) (Raissi et al., 2019), where the automatic differentiation was used to perform the numerical calculation of the derivative of a neural network with respect to its inputs. Feed-Forward Neural Networks were used herein, but the approach is general and can be applied to Recurrent Neural Networks (RNNs) or other",
        "types of ANNs as well. The numerical requirements regarding the mathematical class of appropriate activation functions to be used together with automatic differentiation were investigated. More specifically, the internal restrictions, derived from the first law of thermodynamics, require activation functions whose second gradient does not vanish. This new problem and its remedy was",
        "extensively explored and discussed in the manuscript. 35 le-2 le3 le-3 model",
        "model 1.5 model 0.0 ANN ANN 0 ANN",
        "noised -0.5 1 noised ANN € 1.0 31.0 ANN",
        "2 5 0.5 51.5 C0xxxc0c0xxx006 A -2.0",
        "0.0 noised ANN -2.5 0.0 0.5 1.0 1.5 () le-3",
        "() le-3 usuy () 324 le3",
        "le-3 le-2 1.25 model 0.0 model - model",
        "1.00 TANN 0 TANN 3 TANN noised noised",
        "3 0.75 -0.5 TANN TANN 3 0.50 C",
        "2 -1.0. 5 0.25 0.00 noised TANN -1.5",
        "51 () le-3 () le-3 0.0 0.5 1.0 1.5",
        "(a) strain increment As = 1 X (b) strain increment As = 1 X Ayki () le-4 10-4. 10-3.",
        "(c) strain increment As = 1 X 10-2. Figure 19: Influence of noise in the stress, Oi, for the predictions of the stress, internal variable, and dissipation rate of TANNS and of standard ANNS with respect to the target values, for the tri-axial cyclic loading path for material case H-1 (perfect plasticity). Noise strongly affect the predictions of standard",
        "ANNS, see Fig.s 13-15. TANN, relying on an incremental formulation and on the theoretical developments in Houlsby and Puzrin (2007), posses the special feature that the entire constitutive response of a material can be derived from definition of only two scalar functions: the free-energy and the dissipation rate. This assures thermodynamically consistent predictions both for",
        "seen and unseen data. Differently from the standard ANN approaches, TANN does not have to identify, through learning, the underlying thermodynamic laws. Indeed, predictions of standard ANNs may be thermodynamicaly inconsistent, even though the training of the network has been performed on consistent material data. Being aware of physics, TANNs are found to be a robust approach with the presence of noise measurements in the training",
        "data, contrary to the standard ANN approach. For the cases here investigated, we showed that TANNS are characterized by high accuracy of the predictions, higher than those of standard approaches. The integration of thermodynamic principles inside the network renders TANN's ability of generalization (i.e., make predictions for loading paths different from those used in the training operation)",
        "remarkably good. Consequently, TANN is an excellent candidate for replacing constitutive calculations at Finite Element incremental formulations. Moreover, thanks to the implementation of the free-energy in the network predictions and its thermodynamical relation with the stresses, the Jacobian AE at the material point level is better predicted even for increments far beyond the training data-set range. As a result quadratic convergence in implicit",
        "formulations can be preserved, reducing the calculation cost. Finally, we investigated the presence of noise in data and the effect on the training process 36 and predictions in recall mode. The thermodynamic framework of TANNS shields the training operation and prohibits learning of inconsistent data. As a result, TANNS posses",
        "high degrees of robustness to noise, compared to standard ANNS. Further extensions of TANN in a wide range of applications, for complex materials, are straightforwards, as the thermodynamics principles hold true for any known class of material, at any length (micro- and macro-scale). Acknowledgments",
        "The authors would like to acknowledge the anonymous reviewers whose feed-backs helped improve this work. The author I.S. would like to acknowledge the support of the European Research Council (ERC) under the European Union Horizon 2020 research and innovation program (Grant agreement ID 757848 CoQuake).",
        "37 7.1. Appendix A. Understanding second-order vanishing gradient In the following, we investigate the performance and influence of different activation functions on the computational time to train an ANN with input I, primary output O1,and secondary output O2 = VIO1. Consider the above discussed example with I = x, O1 = x?,",
        "and O2 = 2x. The ANN has one hidden layer, with N, = 6 nodes, and activation functions as reported in Table 5. The output layer has linear activation and null bias. The absolute error is selected as loss function for both Oi and O2. Training is performed on 1000 samples, normalized between -1 and 1. A very small value for the learning rate is selected, i.e., E = 10-5 in order to facilitate the gradient descent algorithm in reaching small values of the",
        "loss function. We use early-stopping. In other words, training is stopped as the error of a validation set (500 samples) starts to increase while the learning error still decreases (Géron, 2019). The validation set is used to avoid over-fitting of the training data. Table 5: Set of activation functions considered to investigate the performance of the network with outputs 0= x and VIo = 2x, with I = x, in the framework of first- and second-order vanishing gradients.",
        "Function Z range S(z) '(z) '\"(z) z<0",
        "0 0 0 ReLUz z20",
        "Z 1 0 z<0 0",
        "0 0 ReLUoS24z z20 0.5:2+z Z+ 1 1",
        "z<0 0 0 0 ReLU",
        "z20 22 2z 2 ELUe",
        "Vz ez-1 ez ez z<0",
        "e-1 ez ez ELU, z20",
        "z 1 0 z<0 e2-1",
        "ez ez ELUOSz z20 0.52?+z Z+ 1 1",
        "z<0 e-1 ez ez ELU,",
        "z20 z2 2z 2 z<0",
        "e-1 ez ez ELU z20",
        "2* 4z3 12z2 z<0 ez-1",
        "ez ez ELU40522 z20 24 + 0.522+z 423+z+1 12:2+1 For each tested activation function, Table 6 shows the adimensional Mean Absolute Error (MAE) calculated using a set of new, unseen data (500 samples) of input-output predictions",
        "for x2 and 2x. The advancement of training is quantified herein as the number of epochs, i.e., the number with which the training algorithm works with the training data-set Géron (2019). Activation functions with quadratic terms, or of higher degree, perform very well, compared to their linear equivalents. RELU2, ELU2 outperform as their shape is very similar to the input-output regression they are trained to learn. Nevertheless, it is worth",
        "38 noticing that training fails when activation functions with vanishing second gradient are used (e.g. RELUZ and ELUz). Figure 20 compares the ANN predictions for a selection of activation functions with the analytical (exact) results. Whilst RELUZ is clearly inadequate, ELUZ predictions overall agree with the analytical values. This is due to the fact that the",
        "ANN takes advantage of the exponential term, for negative Z and thus successfully manage to satisfy both O and Vro. Additional hidden layers may improve the performance of the network. It can be further noticed that activation function of high degree, e.g. ELUe, ELU, and ELUA405242 even if successful, require a large number of epochs. Table 6: Activation functions and performance with unseen data.",
        "Activation function SA L Lo Lv,o no. epochs (10-4) (10-4) (10-4) () ReLUz",
        "1521.2 205.98 1315.18 920 ReLU0.524z 762.4 93.58 668.85 8054",
        "ReLU2 0.061 0.0241 0.0371 148 ELUe 127.2 26.83 100.38 19477",
        "ELU, 108.56 12.12 96.44 17280 ELUOSB4z 65.5 10.91 54.63",
        "12178 ELU,2 0.13 0.067 0.067 88 ELUA",
        "65.36 33.75 31.61 20051 ELU40S 12.94 1.81 11.13 9683",
        "39 1.2 2z 1.0 ReLU,",
        "Z ReLU, 0.8 ELU, ELU,",
        "0.61 o 0.4 0.2 01 Z",
        "-1.0 -0.5 0 0.5 1.0 -1.0 -0.5 0 0.5 1.0 z() 2 (-) 0.25",
        "1.0 2 0.75 2z 0.20 ReLU,",
        "ReLU, ELU, 0.50 ELU, 20.15",
        "0.25 0 0.14 C' 0 -0.25 0.05",
        "0.50 01 -0.75 -0.4 -0.2 0 0.2 0.4 -1.0 -0.4 -0.2 0 0.2 0.4",
        "z() 2 (-) (a) x2 predictions, O1, using ReLU and ELUz- (b) 2x predictions, O2, using ReLU and ELUz- le-2",
        "le-1 65  4 2z ELU, ELU,",
        "ELUAsAs 2 C E EV",
        "ELU. ELU, d 0 ELU, -2",
        "-4 0 2 0 2",
        "(-) le-1 (-) le-1 le-2",
        "le-2 1.0 2 3 2z 0.8",
        "ELU, 2 ELU, ELUasAs ELUAsA",
        "0.6 ELU. C ELV 0.4",
        "E, d 0 ELU, 0.2 -1",
        "-2 U -0.5 0.5 -3 -1.5 -1.0 -0.5 0 0.5 1.0 1.5",
        "(-) le-1 z() le-1 (c) 2 predictions, O1, using ELUZ, ELU0522+2 (a) 2x predictions, 02, using ELUz, ELUOS2",
        "ELU,, ELUe, and ELU. ELU2, ELUe, and ELU. Figure 20: Comparison of different activation functions for the prediction of the primary output, x2 (a), and secondary output, 2x (b). From top to bottom the range of Z decreases from larger to smaller values, to observe the behavior at z & 0.",
        "40 Appendix B. Derivation of the incremental material formulation By differentiating the energy expressions (13) and rearranging the terms, we obtain the following non-linear incremental relations à = OsF-g+ > OEAF-5k + OFê",
        "(28a) -Ki = OyF.E+ OLE F + agoFè (28b) -S = OF.E+ > dog F . 5k + OgFè, (28c)",
        "where the following notation is adopted 8F 8PF OgsF = deyF=",
        "dEioEu dEiO5 8F 8?F OEoF =",
        "O0F = dEij00 a02 We introduce the thermodynamic dissipative stresses X+ = (X1,. ...,XN) with aD",
        "X,= ViE[1,N). (29) ab For a rate-independent material, the dissipation is a homogeneous first-order function in the",
        "internal variable rates 5i (Houlsby and Puzrin, 2007). This homogeneity can be expressed by the Euler's relation OD D = x-5",
        "(30) i=1 a5i which, together with (11), implies x-x)-4=0 (31)",
        "i-1 Ziegler's orthogonality condition (Ziegler, 2012) is further assumed, i.e., X; = Xi Vie [1,N]. Being D homogeneous first-order function in 5i the Legendre transform, conjugate to Xi, is degenerate, that is equal to zero, and represents the yield function y = 5(6,6,Z,X), i.e. ly = 2x-4-D=0,",
        "(32) where 2 is a non-negative multiplier. From the properties of Legendre transform, the following flow rules must hold dy 5 = A",
        "Vie[ [1,N]. (33) ax; 41 Since 2 N 0 and ly = 0,<0. If y=0, the following consistency equation is met",
        "Oy dy dy y - + B=0.",
        "(34) ds i=1 asi i-1 ax; 00",
        "By further using the flow rules (33) and Ziegler's normality condition, we obtain CE Co A = -",
        "e, (35) B B with",
        "dy dy C= OyeF, Os",
        "ax; i-1 dy dy Ce",
        "OgoF, 00 ax; i=1 and",
        "dy dy dy ày B = OueF",
        "= a5i ax; - aX; aXk Finally, we arrive to the following, incremental non-linear formulation, for y = 0, MgE Mgo",
        "o Mje Mxe & Mos",
        "Moo E = Ml-0 5, with É= -S 0 Ml-0 = CE.B Co dy (36) ax; B ax;",
        "A Bc. Ce B B",
        "and MgE = OgsF- Zk OF-(% ax, Mge = OseF - Zk deF-( axk Mxe = agsF- ZOuF-(. axk y Mxo = OyoF - ZOuF-(. ax;",
        "Mes = OBsF = Zk OF-( axe) J Moe = O00F - Zk Oes F axk In case of y < 0, relation (36) becomes OEsF OEoF]",
        "ageF OyoF E = Mlyco 5, with Mlyso = OBsF O0oF (37) 0 0",
        "0 0 42 References O. Lloberas Valls, M. Raschi Schaw, A. E. Huespe, X. Oliver Olivella, Reduced finite element square",
        "techniques (rfe2): towards industrial multiscale fe software, in: COMPLAS 2019: XV International Conference on Computational Plasticity: Fundamentals and Applications, International Centre for Numerical Methods in Engineering (CIMNE), 2019, pp. 157-169. M. Nitka, G. Combe, C. Dascalu, J. Desrues, Two-scale modeling of granular materials: a DEM-FEM approach, Granular Matter 13 (2011) 277-281. doi:10.1007/s10035-011-0255-6.",
        "F. Feyel, A multilevel finite element method (FE2) to describe the response of highly non-linear structures using generalized continua, Computer Methods in Applied Mechanics and Engineering 192 (2003) 3233- 3244. doi10.1016/S0045-7825(03/00348-7. N. Bakhvalov, G. Panasenko, Homogenisation: Averaging Processes in Periodic Media: Mathematical Problems in the Mechanics of Composite Materials, 1989.",
        "G. Houlsby, A. Puzrin, A thermomechanical framework for constitutive models for rate-independent dissipative materials, International journal of Plasticity 16 (2000) 1017-1047. I. Einav, G. Houlsby, G. Nguyen, Coupled damage and plasticity models derived from energy and dissipation potentials, International Journal of Solids and Structures 44 (2007) 2487-2508. G. T. Houlsby, A. M. Puzrin, Principles of hyperplasticity: an approach to plasticity theory based on",
        "thermodynamic principles, Springer Science & Business Media, 2007. I. Einav, The unification of hypo-plastic and elasto-plastic theories, International Journal of Solids and Structures 49 (2012) 1305-1315. F. Masi, I. Stefanou, V. Maffi-Berthier, P. Vannucci, A discrete element method based-approach for arched masonry structures under blast loads, Engineering Structures 216 (2020) 110721. dothtps//dolor/1.",
        "lolpempinet2P.IUTAL. F. Masi, I. Stefanou, P. Vannucci, A study on the effects of an explosion in the Pantheon of Rome, Engineering Structures 164 (2018) 259-273. doi10.1016/jengstruct.2018.02.082- H. Rattez, I. Stefanou, J. Sulem, The importance of thermelydromechanical couplings and microstructure to strain localization in 3d continua with application to seismic faults. part i: Theory and linear stability",
        "analysis, Journal of the Mechanics and Physics of Solids 115 (2018a) 54 = 76. domhtps/dolorg/l. 1016/3mps2018.03.00. H. Rattez, I. Stefanou, J. Sulem, M. Veveakis, T. Poulet, The importance of hemmoelydromechanica. couplings and microstructure to strain localization in 3d continua with application to seismic faults. part ii: Numerical implementation and post-bifurcation analysis, Journal of the Mechanics and Physics of",
        "Solids 115 (2018b) 1 29. doihttps://doi.or/10.1016/mps:.2018.03.003. N. A. Collins-Craft, I. Stefanou, J. Sulem, I. Einav, A cosserat breakage mechanics model for brittle, granular media, Journal of the Mechanics and Physics of Solids (2020) 103975. dolhttps//dol.org/10.1016/mps. 2020.103975. A. P. V. D. Eijnden, P. Bésuelle, F. Collin, R. Chambon, J. Desrues, Modeling the strain localization around",
        "an underground gallery with a hydro-mechanical double scale model ; effect of anisotropy, Computers and Geotechnics (2016). do10.1016/.compge0.2016.08.06. A. Geron, Hands-on MachineLearning with Scikit-Learn & Tensorflow, volume 1, O'Reilly Media, 2015. dol10.1017/CB0978107415324.00.. MAarXIIOILIGRN4. T. M. Mitchell, et al., Machine learning. 1997, Burr Ridge, IL: McGraw Hill 45 (1997) 870-877.",
        "G. Cybenko, Approximation by superpositions of a sigmoidal function, Mathematics of control, signals and systems 2 (1989) 303-314. T. Chen, H. Chen, Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems, IEEE Transactions on Neural Networks 6 (1995) 911-917.",
        "J. Ghaboussi, J. H. Garrett, X. Wu, Koowlatga200hased modeling of material behavior with neural networks, Journal of Engineering Mechanics 117 (1991) 132-153. doi10.1061/(ASCE)733-9390991) 117:1(132). 43 J. Ghaboussi, D. Sidarta, New nested adaptive neural networks (nann) for constitutive modeling, Computers",
        "and Geotechnics 22 (1998) 29-52. M. Lefik, B. A. Schrefler, Artificial neural network as an incremental non-linear constitutive model for a finite element code, Computer methods in applied mechanics and engineering 192 (2003) 3265-3283. S. Jung, J. Ghaboussi, Neural network constitutive model for rate-dependent materials, Computers & Structures 84 (2006) 955-963.",
        "C. Settgast, M. Abendroth, M. Kuna, Constitutive modeling of plastic deformation behavior of open-cell foam structures using neural networks, Mechanics of Materials 131 (2019) 1-10. Z. Liu, C. Wu, Exploring the 3d architectures of deep material network in data-driven multiscale mechanics, Journal of the Mechanics and Physics of Solids 127 (2019) 20-46. X. Lu, D. G. Giovanis, J. Yvonnet, V. Papadopoulos, F. Detrez, J. Bai, A data-driven computational",
        "homogenization method based on neural networks for the nonlinear anisotropic electrical response of graphene/Polymer nanocomposites, Computational Mechanics 64 (2019) 307-321. K. Xu, D. Z. Huang, E. Darve, Learning constitutive relations using symmetric positive definite neural networks, arXiv preprint arXiv:2004.00265 (2020). D. Z. Huang, K. Xu, C. Farhat, E. Darve, Learning constitutive relations from indirect observations using",
        "deep neural networks, Journal of Computational Physics (2020) 109491. S. Gajek, M. Schneider, T. Bohlke, On the micromechanics of deep material networks, Journal of the Mechanics and Physics of Solids (2020) 103984. M. B. Gorji, M. Mozaffar, J. N. Heidenreich, J. Cao, D. Mohr, On the potential of recurrent neural networks for modeling path dependent plasticity, Journal of the Mechanics and Physics of Solids (2020) 103972.",
        "Y. Heider, K. Wang, W. Sun, S0(3)-invariance of informed-graph-based deep neural network for anisotropic elastoplastic materials, Computer Methods in Applied Mechanics and Engineering 363 (2020) 112875. doihttps://doi.org/10.1016/.cma.cma.2020.112875. F. Ghavamian, A. Simone, Accelerating multiscale finite element simulations of history-dependent materials using a recurrent neural network, Computer Methods in Applied Mechanics and Engineering 357 (2019)",
        "112594. M. Mozaffar, R. Bostanabad, W. Chen, K. Ehmann, J. Cao, M. Bessa, Deep learning predicts path- dependent plasticity, Proceedings of the National Academy of Sciences 116 (2019) 26414-26420. A. L. Frankel, R. E. Jones, C. Alleman, J. A. Templeton, Predicting the mechanical response of oligocrystals with deep learning, Computational Materials Science 169 (2019) 109099.",
        "D. Gonzalez, F. Chinesta, E. Cueto, Learning corrections for hyperelastic models from data, Frontiers in Materials 6 (2019) 14. M. Lefik, D. Boso, B. Schrefler, Artificial neural networks in numerical modelling of composites, Computer Methods in Applied Mechanics and Engineering 198 (2009) 1785-1804. T. Kirchdoerfer, M. Ortiz, Data-driven computational mechanics, Computer Methods in Applied Mechanics",
        "and Engineering 304 (2016) 81-101. R. Ibanez, D. Borzacchiello, J. V. Aguado, E. Abisset-Chavanne, E. Cueto, P. Ladevèze, F. Chinesta, Data-driven non-linear elasticity: constitutive manifold construction and problem discretization, Computational Mechanics 60 (2017) 813-826. T. Kirchdoerfer, M. Ortiz, Data-driven computing in dynamics, International Journal for Numerical Methods",
        "in Engineering 113 (2018) 1697-1710. R. Ibanez, E. Abisset-Chavanne, J. V. Aguado, D. Gonzalez, E. Cueto, F. Chinesta, A manifold learning approach to data-driven computational elasticity and inelasticity, Archives of Computational Methods in Engineering 25 (2018) 47-57. R. Eggersmann, T. Kirchdoerfer, S. Reese, L. Stainier, M. Ortiz, Model-free data-driven inelasticity,",
        "Computer Methods in Applied Mechanics and Engineering 350 (2019) 81-99. M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, Journal of Computational Physics 378 (2019) 686-707. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, J. M. Siskind, Automatic differentiation in machine learning:",
        "44 a survey, The Journal of Machine Learning Research 18 (2017) 5595-5637. G. A. Maugin, W. Muschik, Thermodynamics with internal variables. Part I. General concepts, 1994. P. M. Mariano, L. Galano, Fundamentals of the Mechanics of Solids, Springer, 2015. L. Anand, O. Aslan, S. A. Chester, A large-deformation gradient theory for elastic-plastic materials: Strain",
        "softening and regularization of shear bands, International Journal of Plasticity 30-31 (2012) 116 - 143. doi.https//doi.or/10.016/jplas.2011.10.002. Y. H. Hu, J.-N. Hwang, Handbook of neural network signal processing, 2002. A. Géron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, O'Reilly Media, 2019.",
        "M. Bessa, R. Bostanabad, Z. Liu, A. Hu, D. W. Apley, C. Brinson, W. Chen, W. K. Liu, A framework for data-driven analysis of materials under uncertainty: Countering the curse of dimensionality, Computer Methods in Applied Mechanics and Engineering 320 (2017) 633-667. A. Karpatne, W. Watkins, J. Read, V. Kumar, Physics-guided neural networks (pgnn): An application in lake temperature modeling, arXiv preprint arXiv:1710.11431 (2017).",
        "H. Ziegler, An introduction to thermomechanics, Elsevier, 2012. P. Bogacki, L. F. Shampine, A 3 (2) pair of Runge-Kutta formulas, Applied Mathematics Letters 2 (1989) 321-325. T. Dozat, Incorporating Nesterov momentum into Adam (2016). 45"
    ],
    "summaries": [
        "Chunk 1: Researchers at the Massachusetts Institute of Technology (MIT)",
        "Chunk 2: \"Themmodynamic-based Neural Networks\" by Filippo Masi, Ioan",
        "Chunk 3: The HAL archive is a multi-disciplinary open archive for the deposit and dissemination",
        "Chunk 4: The aim of this project is to bring together all the research papers published in France in the last five years.",
        "Chunk 5: In: Proceedings of the 6th International Conference on Modelling and Simulation of Earth",
        "Chunk 6: The following is a list of contact details for some of the",
        "Chunk 7: Machine learning and artificial intelligence (AI) have emerged as key areas of research in the fields of computer science and engineering.",
        "Chunk 8: A new class of artificial neural networks has been developed for the study of thermo-dynamic processes.",
        "Chunk 9: In this paper we present TANNS, a novel method for training predictions of thermodynamic laws.",
        "Chunk 10: In this paper, we report the results of a novel method for predicting the properties of elas",
        "Chunk 11: In this paper, we present a new approach to the training of materials, called TANNS, based on",
        "Chunk 12: A large spectrum of models have been proposed in the literature, based on the following:",
        "Chunk 13: The history and state of a material can be taken into account through ad hoc enrichment of simpler laws.",
        "Chunk 14: The aim of this project is to develop new methods for the study of the laws of thermodynamics, i.e.",
        "Chunk 15: The aim of this project is to develop a new generation of simulations for the study of micromechanical phenomena.",
        "Chunk 16: The aim of this project is to develop a new scale for describing the microstructure of materials.",
        "Chunk 17: The current state-of-the-art in materials science and engineering is limited by the limited number of descriptors that can be used to represent",
        "Chunk 18: One of the major challenges in the area of computer science is the problem of learning from large amounts of data, which has been the focus of many",
        "Chunk 19: Machine learning is a branch of computer science that aims to learn from experience.",
        "Chunk 20: This article is part of a series of letters from leading scientists about",
        "Chunk 21: Machine Learning is a branch of computer science that aims to train computers on a large scale.",
        "Chunk 22: In this paper, we deal with the problem of approximators.",
        "Chunk 23: Non-linear models of materials are emerging as an important tool in the study of materials science and engineering.",
        "Chunk 24: We present a series of works on artificial neural networks (ANNs).",
        "Chunk 25: Neural networks (ANNs) have the potential to represent a wide range of phenomena, such as:",
        "Chunk 26: In this paper, a novel model of solid under mechanical couplings, known as Reaction Neural Networks (RNNs), is proposed",
        "Chunk 27: The aim of this paper is to replace the following:",
        "Chunk 28: In this paper, we present a new data-driven approach to the buckling stiffness model (BVP) for materials, based on",
        "Chunk 29: The aim of this paper is to introduce a new class of methods for data-driven computing.",
        "Chunk 30: ANNs are 'black-box' solvers that can be used in the context of classical buckling theory (BVP).",
        "Chunk 31: The aim of this paper is to train a new class of artificial intelligence (ANNs) based on the principles of thermodynamics.",
        "Chunk 32: The goal of this paper is to improve the prediction accuracy of neural networks.",
        "Chunk 33: The aim of this paper is to develop a new approach for the training and prediction of thermodynamical laws in micromechanical simulations.",
        "Chunk 34: In our work, we have developed an artificial neural network (ANN) that can be used to constrain the free-to-free relationships between stresses",
        "Chunk 35: In this paper, we present a new method for the calculation of the derivatives of an anti-Stokes neural network (ANN",
        "Chunk 36: In this paper we present a new problem for the training of artificial neural networks (ANNs).",
        "Chunk 37: In this paper we propose a new architecture for artificial neural networks (ANNs) based on the Thermodynamics-based",
        "Chunk 38: The aim of this paper is to provide an overview of the state-of-the-art in the design and implementation of",
        "Chunk 39: In this paper, we consider the training of artificial neural networks (ANNs) for the modelling of three-dimensional elasto-plastic",
        "Chunk 40: In the first phase of this work, we investigate the performance and robustness of thermodynamics-based artificial neural networks (TANNS), Section 4.",
        "Chunk 41: This manuscript is copyrighted and may not be reproduced in any form without the written permission of",
        "Chunk 42: The coefficients of the Cauchy stress tensor are as follows:",
        "Chunk 43: The second law of thermodynamics can be formulated in",
        "Chunk 44: BBC Sport takes a look at some of the key",
        "Chunk 45: The entropy inequality between the energy equation (1) and the entropy inequality (2) is given as the following:",
        "Chunk 46: p(es - é) + a VSymy",
        "Chunk 47: The thermal dissipation rate and the mechanical dissipation rate are both non-negative.",
        "Chunk 48: The following is a list of some of the most",
        "Chunk 49: In this paper, we are going to solve the equation for the internal energy of a material, i.e., its entropy.",
        "Chunk 50:  eS + a -D.",
        "Chunk 51: The coefficients of the coefficients of the coefficients of the coefficients of the coefficients of the coefficients of the",
        "Chunk 52: A finite-strain is a tensor that represents the state of a system.",
        "Chunk 53: The material response to a small strain regime is investigated in the context of a large class of materials and an updated Lagrangian scheme.",
        "Chunk 54:     ",
        "Chunk 55: The winning numbers in Saturday's drawing of the",
        "Chunk 56: BBC Sport looks at some of the best images from",
        "Chunk 57: The arbitrariness of $, é",
        "Chunk 58: BBC Sport looks at some of the key statistics behind",
        "Chunk 59: BBC Sport takes a look back at some of the",
        "Chunk 60: The coefficients of the Bose-Einstein condensate (B",
        "Chunk 61: China's National People's Congress (NPC",
        "Chunk 62: In this paper we present the results of a study",
        "Chunk 63: The equation for isothermal process, E - Se = F(,,Z), which is the Legendre transform",
        "Chunk 64: Find out more at www.bbc.co.",
        "Chunk 65: Researchers at the University of California, Los Angeles,",
        "Chunk 66: In this paper, we study the relationship between artificial neural networks (ANNs) and graph-based models.",
        "Chunk 67: This paper presents a direct approach to the problem of model-based plasticity.",
        "Chunk 68: AE Aa AE Aa Aa Aa",
        "Chunk 69: A look at some of the best images from this",
        "Chunk 70: Figure 1: Examples of direct, black-box (a) and informed (b) neural networks for",
        "Chunk 71: In this paper, we present a novel artificial intelligence (ANN) scheme for predicting the stress increment in a precedent strain.",
        "Chunk 72: guided, informed and path-dependency-based artificial neural networks (ANNS) have been developed for the modelling of material properties, i.e.",
        "Chunk 73: In this chapter, we will see how the stress at the precedent state, d', is considered to be an ANN state variable.",
        "Chunk 74: We present two new classes of artificial intelligence-based material models, one for temperature-dependent material response and one for non-temperature-dependent material response",
        "Chunk 75: In this paper, we present a new framework for artificial neural networks (ANNs).",
        "Chunk 76: In this paper, we develop a novel material model for topological acoustic networks (TANNS) based on the idea of two potential functions.",
        "Chunk 77: In this paper, we develop a new method for predicting the behaviour of materials, called TANNS, based on the assumption that the",
        "Chunk 78: This paper presents a novel strain-induced material state model (TANNS), which can be used to study the behaviour of a variety of materials.",
        "Chunk 79: In this paper, we derive a set of outputs from a set of inputs, each of which consists of 8 variables.",
        "Chunk 80: In this paper, we present a new class of artificial neural networks, in which the main interest of the network is the stress increment.",
        "Chunk 81: In the first part of this paper, we show how to train an artificial neural network (ANN) in the presence of a perturbation (paragraph 2.8).",
        "Chunk 82: Figure 1: At AE Aa 9a, the",
        "Chunk 83: The following table shows the basic concepts of artificial neural networks (ANNs).",
        "Chunk 84: An ANN is a signal processing unit.",
        "Chunk 85: An ANN is a network composed of several hidden layers.",
        "Chunk 86: To layer (I) according to (I-1)",
        "Chunk 87: Figure 9 shows the relationship between the outputs of node k, at layer (I), and the activation function of layer.",
        "Chunk 88: The functions of the output layer, A(out), and the activation function",
        "Chunk 89: In this paper, we present a new method for the estimation of the mean error between a benchmark, 0, and a prediction, O,",
        "Chunk 90: The following table shows the errors that can be caused",
        "Chunk 91: propagated to the nodes in the hidden layers and used to calculate",
        "Chunk 92: Researchers at the University of California, Los Angeles, have",
        "Chunk 93: Function af wP-R-w =",
        "Chunk 94: The ANN is trained on a subset of the input-output data-set where E is the so-called learning rate.",
        "Chunk 95: An adjoint neural network (ANN) is a machine-learning algorithm that trains an approximator of a problem, e.g.",
        "Chunk 96: The periodic table of the periodic table contains the properties of the",
        "Chunk 97: BBC News NI takes a look at some of the",
        "Chunk 98: In our paper, we present a solution to a well-known problem in artificial neural networks (ANNs).",
        "Chunk 99: The first-order vanishing gradient is a function of the loss function with respect to a certain weight.",
        "Chunk 100: In this section, we are going to look at some of the most",
        "Chunk 101: A tanh(:) max(0,",
        "Chunk 102: A' A' A' A' A'",
        "Chunk 103: Some of the most common activation functions and their first",
        "Chunk 104: functions and their derivatives-that is, the logistic (sigmoid) function, the hyperbolic tangent, the Rectified Linear",
        "Chunk 105: In this paper, we show how the first-order gradient function can be used to train neural networks.",
        "Chunk 106: The coefficients of the activation functions Z and ReLU are given by the coefficients of the sigmoid.",
        "Chunk 107: In this paper, I present a solution to the vanishing gradient problem of the ZLU.",
        "Chunk 108: In this paper, we are going to look at a function called TANNS, which has three main features: first, it has a first-order vanishing gradient for Z  0,",
        "Chunk 109: In this paper, we will consider a hidden layer, with activation function A and N, nodes.",
        "Chunk 110: The winning numbers in Saturday evening's drawing of",
        "Chunk 111: In this paper, we show how to use the ANN to compute the outputs of a set of",
        "Chunk 112: The winning numbers in Saturday evening's drawing of",
        "Chunk 113: Consider the following loss function L= WoLo + W",
        "Chunk 114: Figure 5: ANN which takes as input x and returns O1 = x2 and",
        "Chunk 115: In this paper, we present a new method for the estimation of the weights and biases of",
        "Chunk 116: To License This Clip, Click Here: http://",
        "Chunk 117: BBC Wales Sport takes a look at some of the",
        "Chunk 118: BBC Sport takes a look back at some of the",
        "Chunk 119: A gradient descent algorithm for the activation function A is presented.",
        "Chunk 120: In this article, I will show you how to select activation functions that do not have second-order vanishing gradients.",
        "Chunk 121: In this paper we discuss the architecture of Thermodynamic-based Artificial Neural Networks (TANNS).",
        "Chunk 122: The tensor adversarial network (TANN) is a set of equations that can be used to predict the behaviour of",
        "Chunk 123: This paper presents the results of (a) a",
        "Chunk 124: Key words: kinematics, temperature, energy potential,",
        "Chunk 125: The following table shows the results for the following equations:",
        "Chunk 126: BBC Sport takes a look back at some of the",
        "Chunk 127: Northern Ireland's Deputy First Minister Martin McGuinness has",
        "Chunk 128: All photographs  AFP, EPA, Getty Images",
        "Chunk 129: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 130: Figure 6: Architecture of TANNS: general case (a) and for isothermal processes.",
        "Chunk 131: The ANNs of the TANNS architecture are given as follows: SNNy, SNNe, and SNNF.",
        "Chunk 132: This paper presents a novel model for the prediction of the energy potential of a material.",
        "Chunk 133: The Helmholtz-Zentrum fr Naturkunde (Zentrum fr Naturkunde) is one of the",
        "Chunk 134: In this paper, we show how dissipation, from expression (13), affects the learning of artificial neural networks (ANNs).",
        "Chunk 135: This paper presents a new approach to the study of elastic forces, one that brings two important benefits: first, it brings the information to distinguish between reversible and",
        "Chunk 136: In this paper, we describe the techniques used to generate material data TANNS.",
        "Chunk 137: The following procedure is used to generate data for hyper-plastic von Mises models with kinematic hardening (Houlsby and Puzrin, 2000,",
        "Chunk 138: We have imposed the following restrictions on the TANN architecture:",
        "Chunk 139: In this paper, we consider the Hypo-plasticity hypothesis for the generation of data encoded in TANNs.",
        "Chunk 140: This paper presents a case study to test TANNS against materials with a",
        "Chunk 141: The non-linear incremental strain-rate relation for strain-rate independent materials, undergoing 15 infinite",
        "Chunk 142: The Latin phrase ZaF-5 has been changed to",
        "Chunk 143:     ",
        "Chunk 144: In this paper, we present a new method for",
        "Chunk 145: This paper presents the results of the Zieglerss",
        "Chunk 146:     ",
        "Chunk 147:     ",
        "Chunk 148: Match reports from the weekend's Premier League games",
        "Chunk 149: The following table shows the relationship between the following indices:",
        "Chunk 150: quantity (scalar or tensorial, depending",
        "Chunk 151: BBC Wales Sport takes a look at some of the",
        "Chunk 152:     ",
        "Chunk 153: This paper presents the results of the aXk 4.1.2 (hypo-plasticity",
        "Chunk 154: In this paper we present a h'plastic model, which can be used to derive material formulations",
        "Chunk 155: The relaxation strain rate (z) and stress increment",
        "Chunk 156: BBC Sport takes a look back at some of the",
        "Chunk 157:     ",
        "Chunk 158: The strain rate tensors for simple shear are given as the following:",
        "Chunk 159: Researchers at the University of Bristol have developed a method for the generation of strain and stress data for a variety of polymers, including",
        "Chunk 160: , , , ",
        "Chunk 161: The state of a strain at a new time t + At, i.e., o1+",
        "Chunk 162:     ",
        "Chunk 163: In the case of martensitic steel, data are generated in the form of internal variables Si, deformation 8, and stress to represent the material state at",
        "Chunk 164: In this paper, we report on the state-of-the-art of artificial neural networks (ANNs).",
        "Chunk 165: strain-stress loading paths for artificial neural networks (ANNs) are proposed.",
        "Chunk 166: In this paper, we present a novel artificial neural network (ANN) for the modelling of plastic waste.",
        "Chunk 167: In this paper we derive the strain increment for a set of Nsamples ordered pairs (E,8,Ae; EA), from which the corresponding strain increment is generated",
        "Chunk 168: In this work we investigate the properties of three-dimensional (3-D) amorphous silicon nitrides in terms of yield surface and hypo- plasticity.",
        "Chunk 169: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 170: BBC Sport takes a look at some of the key",
        "Chunk 171: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 172: The world's most powerful 4x4 vehicle",
        "Chunk 173: BBC Sport takes a look back at some of the",
        "Chunk 174: Strain Arain 600 - Strain p.pAr",
        "Chunk 175: BBC Sport takes a look back at some of the",
        "Chunk 176: The winners of this year's Isle of Man",
        "Chunk 177: The winning numbers in Saturday evening's drawing of",
        "Chunk 178: The BBC's weather forecast for the weekend,",
        "Chunk 179: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 180: A selection of photos from around the world this week",
        "Chunk 181: The winning numbers in Saturday evening's drawing of",
        "Chunk 182: ,,,,",
        "Chunk 183: BBC Sport takes a look at some of the best",
        "Chunk 184:     ",
        "Chunk 185: Figure 7: Sampling for material case H-1 (cf.",
        "Chunk 186: The results of this study are presented in terms of the following:",
        "Chunk 187: A new class of neural networks has been proposed for the study of plasticity in materials.",
        "Chunk 188: In paragraph 4 of this paper, we introduce a new class of material models, called hypo-plastic material models.",
        "Chunk 189: In this paper, we show how to train networks on a set of hyper-parameters (i.e.",
        "Chunk 190: In this Section, we present a validation set for a deep feed-forward neural network that is trained on a coarse-grained dataset",
        "Chunk 191: In this paper, we study the properties of topological insulators (TANNs) in terms of",
        "Chunk 192: The winning numbers in Saturday evening's drawing of",
        "Chunk 193: The coefficients of the shear parameters Z, 2 D, K, G, Ep",
        "Chunk 194: The coefficients of the strain tensors e and Z are used to derive the yield surface of",
        "Chunk 195: Material parameters of 3D elasto-",
        "Chunk 196: All photographs  AFP, EPA, Getty Images",
        "Chunk 197: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 198: This paper presents the results of a training scheme for the estimation of the deformation rate of a deformable object",
        "Chunk 199: In this paper we present the results of a large-scale numerical study of the properties of two-dimensional (2-D) microstructures.",
        "Chunk 200: In this paper, we show how to train a Markov chain Monte Carlo Monte Carlo model.",
        "Chunk 201: In this paper, we present a novel spatio-temporal network architecture (SNN) for the analysis of loss functions in the brain.",
        "Chunk 202: In this paper, we train two sub-networks of neurons in the ventral striatum.",
        "Chunk 203: In this paper, we show how to recover plasticity-like phenomena in a Petri dish.",
        "Chunk 204: In this paper, we train a topological artificial intelligence network (TANN) to predict the material response to a random loading path.",
        "Chunk 205: The following table shows the mean and standard deviation of",
        "Chunk 206: BBC Sport takes a look back at some of the",
        "Chunk 207: The winning numbers in Saturday evening's drawing of",
        "Chunk 208: A look back at some of the most memorable moments",
        "Chunk 209: Nasa's Deep Space Observatory (DSO)",
        "Chunk 210: Le 1 Afitrain Afival le-2",
        "Chunk 211: France's President Francois Hollande has said he will",
        "Chunk 212: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 213: Figure 8: Errors in terms of the adimensional Mean Absolute Error (MAE) of the predictions of TANN (",
        "Chunk 214: In this paper we introduce a new type of artificial intelligence (ANN), the topological adversarial network (TANNS).",
        "Chunk 215: A novel artificial neural network (ANN) has been developed to study the activity of neurons in the brain.",
        "Chunk 216: In this paper we present the results of our work on the prediction of user-generated content (UGC) using artificial neural",
        "Chunk 217: In this paper, we examine the impact of the",
        "Chunk 218: The winning numbers in Saturday evening's drawing of",
        "Chunk 219: The following table lists the most common loading paths used",
        "Chunk 220: TANN is the world's best-selling",
        "Chunk 221: BBC Sport takes a look at some of the key",
        "Chunk 222: BBC Sport takes a look at some of the key",
        "Chunk 223: The effect of stress on the brain's ability",
        "Chunk 224: TANN has launched a new range of women'",
        "Chunk 225: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 226: Figure 9: Predictions of TANN for a uniaxial loading path, compared with the target model",
        "Chunk 227: In this paper we present a new approach for predicting the material response for cyclic isotropic loading paths, based on (a)",
        "Chunk 228: In this paper, we compare the stress predictions of TANNS and ANNs with the stress predictions of standard ANNS.",
        "Chunk 229: In this paper, we compare the performance of two networks, one based on",
        "Chunk 230: All photographs  Getty Images, AFP, EPA",
        "Chunk 231: The scheme described in clause (a) of the",
        "Chunk 232: Figure 10: (a) ANN architecture.",
        "Chunk 233: TANN Ak,train TANN Abival ",
        "Chunk 234: France's women's rugby sevens team",
        "Chunk 235: ANN Acitrain le-4 ANN Ad",
        "Chunk 236: Training of ANNs compared with TANNS evaluated with respect to",
        "Chunk 237: path nT ni Agi = Assgn",
        "Chunk 238: Find out more at www.bbc.co.uk/bbc",
        "Chunk 239: We show that TANNS predicts the free-energy and dissipation rate of the material response to a variety of stresses and inelastic strains.",
        "Chunk 240: In this paper, we study the performance of artificial neural networks (ANNs) on predicting the material response to a strain.",
        "Chunk 241: In this paper, we show how standard ANNs can be used to train experimental systems.",
        "Chunk 242: In this paper, we show that a new class of artificial neural networks (ANNs), called TANNS, can be used to predict",
        "Chunk 243: In this paper we present a novel model for predicting the response of a strain to a given chemical reaction, called TANNS.",
        "Chunk 244: In this paper, we present the results of a uni-axial loading scenario, in Figures S1 and S2, for material case H-1 (perfect plasticity).",
        "Chunk 245: In this paper, we present TANNS, a new approach for the prediction of the Jacobian equation.",
        "Chunk 246: In this paper, we compare the performance of tensor-ANNs (TANNs) and standardANNs",
        "Chunk 247: le3 e--3 1.0 model 3.0 model",
        "Chunk 248: A look back at some of the most memorable moments",
        "Chunk 249:     ",
        "Chunk 250: BBC Sport takes a look back at some of the",
        "Chunk 251: TANN ANN 51 () le-3 Ep",
        "Chunk 252: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 253: le3 el model TANN ANN fue",
        "Chunk 254: BBC Sport takes a look at some of the best",
        "Chunk 255: A-model -4 TANN 5 TANN",
        "Chunk 256: The winning numbers in Sunday evening's drawing of",
        "Chunk 257: E1 (-) Ep. 5p -",
        "Chunk 258: The following table lists the most common strains of strain",
        "Chunk 259: A look at some of the top stories of the",
        "Chunk 260: BBC Sport takes a look back at some of the",
        "Chunk 261: BBC Sport takes a look at some of the best",
        "Chunk 262:     ",
        "Chunk 263: BBC Sport takes a look back at some of the",
        "Chunk 264: The stress, energy, and dissipation predictions of TANNS and standard ANNs are compared in Figure 12.",
        "Chunk 265: The world's smallest car, the le3",
        "Chunk 266: ,,,,",
        "Chunk 267: The winning numbers in Saturday evening's drawing of",
        "Chunk 268: E1 () le-3 63 (-)",
        "Chunk 269: The world's smallest car, the le3",
        "Chunk 270: A selection of photographs from around the world this week",
        "Chunk 271: The winning numbers in Saturday evening's drawing of",
        "Chunk 272: BBC Sport takes a look back at some of the",
        "Chunk 273: BBC Sport takes a look at some of the best",
        "Chunk 274: A look back at some of the most memorable moments",
        "Chunk 275: BBC Sport takes a look back at some of the",
        "Chunk 276: BBC Sport looks at some of the key statistics from",
        "Chunk 277: E3 (c) strain increment As = 1",
        "Chunk 278: All photographs courtesy of Getty Images and AFP.",
        "Chunk 279: BBC Sport takes a look back at some of the",
        "Chunk 280: Figure 13: Comparison of the stress predictions of TANNS and standard ",
        "Chunk 281: This table shows the predicted values for the cyclic loading path of",
        "Chunk 282: BBC Sport takes a look at some of the best",
        "Chunk 283: BBC Sport takes a look at some of the best",
        "Chunk 284: The winning numbers in Saturday evening's drawing of",
        "Chunk 285: BBC Sport takes a look at some of the key",
        "Chunk 286: le-3 (a) strain increment As = 1",
        "Chunk 287: BBC Sport takes a look back at some of the",
        "Chunk 288: BBC Sport takes a look back at some of the",
        "Chunk 289:     ",
        "Chunk 290: BBC Sport takes a look at some of the key",
        "Chunk 291: le-3 (a) strain increment As = 1",
        "Chunk 292: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 293: BBC Sport takes a look back at some of the",
        "Chunk 294: BBC Sport takes a look back at some of the",
        "Chunk 295: The BBC's science programme takes a look at",
        "Chunk 296: The world's biggest retailer, Amazon, has",
        "Chunk 297: The winning numbers in Saturday evening's drawing of",
        "Chunk 298: BBC Sport looks at some of the key statistics from",
        "Chunk 299: Figure 14: Comparison of the internal variable predictions of TANNS and standard ANNs with respect",
        "Chunk 300: This table shows the predicted changes in the odds of",
        "Chunk 301: TANN 3 TANN & ANN ANN",
        "Chunk 302: BBC Sport takes a look at some of the key",
        "Chunk 303: Match report from the Premier League match between Arsenal and",
        "Chunk 304: le-4 (a) strain increment As = 1",
        "Chunk 305: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 306: The winning numbers in Saturday evening's drawing of",
        "Chunk 307: BBC Sport takes a look at some of the key",
        "Chunk 308: BBC News NI takes a look at some of the",
        "Chunk 309: TANN has announced the launch of TANN T",
        "Chunk 310: The winning numbers in Saturday evening's drawing of",
        "Chunk 311: A look back at some of the most memorable moments",
        "Chunk 312: Here's a look at some of the more",
        "Chunk 313: A selection of the best-selling children's",
        "Chunk 314: BBC Sport takes a look at some of the best",
        "Chunk 315: The winning numbers in Sunday evening's drawing of",
        "Chunk 316: Figure 15: Comparison of the energy and dissipation rate predictions of",
        "Chunk 317: In this paper, we present a new method for predicting smooth material behavior, TANNS, with respect to the properties",
        "Chunk 318: The energy potential and dissipation rate of a low-cost",
        "Chunk 319: We consider the following equation for the differential equation 2 +G",
        "Chunk 320: The results of this study are presented in terms of the mean and deviatoric stresses, p and 9, and deformations, Ep and e.",
        "Chunk 321: In this paper, we present a new method for the determination of the inelastic strain (TANNS) in a hyper- plastic case.",
        "Chunk 322: The following is a description of the TANN network:",
        "Chunk 323: In this paper, we compare the performance of two types of artificial neural networks (ANNs): the hyper-plastic",
        "Chunk 324: The following table shows the results of the TANNs' predictions for the response of the Higgs boson",
        "Chunk 325: In our series of letters from African journalists, filmmaker",
        "Chunk 326: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 327:     ",
        "Chunk 328: Match reports from the weekend's Premier League games",
        "Chunk 329: E1 (-) le-3 (-)",
        "Chunk 330: le2 le-2 model 2 ANN TANN",
        "Chunk 331:     ",
        "Chunk 332: BBC Sport takes a look back at some of the",
        "Chunk 333: BBC Sport takes a look back at some of the",
        "Chunk 334: le-2 (a) strain increment As = 2 X 10-3.",
        "Chunk 335: In this paper, we report the results of a novel finite element method (TANNS) for the prediction of stresses during cyclic loading.",
        "Chunk 336: BBC Sport takes a look at some of the best",
        "Chunk 337: BBC Sport takes a look back at some of the",
        "Chunk 338:     ",
        "Chunk 339: BBC Sport takes a look at some of the best",
        "Chunk 340: The winning numbers in Saturday evening's drawing of",
        "Chunk 341: le-3 2 (-) le-3 a principal",
        "Chunk 342: A selection of the best photographs from around the world",
        "Chunk 343:     ",
        "Chunk 344: A look back at some of the more memorable moments",
        "Chunk 345: BBC Sport takes a look at some of the key",
        "Chunk 346: le-3 (a) principal external variables.",
        "Chunk 347: BBC Sport takes a look at some of the best",
        "Chunk 348: BBC Sport takes a look back at some of the",
        "Chunk 349: BBC Sport takes a look back at some of the",
        "Chunk 350: Figure 17: Comparison of the predictions of TANNS and standard ANNs with",
        "Chunk 351: In this paper, we present a novel approach to the training of artificial neural networks (TANNS) based on",
        "Chunk 352: In this paper, we consider the use of artificial noise to train TANNS and ANNs.",
        "Chunk 353: In this paper we present the results of a series of experiments on the noise levels of the Earth's atmosphere.",
        "Chunk 354: In this paper, we study the effect of noise on the training of fighter pilots in two different scenarios, i.e.",
        "Chunk 355: The aim of this study is to investigate the impact of added noise on the training of neural networks.",
        "Chunk 356: In this paper, we evaluate the predictions of TANNS with respect to the training and validation data-sets.",
        "Chunk 357: In this paper we show that TANNS are unable to learn the noised signal, i.e., Ad.",
        "Chunk 358: In this paper we present a new method for training and validation of experimental data-sets.",
        "Chunk 359: le 1 Afitrain Alival le-2 F",
        "Chunk 360:     ",
        "Chunk 361: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 362: The results of this study are presented in the following tables: Figure 18: Errors of the predictions of TANN, as the training",
        "Chunk 363: The training is carried out over a long period of time and is carried out",
        "Chunk 364: BBC Wales Sport takes a look at some of the",
        "Chunk 365: The winning numbers in Saturday evening's drawing of",
        "Chunk 366: In this paper, we show how to train a network to learn the laws of thermodynamics.",
        "Chunk 367: In this paper, we present a new approach to the analysis of inelastic strain data, which is based on a new approach to",
        "Chunk 368: In this paper, we show how standard ANNs can be trained to make predictions in noisy data-sets.",
        "Chunk 369: This paper presents the results of a coarse-grained training of standard ANNs with respect to the",
        "Chunk 370: BBC Sport takes a look at some of the key",
        "Chunk 371: We have compared the performance of two artificial neural networks (ANNs): TANNS and standard ANNs.",
        "Chunk 372: In this paper, we study the effect of noise on the performance of ANNs.",
        "Chunk 373: In this paper, we study the robustness of tensor learning networks (TANNs) to noise.",
        "Chunk 374: A new class of artificial neural networks were proposed to replace laws and predict the material response at the material point level",
        "Chunk 375: In this paper, we present a novel method for the derivation of the derivative of a neural network with respect to its inputs, i.e.",
        "Chunk 376: In this paper, a new problem has been solved in the field of artificial neural networks (ANNs).",
        "Chunk 377: This article is part of a series of papers on",
        "Chunk 378: The world's most powerful car, the Tesla",
        "Chunk 379:     ",
        "Chunk 380: C0xxxc0xxx006 A -2.0",
        "Chunk 381:     ",
        "Chunk 382: (Nos  jours, ",
        "Chunk 383: le-3 le-2 1.25 model 0.0 model -",
        "Chunk 384: A look back at some of the most memorable moments",
        "Chunk 385: The winning numbers in Saturday evening's drawing of",
        "Chunk 386: A look at some of the key stories of the",
        "Chunk 387: The winning numbers in Saturday evening's drawing of",
        "Chunk 388: Turkey's President Recep Tayyip Erdogan and his",
        "Chunk 389: (a) strain increment As = 1 X 10-2.",
        "Chunk 390: TANN is a new approach to the theory ofStokes-Stokes-Stokes-Stokes reactions.",
        "Chunk 391: In this paper, we present a new approach to the training of artificial neural networks (ANNs), called tensor annealing networks (TANNs).",
        "Chunk 392: In this paper, we report the results of a new approach to the prediction of loading paths based on tensor annealing neural networks (TAN",
        "Chunk 393: In this paper we present a new model for the prediction of stresses at the material point level, the tensor angular momentum network (TANN).",
        "Chunk 394: In this paper, we report the development of TANNS, a novel method for the prediction of the stability of",
        "Chunk 395: TANN is a novel approach to the study of solid-state heat transfer.",
        "Chunk 396: The authors would like to acknowledge the support of the European Research Council (ERC) under the European Union",
        "Chunk 397: In this paper, we investigate the performance and influence of different activation functions on the computational time to train an ANN with",
        "Chunk 398: An artificial neural network (ANN) is used to train a gradient descent algorithm on a set of samples, i.e.",
        "Chunk 399: Table 5: Set of activation functions considered to investigate the performance of the network with outputs 0= x and VIo = 2x, with I = x, in the framework of first and",
        "Chunk 400: Function Z range S(z) '(z",
        "Chunk 401: BBC Sport takes a look at some of the key",
        "Chunk 402: BBC Sport takes a look at some of the key",
        "Chunk 403: ReLUoS24z z20 ",
        "Chunk 404: A look back at some of the most memorable moments",
        "Chunk 405:     ",
        "Chunk 406:     ",
        "Chunk 407:     ",
        "Chunk 408: BBC Sport takes a look back at some of the",
        "Chunk 409:     ",
        "Chunk 410:     ",
        "Chunk 411: , , , ",
        "Chunk 412:     ",
        "Chunk 413:     ",
        "Chunk 414: For each tested activation function, Table 6 shows the adimensional Mean Absolute Error (",
        "Chunk 415: In this paper, we show that activation functions with terms of higher degree, i.e.",
        "Chunk 416: This paper presents the results of the ANN-ELUZ training scheme.",
        "Chunk 417: In this paper we show that ANN is able to perform the activation function of O and Vro with unseen data.",
        "Chunk 418: The following table shows the number of epochs in",
        "Chunk 419: BBC Sport takes a look back at some of the",
        "Chunk 420: ReLU2 0.061 0.0241 0.0371 148",
        "Chunk 421: BBC Sport takes a look at some of the best",
        "Chunk 422: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 423: BBC Sport takes a look at some of the best",
        "Chunk 424: BBC Sport takes a look at some of the best",
        "Chunk 425: BBC Sport takes a look at some of the key",
        "Chunk 426: The winning numbers in Saturday evening's drawing of",
        "Chunk 427: The winning numbers in Saturday evening's drawing of",
        "Chunk 428: BBC Sport takes a look at some of the best",
        "Chunk 429: ,,,,",
        "Chunk 430: The winning numbers in Saturday evening's drawing of",
        "Chunk 431: The winning numbers in Saturday evening's drawing of",
        "Chunk 432: BBC Sport takes a look at some of the best",
        "Chunk 433: BBC Sport takes a look at some of the best",
        "Chunk 434: BBC Sport takes a look at some of the best",
        "Chunk 435: Match reports from the Champions League quarter-final between",
        "Chunk 436: All photographs  AFP, EPA, Getty Images",
        "Chunk 437:     ",
        "Chunk 438: BBC Sport takes a look back at some of the",
        "Chunk 439: ELU, 2 ELU, ELUas",
        "Chunk 440: BBC Sport takes a look at some of the key",
        "Chunk 441: E, d 0 ELU, 0.2 -1",
        "Chunk 442: Match details, team news and stats for Saturday'",
        "Chunk 443: BBC Sport takes a look back at some of the",
        "Chunk 444: Figure 20: Comparison of different activation functions for the prediction of the primary output, x2 (a), and secondary",
        "Chunk 445: We derive the following non-linear incremental relations  = OsF",
        "Chunk 446: BBC Sport takes a look at some of the key",
        "Chunk 447: The following is a list of the most commonly used",
        "Chunk 448: dEioEu dEiO5 8",
        "Chunk 449: In this paper we present a new theory of the",
        "Chunk 450: The coefficients of the first-order dissipation of a",
        "Chunk 451: The homogeneity of the Euler's relation OD D",
        "Chunk 452: This table shows the number of letters in the word",
        "Chunk 453: The Legend, conjugate to Xi, is assumed to be D homogeneous first-order function in 5i",
        "Chunk 454: The coefficients of the coefficients of Legendre transform are",
        "Chunk 455: The consistency equation is given as the following:",
        "Chunk 456: Oy dy dy y - + B=0",
        "Chunk 457: BBC Sport takes a look back at some of the",
        "Chunk 458: We derive the coefficients of flow (CE Co A",
        "Chunk 459: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 460:     ",
        "Chunk 461:     ",
        "Chunk 462: BBC Sport takes a look back at some of the",
        "Chunk 463: dy dy y B = O",
        "Chunk 464: In this week's Mathematica, we look at",
        "Chunk 465: BBC Sport takes a look back at some of the",
        "Chunk 466: Moo E = Ml-0 5, with ",
        "Chunk 467: A Bc a Bc a Bc a",
        "Chunk 468: The following table lists the most common denominators in the",
        "Chunk 469: The following table shows the results of a test carried out",
        "Chunk 470: Match reports from the weekend's matches in the",
        "Chunk 471: In: Proceedings of the 11th International Conference on",
        "Chunk 472: Nitka, M., Combe, G., Dascalu, C., Desrues, J.",
        "Chunk 473: Mathematical problems in the Mechanics of Composite Materials.",
        "Chunk 474: In: Houlsby, G. T., Puzrin, A., and Nguyen, G.",
        "Chunk 475: The following papers have been published in refereed journals over the past 10 years.",
        "Chunk 476: A study on the effects of an explosion in the Pantheon of Rome.",
        "Chunk 477: The importance of hemmoelydromechanica couplings and microstructure to strain in 3da with application to seismic faults.",
        "Chunk 478: A cosserat breakage mechanics model for brittle, granular media, Journal of the Mechanics and Physics of Solids.",
        "Chunk 479: Geron, A., and Mitchell, T.",
        "Chunk 480: In: Cybenko, G., and Chen, T.",
        "Chunk 481: Neural networks have been used to model the behaviour of materials for more than 20 years.",
        "Chunk 482: Neural network models for rate-dependent materials.",
        "Chunk 483: Researchers at the Massachusetts Institute of Technology (MIT) and the University of California, Los Angeles (UCLA) have been working on new ways to study the properties of",
        "Chunk 484: Zhang, D., Xu, K., Huang, D., Darve, E.",
        "Chunk 485: The potential of recurrent neural networks for model path dependent plasticity, Journal of the Mechanics and Physics of Solids (2020) 103972.",
        "Chunk 486: Researchers at the University of California, Berkeley, have developed a novel deep neural network-based model for anisotropic ",
        "Chunk 487: Researchers from the Massachusetts Institute of Technology (MIT) and the Massachusetts Institute of Technology (MIT) are working together to develop new",
        "Chunk 488: Data-driven computational mechanics, Computer Methods in Applied Mechanics",
        "Chunk 489: In: Ibanez, R., Borzacchiello, D., Aguado, J.,",
        "Chunk 490: A manifold learning approach to data-driven computational elasticity and inelasticity, Archives of Computational Methods in Engineering 25 (2018)",
        "Chunk 491: In: Raissi, M., Perdikaris, P., and Karniadakis, G.",
        "Chunk 492: In: Maugin, G. A., and Muschik, W.",
        "Chunk 493: Y. H Hu, J.-N. Hwang, Handbook of neural network processing, Handbook of Machine Learning",
        "Chunk 494: A framework for data-driven analysis of materials under uncertainty.",
        "Chunk 495: In: Ziegler H., Bogacki P., Shampine L."
    ]
}