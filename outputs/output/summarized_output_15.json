{
    "original_text_chunks": [
        "UNIVERSITE DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématique et Informatique Année universitaire : 2018-2019 Interrogatoire : Théorie et pratique du Datamining",
        "Durée : 75 mn Exercice 01 (03 pts : 10 Mn) : Répondez brièvement aux questions suivantes : 1. Expliquer le fonctionnement de la méthode de rééchantionnage Bootstrap ?. 2. Plusieurs domaines sont la base des techniques de Datamining. Citez cinq domaines ? 3. Es ce qu'une règle d' association avec un support et une confiance acceptable veut dire que cette règle",
        "est pertinente ? expliquez notamment avec des exemples ? Exercice 02 (08 pts : 40 Mn) : Le tableau suivant contient des données sur les No Doublant Série Mention Classe",
        "résultats obtenus par des étudiants de Tronc Commun 01 Non Maths ABien Admis (première année à Université). Chaque étudiant est 02 Non",
        "Techniques ABien Admis décrit par 3 attributs : Est-il doublant ou non, la série 03 Oui Sciences ABien Non Admis",
        "du Baccalauréat obtenu et la mention. Les étudiants 04 Oui Sciences Bien Admis sont répartis en deux classes : Admis et Non Admis. 05 Non",
        "Maths Bien Admis On veut construire un arbre de décision à partir des 06 Non Techniques Bien",
        "Admis données du tableau, pour rendre compte des éléments 07 Oui Sciences Passable Non Admis qui influent sur les résultats des étudiants en Tronc 08 Oui",
        "Maths Passable Non Admis Commun. Les lignes de 1 à 12 sont utilisées comme 09 Oui Techniques Passable Non Admis données d'apprentissage. Les lignes restantes (de 13 à 10 Oui",
        "Maths TBien Admis 11 Oui",
        "Techniques TBien Admis 16) sont utilisées comme données de tests. 12 Non Sciences",
        "TBien Admis 1. Utiliser les données d'apprentissage pour 13 Oui Maths Bien",
        "Admis construire I'(les) arbre(s) de décision en utilisant 14 Non Sciences ABien Non Admis l'algorithme ID3. Montrez toutes les étapes et 15 Non",
        "Maths TBien Admis formules de calcul. Dessinez l'arbre final. 16 Non",
        "Maths Passable Non Admis 2. Déduire de l'arbre trouvé la petite règle correspondante. 3. Classer l'instance N°17: Doublant-Oui, Série-Maths, Mention-ABien. Que remarquez-vous ? 4. Quels sont les résultats de test de l'arbre obtenu sur les données de tests ? déduisez le taux d'erreur ? En",
        "comparant les résultats obtenus, que suggérez-vous concernant l'arbre résultante ? 5. En se basant sur la comparaison et la suggestion de la question 5, que pouvez dire sur la prédiction de l'avenir des étudiants de tronc commun par rapport aux résultats obtenus au baccalauréat. Exercice 03 (04 pts : 10 Mn). : Soit l'ensemble D des entiers suivants : D= £2,5,8, 10, 11, 18, 20 j. On veut répartir les données de D en",
        "trois (3) clusters en utilisant l'algorithme Kmeans et la distance de manhathan 1/ Appliquez Kmeans en choisissant comme centres initiaux des 3 clusters respectivement : 8, 10 et 11. Montrez toutes les étapes de calcul. 2/ Donnez le résultat final et précisez le nombre d'itérations qui ont été nécessaires. 3/ Peut-on avoir un nombre d'itérations inférieur pour ce problème ? Discutez.",
        "Good To succeed in life one must have the courage to pursue what he wants CK Enseignant : M' K. Boudjebbour Page 1/1 1",
        "UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématiques et Informatique Année universitaire : 2018-2019 Corrigé type EFS S1 : Théorie et pratique du Datamining",
        "Exercice 01 (06 Pts) : a) Appliquer la classification hiérarchique ascendante sur l'ensemble X X 1 2 9 12 20 On va utilisé la distance de Manhattan entre instances : D(X,Y) = Li-Xi - Yil Et la distance minimale entre toutes les paires de données des 2 clusters (single link method) :",
        "Dsinglel(i.j) = MinxEi y€j D(X,Y) 0.5.PE Les tableaux suivants représentent les différentes distances Dsingle entre différents clusters : Etape 1 : 1 2 9 12 20",
        "1 1 8 11 19 2 7 10 18 Regroupement des clusters (1) et [2) en (1,2)",
        "9 3 11 12 8 Etape 2 :",
        "1,2 9 12 20 3.Pt 1,2 7 10 18 Regroupement des clusters [9) et (12) en [9,12) 9",
        "3 11 12 8 Etape 3: 1,2 9,12 20",
        "1,2 7 18 Regroupement des clusters (1,2) et [9,12) en (1,2,9,12) 9,12 8",
        "Etape 4 : 1,2,9,12 20 1,2,9,12 8 Regroupement des clusters (1,2,9,12) et (20) en (1,2,9,12,20)",
        "Dsingle Dendrogramme : b) L'inertie intra-cluster IA = Ek ENK D?(i,Gk) i: instance ; Gk : centroid du groupe k ; Nk : Nombre d'instance du groupe k",
        "1Pt Un regroupement en 2 clusters : CI=(1,2,9,12) centroid C1 =6 6 C2=(20) centroid C2 = 20 1.5.Pti",
        "IA= ((1-6)2+ (2-6)2+ (9-6)2+ (12-6)2)+ (20-20)2-86 Données 1 2 9",
        "12 20 Uni regroupement en 3 clusters : Cl-(1,2)-centroid C1=1,5 C2-19,12)centroid C2 = 10,5 et C3-(20)centroid C3 = 20 IA= ((1-1,5)2+ 2-1,5)4(9-105)4 (12-10,5)2)+ (20-20)2-5",
        "Donc le meilleur regroupement est celui de 3 clusters car son inertie intra-cluster IA est la plus petite. Enseignant : M' K. Boudjebbour Page 1/4 UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences",
        "Département de Mathématiques et Informatique Année universitaire : 2018-2019 Exercice 02 (09 Pts) : 11 11 7",
        "7 1) On calcul l'entropie sur l'ensemble des données : I(11,7)= log log - 0,964 0,5PE 18",
        "18 18 18 Ensuite on calcul le gain de chaque attribut : 6 6",
        "6 Gain (DegStr)= d1,72EDe5-0964G I(3, 3)+ I(3,3) + I(5,1))= 0,081 18 18 18",
        "6 6 6 Gain (HrSom)= d1,7-Edlisom)-096H I(6,0)+ I(1,5) + I(4,2))= 0,441 1Pt",
        "18 18 18 9 9",
        "Gain (Fum)= d17-E(um)-0.964-G I(4,5)+ I(7,2))= 0,086 18 18 Donc on choisit l'attribut < HrSom > avec le gain le plus grand (Gain-0.411) qui représente la racine de l'arbre, Donc l'arbre initial sera :",
        "HrSom 0.5.Pt Egal Supérieur Moins",
        "* Inst : 5, à 12 ??? - ??? Inst : 13 à 18 Yes",
        "Les valeurs Egal et Supérieur donnent deux valeurs de la classe, donc, il faut refaire le même travail (calcul du gain) pour l'ensemble des données S#13.4.9.10,15,16) et 515.611,2.7.8. I(SEg) =I(1,5)-0,650 2 Gain (SEg, DegStr)= I(1,5)-E(SEg, DegStr)= 0,650-C I(0,2)+ I(0,2) + 2 I(1,1))= 0,317",
        "6 6 6 3 3",
        "Pt Gain (SEg: Fum)= I(1,5)-E(SEg Fum)= 0,650-C I(0,3)+ I(1,2))= 0,191 6 6 HrSom Donc on choisit l'attribut < DegStr > avec",
        "Egal Supérieur K a le gain le plus grand (Gain-0.317), et l'arbre devient :",
        "DegStr Moins y ??? Petit ou Normal",
        "Fort Yes Inst : 13 à 18 A No",
        "Fum Non Oui I(SSup) I(4,2)-0,919 4",
        "No Yes Gain (SSup, DegStr)= I(4,2)-E(SSup, DegStr)= 0,919-614.1) 2 2",
        "- I(1,1)+ - I(2,0))= 0,252 HrSom 6 6 Gain (SSup, Fum)= I(4,2)-E(SSup: Fum) 1Pt",
        "Egal Supérieur 3 3 Moins",
        "4 = 0,919-C I(1,2)+ = I(3,0))= 0,495 DegStr Fum 6",
        "6 Petit ou Normal Fort Yes Oui",
        "Non Donc on choisit l'attribut < Fum > avec - 4 K",
        "le gain le plus grand (Gain-0.495), No Fum Yes DegStr",
        "et l'arbre final devient : Non Oui Petit ou Normal Fort K",
        "A No Yes No Yes",
        "2) Règle : (HrSom = Moins) ou ((HrSom + Moins) et (DegStr-Fort) et (Fum=Oui) ou ((Fum-Non) et (DegStr-Fort)) 1Pt Enseignant : M' K. Boudjebbour Page 2/4",
        "UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématiques et Informatique Année universitaire : 2018-2019 3) On applique l'ensemble test' T sur l'arbre de décision et on trouve la classe prédite :",
        "Instance DegStr HrSom Fum Classe réelle Classe prédite",
        "19 Petit Supérieur Oui Yes",
        "Yes 20 Fort Superieur Non",
        "Yes Yes 21 Petit Egal",
        "Non No No 22 Fort",
        "Egal Non Yes No 23",
        "Normal Supérieur Oui No Yes",
        "24 Petit Egal Oui No",
        "No Matrice de 1Pt Prédite (Yes) Prédite (No) Total",
        "confusion : Classe réelle (Yes) 2 1 3",
        "Classe réelle (No) 1 2 3 Total",
        "3 3 6 Taux d'erreur = b+c / n, Donc le taux d'erreur est : 2/6 = 0,3333 = 33,33 % 0,5.Pt: Précision = a/(a+c) = 66,66 % : représente le pourcentage des colopathies positivement prédites",
        "par rapport aux total des colopathies prédites 0.5.Pt. Spécificité = d/(c+d) = 66,66 % représente le pourcentage des non colopathies positivement prédite par rapport aux total des non colopathies réelles. 4) II faut calculer la distance entre l'instance No19 et les 18 autres instances tel que : D1(Xi,Yi)= (P-M) /1 P tel que : P est le nombre total d'attributs (-2) et M le nombre de",
        "ressemblance entre les deux attributs énumératifs < DegStr > et < HrSom > D2(Xi,Yi)= 0 si Xi = Yi Concerne l'attribut binaire < Fum > 0,5 Pt 1 sinon",
        "Ensuite, calculer la distance global D avec une distance d'attributs numériques par exemple avec la distance de manhattan : D(X,Y)= Z-1Xi = Yil Donc : DX,Y)-DICX,) + D2(Xi,Yi) Instance 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 D1",
        "0,5 0,5 0,5 0,5 0 0 1 1 1 1 0,5 0,5 1 1 1 1 0,5 0,5 D2 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 D 1,5 0,5 1,5 0,5 1 0 2 1 2 1 1,5 0,5 2 1 2 1 1,5 0,5",
        "rang 4 2 4 2 3 1 5 3 5 3 4 2 5 3 5 3 4 2 1,5 Pt Enseignant : M' K. Boudjebbour Page 3/4",
        "UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématiques et Informatique Année universitaire : 2018-2019 Exercice 03 (06 Pts) :",
        "On génère d'abord les itemsets fréquents de support minimum = 2: C1 itemset [A) (B) (C) (D) (E) Support 3 3",
        "4 1 4 F1 itemset Oui Oui Oui Non Oui 2Pt",
        "C2 itemset [A,B) [A,C) [A,E) B,C) (B,E) C,E) Support 1 3 2",
        "2 3 3 F2 itemset Non Oui",
        "Oui Oui Oui Oui C3 itemset (A,B,C) [A,B,E) [A,C,E) B,C,E)",
        "C4 itemset A,B,C,E) Support / /",
        "2 2 Support / 4",
        "F3 itemset Non Non Oui Oui",
        "= F4 itemset Non Cause A,B) non Fréquent A,B,C)",
        "Cause non Fréquent On génère maintenant les règles d'associations d'une confiance minimale = 60 % pour tout sous ensembles non vides fréquents : Pour l'itemset fréquent A,C,E)",
        "Règle (A,C)E (A,E)C [C,E)A A(C,E) C[A,E) E[A,C) Confiance 66,66 % 100 %",
        "66,66 % 66,66 % 50 % 50 % Conclusion Acceptée",
        "Acceptée Acceptée Acceptée Rejetée Rejetée",
        "Pour l'itemset fréquent [B,C,E) Règle (B,C)-E (B,E)-C (C,E)->B B[C,E) C(B,E) E>(B,C) Confiance 100 %",
        "66,66 % 66,66 % 66,66 % 50 % 50 %",
        "Pt: Conclusion Acceptée Acceptée Acceptée Acceptée Rejetée Rejetée Pour les autres itemset",
        "AC,AE sont: redondantes par rapport à A[C,E) BC,BE sont redondantes par rapport à B[C,E) L Règle CA",
        "EA CB EB CE EC",
        "Confiance 75 % 50 % 50 % 75 %",
        "75 % 75 % Conclusion Acceptée Rejetée Rejetée",
        "Acceptée Acceptée Acceptée - Un motif fréquent est dit fermé s'il ne possède aucun sur-motif qui a le même support, exp : (A,C) Pt; Un motif fréquent est dit Maximal si aucun de ses sur-motifs immédiats n'est fréquent, exp:A,C,E!\"",
        "Enseignant : M' K. Boudjebbour Page 4/4 UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématique et Informatique",
        "Année universitaire : 2017-2018 Interrogatoire : Théorie et pratique du Datamining Exercice 01 (02 pts) : Supposons qu'on veut utiliser des données binaires dans un processus de clustering. Citer (ou proposer) une (des) mesure(s) de similarité (distance(s)) pour ce type de données. Evaluer la (les) distance(s) entre les objets X = 0101010001 et Y = 0100011000. Que remarquez-vous ?",
        "Déduisez la distance de Hamming associée. A quoi la valeur trouvée correspond-elle ? Exercice 02 (03 pts) : Répondez brièvement aux questions suivantes : 1. Que signifie l'élagage et quel est son objectif ? 2. Quelle est la différence entre les techniques descriptives et les techniques prédictives de datamining ? 3. Dans le processus ECD, une phase de préparation des données est nécessaire. Que signifie la",
        "transformation des données ? expliquer en donnant des exemples Exercice 03 (05 pts) : Soit l'ensemble d'apprentissage ci-dessous. La classe est < Edible >. No Shape Color Odor Edible 1 C B",
        "1 Y 2 D B 1 Y 3 D",
        "W 1 Y 4 D W 2 Y",
        "5 C B 2 Y 6 D B",
        "2 N 7 D G 2 N 8 C",
        "U 2 N 9 C B 3 N",
        "10 C W 3 N 11 D W",
        "3 N 1. En utilisant l'algorithme ID3 et le gain d'information, construire l'arbre de décision du dataset. Donner les détails des calculs. 2. Déduire de l'arbre trouvé une seule règle comportant 2 disjonctions et 2 conjonctions au maximum. 3. En utilisant l'arbre construit, classer l'instance No12: Shape-C, Color-G, Odor-2.",
        "4. En utilisant l'ensemble des onze instances, et en supposant que les attributs < Color > et < Odor > sont des variables énumératives, dites lequel des instances est plus proche de l'instance No 12 ? quelle est la distance utilisée ? Que représentent ces calculs (donner le nom de ces calculs) ? BON - To succeed in life one must have the courage to pursue what he wants",
        "CRRIG Enseignant : M' K. Boudjebbour Page 1/1 UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences",
        "Département de Mathématique et Informatique Année universitaire : 2017-2018 Corrigé Interrogatoire : Théorie et pratique du Datamining Exercice 01 (02,50 Pts): il faut dessiner la table de dissimilarité (contingence) Y",
        "on a trois cas 0,5Pt possibles : 1 0 1. Similarité invariante, si toutes les variables sont symétriques (Coefficient de",
        "correspondance simple) : b+c 3 1 2 2 D1(X,Y) 0,3 0.5Pt",
        "a+b+c+d 10 X 2. Similarité non invariante, si toutes les variables sont asymétriques (Coefficient 0 1 5 de Jaccard):",
        "b+c 3 D2(X,Y) 0,6 0,5PL a+b+c 5 3. Si les variables sont symétriques et asymétriques : il faut spécifier la nature de chaque variable. 0,25Pt.",
        "E0.5.P:Malgré que D1 et D2 représentent deux distances entre les mêmes instances, on remarque que qu'elles sont très éloignées car D2=2*D1 Distance de hamming = b+c = 3. Elle représente le nombre de caractéristiques différentes entre X et y.:0,25 Pt Exercice 02 (03 Pts). : 1. L'élagage est la suppression de quelques sous-arbres dans la l'arbre de décision. Son objectif",
        "principal est la réduction de l'arbre afin d'améliorer le taux d'erreur. 2. les techniques descriptives de datamining visent à mettre en évidence des informations présentes mais cachées par le volume de données alors que les techniques prédictives visent à extrapoler de nouvelles informations à partir des informations présentes. Elles se basent essentiellement sur des modèles qui utilisent des données présentes ou passées pour construire des scénarios futurs.",
        "3. La transformation des données est la transformation d'un attribut A en une autre variable A' qui serait selon les objectifs de l'étude, plus appropriée. Exp 1: Variable continue en variable discrète et vice versa Exp 2: La construction d'agrégats par exemple, le prix au mètre-carré d'un appartement Exercice 03 (05,50 Pts):",
        "- 5 6 1) On calcul l'entropie sur l'ensemble des données : I(5,6)= log",
        "log = 0,994 0,5Pt 11 11 11 11",
        "Ensuite on calcul le gain de chaque attribut : 5 6 Gain (Shape)= (5,6)-E(Shape)- I(5,6)-( I(2,3)+ I(3,3))-0,008 11",
        "11 5 4 1 1",
        "Gain (Color)= (5,6)-E(Color)- I(5,6)-( I(3,2)+ I(2,2)+ I(0,1)+ I(0,1)-0,189 11 11 11 11",
        "3 5 3 Gain (Odor)= (5,6)-E(Odor)- I(5,6)-( I(3,0)+ I(2,3)+ I(0,3))-0,553 11",
        "11 11 Donc on choisit l'attribut < Odor > avec le gain le plus grand (Gain-0.553) qui représente le noeud la racine de l'arbre, Donc l'arbre initial sera : Odor",
        "1Pt: 2 3 1 ?????",
        "N Y Instances : 4,5,6,7,8 Instance : 9,10,11 Instances : 1,2,3",
        "Page 1/2 UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématique et Informatique Année universitaire : 2017-2018",
        "La valeur Odor = 2 donne plusieurs valeurs de l'attribut classe, donc, il faut refaire le même travail (calcul du gain) pour l'ensemble des données S2 (4,5,6,7,8). I(S2) =I(2,3)-0,971 1 2 1",
        "1 Gain (S2, Color)= I(2, 3)-E(S2, Color)-0,971-E I(1,0) + I(1,1)+ I(0,1)+ - I(0,1))-0,571 5 5 5",
        "3 Z Gain (S2, Shape)= I(2, 3)-E(S2, Shape)-0,971-E I(1,2) + I(1,1))-0,020 5 Donc on choisit l'attribut < Color> avec le gain le plus grand (Gain=0,571). On aura deux branches",
        "avec des noeuds terminaux et la branche B qui sera nécessairement départagée par le seul attribut restant à savoir <<Shapex et l'arbre final sera : Odor 2 3",
        "1 Color N Y Instance : 9,10,11",
        "B G ou U W Instances : 1,2,3 - Shape",
        "N Y Pt D Instances : 7,8 Instance : 4 -",
        "Y N Instance : 5 Instance : 6 2) La règle qu'on peut déduire est : (Odor = 1) V ((Odor = 2) A ((Color = W) V ((Color = B) A (Shape = C)))",
        "Pt: 3) La classe est : N 0,25Pt 4) II faut calculer la distance entre l'instance No12 et les 11 autres instances : D,Y)-DICXi,Y) + D2(Xi,Yi) D1(Xi,Yi)= (P-M) /1 P tel que : P est le nombre total d'attributs et M le nombre de ressemblance",
        "Qui concerne les deux attributs énumératifs < Odor > et < Color > D2(Xi,Yi)= 0 si Xi = Yi; 1 sinon Concerne l'attribut binaire < Shape > 01 Pt: No Instance D1 D2 D No Instance D1 D2 D 1",
        "1 1 2 7 0,5 0",
        "0,5 2 1 0 1",
        "8 0 1 1 3",
        "1 0 1 9 1",
        "1 2 4 0,5 0 0,5",
        "10 1 1 2 5",
        "0,5 1 1,5 11 1 0",
        "1 6 0,5 0 0,5 Donc, les instances les plus proches de l'instance No12 sont : l'instance No 4, No 6 et No 7. 0,25 Pt",
        "La distance utilisée est la distante mixte entre deux type d'attributs (dans notre cas, on a utilisé la distance de Manhattan) qui représente un calcul de similarité entre instantes afin d'appliquer une méthode de clustering. 0,5 Pt: Page 2/2"
    ],
    "summaries": [
        "Chunk 1: Researchers at the University of Rennes in France are developing new",
        "Chunk 2: Durée : 75 Exercice 01 (03 pts : 10 Mn) : Répondez aux questions suivantes",
        "Chunk 3: Le tableau suivant contient des dons sur les No",
        "Chunk 4: Chaque étudiant s'est",
        "Chunk 5: Une Technique ABien Admis s'",
        "Chunk 6: Bien Admis, nous avons ",
        "Chunk 7: Maths Bien Admis On veut construire",
        "Chunk 8: Une série de sciences passable non Ad",
        "Chunk 9: Exam questions and answers for the GCSEs in maths, English and",
        "Chunk 10: A look back at some of the key moments in",
        "Chunk 11: Tests for tuberculosis (TB) are increasingly being used",
        "Chunk 12: Une tte tte",
        "Chunk 13: Montrez toutes les étapes et",
        "Chunk 14: The BBC's science and technology correspondent Tomi",
        "Chunk 15: How many of the following do you remember from last year's exam?",
        "Chunk 16: Les entiers suivants ont t t t t t t t t ",
        "Chunk 17: Une tte--tte de manhathan s'est utilisant  l'",
        "Chunk 18: Good To succeed in life one must have the courage",
        "Chunk 19: The European Bioinformatics Institute (EBI) at the University of Nottingham",
        "Chunk 20: The BBC's science and technology team looks at the latest astronomical data from around the world.",
        "Chunk 21: Dsinglel(i.j) = Minx",
        "Chunk 22: Regroupement des clusters (1) et (2) en (1",
        "Chunk 23: BBC Sport takes a look back at some of the",
        "Chunk 24: BBC Sport outlines the key statistics behind England's",
        "Chunk 25: BBC Sport takes a look back at some of the",
        "Chunk 26: Regroupement des clusters (1,2) et en",
        "Chunk 27: Regroupement des clusters (1,2,9,12)",
        "Chunk 28: The full text of the Dendrogramme can be found",
        "Chunk 29: Un regroupement en 2 clusters : CI=",
        "Chunk 30: Match reports from the first round of the Irish Open",
        "Chunk 31: The full set of results from this year's",
        "Chunk 32: Le meilleur regroupement de clusters car son inertie intra-cluster IA",
        "Chunk 33: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 34: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 35:  Subscribe to BBC News HERE: http://",
        "Chunk 36: The winning numbers in Saturday evening's drawing of",
        "Chunk 37: The winning numbers in Saturday evening's drawing of",
        "Chunk 38: BBC Sport takes a look back at some of the",
        "Chunk 39: L'arbre de l'at Som s",
        "Chunk 40: Egal Supérieur Moins - Egal",
        "Chunk 41: The European Court of Human Rights (ECHR)",
        "Chunk 42: Les valeurs de la classe, donc, donnent deux valeurs de la",
        "Chunk 43: BBC Sport takes a look back at some of the",
        "Chunk 44: Pt Gain (SEg: Fum)=",
        "Chunk 45: Egal Supérieur K a le gain le",
        "Chunk 46: DegStr Moins y y Petit ou Normal",
        "Chunk 47:     ",
        "Chunk 48: Fum Non Oui I(SSup)",
        "Chunk 49: No Yes Gain (SSup, DegStr",
        "Chunk 50: The winning numbers in Saturday evening's drawing of",
        "Chunk 51: Egal Supérieur 3 3 Moins",
        "Chunk 52: Match reports from the weekend's Premier League and",
        "Chunk 53: A selection of photos from around the world this week",
        "Chunk 54: Non Donc on choisit l'at",
        "Chunk 55: No Fum Yes DegStr No Fum",
        "Chunk 56: All photographs  AFP, EPA, Getty Images",
        "Chunk 57: A No Yes No Yes A No Yes No Yes",
        "Chunk 58: Match reports from the weekend's Premier League and Championship",
        "Chunk 59: Researchers at the University of Rennes in Rennes, France, have developed a new method",
        "Chunk 60: BBC Sport outlines some of the key stories from the",
        "Chunk 61: A selection of photographs from around the world this week",
        "Chunk 62: Do you agree with the BBC's decision to",
        "Chunk 63: Is it time to end the war in Syria",
        "Chunk 64: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 65: Egal Non Yes No 23 Egal Non Yes",
        "Chunk 66: All photographs  AFP, EPA, Getty Images",
        "Chunk 67: A selection of photos from around the world this week",
        "Chunk 68: No Matrice de 1Pt Prédite",
        "Chunk 69: French voters go to the polls on Thursday to decide",
        "Chunk 70: All photographs  AFP, EPA, Getty Images",
        "Chunk 71: The winning numbers in Saturday evening's drawing of the French Open were:",
        "Chunk 72: The United Nations Population Fund (Unicef) has released the results of a study on the effects of climate change on children in the Democratic Republic of Congo",
        "Chunk 73: L'attribut binaire  Fum s",
        "Chunk 74: Find out how to calculate the distance between two points on the Earth.",
        "Chunk 75: D1 0,5 0,5 0,5 0,5 0,5 1 1 1 0,5 0,5",
        "Chunk 76: Match reports from the French Open, which saw Novak Djokovic beat",
        "Chunk 77: Université Libre de Bruxelles (ULB)",
        "Chunk 78: On génre d'abord les",
        "Chunk 79: Oui Oui Oui Oui Nonui",
        "Chunk 80: BBC Sport looks at some of the key talking points",
        "Chunk 81: BBC Sport takes a look back at some of the",
        "Chunk 82: Oui Oui Oui C3 itemset",
        "Chunk 83: C4 itemset A,B,C,",
        "Chunk 84: BBC Sport takes a look back at some of the",
        "Chunk 85: BBC Sport takes a look back at some of the",
        "Chunk 86:     ",
        "Chunk 87: Les rgles d'associations d'une",
        "Chunk 88: BBC News NI takes a look at some of the",
        "Chunk 89: The winning numbers in Saturday evening's drawing of",
        "Chunk 90: All photographs  AFP, EPA, Getty Images",
        "Chunk 91: L'itemset fréquent [B,",
        "Chunk 92: The winning numbers in Saturday evening's drawing of",
        "Chunk 93: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 94: The European Court of Human Rights (ECHR)",
        "Chunk 95: The European Commission (EC) has launched an investigation",
        "Chunk 96: BBC Sport takes a look at some of the key",
        "Chunk 97: A selection of photos from around the world this week",
        "Chunk 98: Un motif fréquent est dit fermé s'il ne poss",
        "Chunk 99: A selection of some of the most interesting papers published",
        "Chunk 100: Researchers at the University of Montpellier, France, have developed a new method for the clustering of large groups of data.",
        "Chunk 101: Dans le process de ECD, les dons de Hamming associées avaient  l'occasion d'un ",
        "Chunk 102: No Shape Color Odor Edible 1 C B : Soit l'",
        "Chunk 103: The winning numbers in Saturday evening's drawing of",
        "Chunk 104: The winning numbers in Saturday evening's drawing of",
        "Chunk 105: The winning numbers in Saturday evening's drawing of",
        "Chunk 106: Match reports from the weekend's Premier League games",
        "Chunk 107: Match reports from the weekend's Premier League games",
        "Chunk 108: BBC Sport takes a look back at some of the",
        "Chunk 109: The following table presents the results of a study on the effects of shape- C, Color-G, and",
        "Chunk 110: C'est un peu t--tre un peu t--tre un peu t--t",
        "Chunk 111: A selection of some of the best images from the",
        "Chunk 112: The French government has announced that it will introduce a new data mining",
        "Chunk 113: Les variables sont symétriques (",
        "Chunk 114:     ",
        "Chunk 115: The following table shows the coefficients for the following variables",
        "Chunk 116: Une spécifier de chaque",
        "Chunk 117: Le nombre de caractéristiques différentes entre D1 et D2 resentent deux mmes instances,",
        "Chunk 118: The following are some of the main points of the paper:",
        "Chunk 119: La construction d'agrégats par exemple, le prix au m'tre-carré d'",
        "Chunk 120: A selection of photographs from around the world this week",
        "Chunk 121: The winning numbers in Saturday evening's drawing of",
        "Chunk 122: All photographs courtesy of AFP, EPA, Getty Images",
        "Chunk 123: BBC Sport takes a look back at some of the",
        "Chunk 124: The winning numbers in Saturday's drawing of the",
        "Chunk 125: The winning numbers in Saturday evening's drawing of",
        "Chunk 126: Donc on choisit l'attribut  Odor",
        "Chunk 127: The winning numbers in Saturday evening's drawing of",
        "Chunk 128: BBC Sport takes a look at some of the key",
        "Chunk 129: A selection of some of the best news photographs from",
        "Chunk 130: La valeur Odor = 2 donne valeurs de l'at",
        "Chunk 131: The winning numbers in Saturday evening's drawing of",
        "Chunk 132: The results of a study on the effect of shape on the",
        "Chunk 133: C'est un peu t--t",
        "Chunk 134: BBC Sport takes a look back at some of the",
        "Chunk 135: BBC Sport takes a look back at some of the",
        "Chunk 136: BBC Sport takes a look back at some of the",
        "Chunk 137: C'est tre un peut tre ",
        "Chunk 138: The earthquake that struck off the coast of western Japan on Tuesday was the strongest to hit the country in",
        "Chunk 139: L'attribut binaire  Shape  No Instance D1 D2 D",
        "Chunk 140: BBC Sport takes a look back at some of the",
        "Chunk 141: The winners of the BBC Sports Personality of the Year",
        "Chunk 142: BBC Sport takes a look back at some of the",
        "Chunk 143: The winning numbers in Saturday evening's drawing of",
        "Chunk 144: The winning numbers in Saturday evening's drawing of",
        "Chunk 145: The winning numbers in Saturday evening's drawing of",
        "Chunk 146: BBC Sport takes a look back at some of the",
        "Chunk 147: Les instances de l'instance No12 sont ",
        "Chunk 148: La distance utilisée est la distante mixte entre deux type d'"
    ]
}