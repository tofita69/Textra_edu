Chunk Number,Original Text Chunk,Summary
1,Polycopié RO pour le GI Modèles stochastiques pour la prise de décision Mohammed Hadda 4ème année GI-IADS (2022-2023),The International Institute for Strategic Studies (IISS)
2,ENSAM de Meknès Université Moulay Ismail Table des matières 1 Introduction,A chronology of key events in the history of the
3,5 2 Notions générales de probabilités 7 2.1 Espace probabilisé,The BBC News website looks at some of the key
4,7 2.1.1 Espace mesurable 7 2.1.2 Mesurabiité 8,"Espace mesurable, mesurabiité"
5,2.1.3 Probabilités conditionnelles : 10 2.1.4 Mesure image; Lois 10 2.1.5 Variables aléatoires discrètes,Aléatoires discrtes 
6,11 2.1.6 Lois discrètes usuelles : 11 2.1.7 Variables aléatoires réelles et Lois à densité 14,"Lois de la Roche-Sur-Yon,"
7,2.2 Théorèmes de convergence 16 2.2.1 Lois des grands nombres : 16 2.2.2 Théorème central limite :,Lois des grands nombres : 16
8,16 3 Simulation de Variables aléatoires 19 3.1 Simulation de Variables aléatoires discrètes,Simulation de aléatoires discr
9,19 3.2 Simulation de Variables aléatoires à densité 19 3.2.1 Simulation d'une v.a. admettant une densité continue par inversion de la fonction de répartition,Simulation de Variables aléatoires 
10,19 3.2.2 Simulation d'une va. bornée à densité bornée par la méthode de rejet 20 3.2.3 Simulation d'une loi normale : 20,Simulation d'une loi normale :
11,3.3 Méthode de Monte Carlo pour le calcul des intégrales et des moments : 20 3.3.1 Méthode 20 3.3.2 Précision (estimation d'erreur) :,"All photographs courtesy of AFP, EPA, Getty Images"
12,21 3.3.3 Inérêts de la méthode 21 3.3.4 Calcul approché des moments 21,"All photographs courtesy of AFP, EPA, Getty Images"
13,"3.4 Méthode de ""simulation Monte Carlo"" 22 4 Modèles stochastiques 23","All photographs courtesy of AFP, EPA, Getty Images"
14,4.1 Processus stochastiques 23 4.1.1 Introduction 23 4.1.2 Processus stochastiques,Processus stochastiques 23 :
15,23 4.2 Processus de Wiener ou mouvement brownien 24 4.3 Processus gaussien 25,"All photographs  AFP, EPA, Getty Images"
16,4.4 Processus de Poisson 25 4.5 Chaines de Markov 25 4.5.1 définitions et propriétés,The following table lists the most common Markov chains used
17,25 4.5.2 Probabilités et matrice de transition 26 4.5.3 Propriétés des chaines de Markov homogènes 27,A look at some of the key findings from the
18,4.5.4 Classification des états d'une une chaine de Markov 31 4.5.5 Lois ou probabilités stationnaires: : Temps moyen de premier retour 36 4.6 Application :,The BBC's science and technology correspondent Tomi
19,39 5 Processus de décision markoviens 43 5.1 Processus de décision markoviens,"All photographs courtesy of AFP, EPA, Getty Images"
20,43 5.2 Problèmes décisionnels de Markov 53 5.3 Politiques d'actions 53,A look at some of the key stories from the
21,5.4 Politique markovienne et chaine de Markov valuée 55 5.5 Critères de pertormance 56 5.6 Fonctions de valeur,Politique markovienne et chaine de Markov
22,57 5.6.1 1-cas du crière fini 57 5.6.2 2- cas du crière moyen 57,The winning numbers in Saturday evening's drawing of
23,5.7 Politiques markoviennes 58 5.7.1 Equivalence des politiques nstolre-dépendontes et markoviennes 58 5.8 Caractérisation des politiques optimales,Politiques markoviennes 58 5.7.1
24,58 5.8.1 Cas du critère fini 58 5.8.2 Cas du critère moyen 60,BBC Sport takes a look at some of the key
25,5.9 Algorithmes de résolution des MDP 61 5.10 Etude de cas et Application Industrielle; cas d'un MDP 63 5.10.1 Exemple de système de production,The following table summarises the key findings from the
26,63 2 % O o,A look back at some of the most memorable moments
27,- G 9 / - 1,BBC Sport takes a look back at some of the
28,2 % o 39 G -,BBC Sport takes a look back at some of the
29,"5 1. Introduction 1 Dans le cas d'un problème d'optimisation déterministe, les valeurs de tous les paramètres sont supposées connues.",The following is a guide to the main points of
30,"Alors que dans un problème d'optimisation stochastique un certain nombre de paramètres peuvent être considérés comme inconnues et sont modélisés par des variables aléatoires. Plus généralement, l'incertitude est représentée par des expériences aléatoires dont l'issue est notée Q. L'ensemble de toutes les issues possibles est l'espace fondamental . Par exemple, si les éléments de 2 sont les conditions climatiques, ils permettent de décrire des",Les conditions climatiques aléatoires peuvent tre considérés et sont modélisés par des variables a
31,"variables aléatoires telles que la consommation éléctrique. Ainsi, L'optimisation stochastique est un cadre qui permet de formuler un problème d'optimisation dans lequel les données sont incertaines (par exp. les prix des énergies). C' est aussi un ensemble de méthodes et de techniques de résolution. Dans un problème d'optimisation stochastique, on cherche souvent à prendre une décision avant",L'optimisation stochastique est un cadre qui permet de formuler un problme d'opt
32,que les valeurs des réalisations des variables aléatoires ne soient observées. 2 % O o,C'est un peu t-
33,- G 9 / - 1,BBC Sport takes a look back at some of the
34,2 % S% - 2. Notions générales de probabilités,The number of people who have died as a result
35,"Dans ce chapitre introductif nous présentons les notions basiques de théorie des probabilités qui seront utilisées dans la suite de ce cours. 2.1 Espace probabilisé 2.1.1 Espace mesurable On considère un ensemble quelconque 2, qui modélise le caractère aléatoire d'un certain phéno-","Espace mesurable On considre un ensemble quelconque 2, qui"
36,"mène et portera dans la suite le nom d'ensemble fondamental, ou d'espace d'issues (des chances, d'échantillons). On note 9P(2) l'ensemble des parties de 2. Rappelons que si Card. 9P(2) - n; alors Card 9P(2) = 2"" : Par exp si 2 = (1,2,3) alors 9P(2) = 0,0),12),1).0.2) (1,3),12,3),2). Quand 2 est fini ou dénombrable, l'ensemble 9P(2) décrit les événements aléatoires associés à",Le nom d'ensemble 9P(2) peut--tre tre  l'occasion d'un
37,"l'expérience considérée. Quand 2 est infini non-dénombrable, l'ensemble 9P(2) est parfois trop gros pour pouvoir étudier mathématiquement la probabilité de ces événements. On a alors recours à la notion de tribu : Définition 2.1.1 Une tribu ou O-algèbre bg sur 2 est un sous-ensemble de 9(2) contenant 2 et vériflant les propriétés suivantes :",Une tribu ou O-algbre bg sur 2 est un sous-ensemble de 9(2)
38,e SiA € by alors Ac € by (stabilité par passage au complémentaire). . Si (An)n est une suite de by alors UnAn € by (stabilité par réunion dénombrable). Remarque : by est stable par réunion finie et par intersection finie. II faut comprendre une tribu comme la quantité d'information assocée à une certaine expérience aléatoire. Remarquons qu'une tribu contient forcément 0.,Si (An)n est une suite de by alors UnAn  by (stabilité par réunion dénombrable).
39,"Exemples de tribus sur 2 : (0,2) tribu triviale (la plus petite) et 9P(2) tribu complète (la plus grande); ou 0,A,A,2). Si by est une tribu sur , on dit que (2, 9) est un espace mesurable, ou aussi probabilisable 8 Chapitre 2. Notions générales de probabilités",Une tribu triviale (la plus petite) et 9P(2) tribu complte
40,"lorsqu'on va le munir d'une mesure de probabilté, et les éléments de by sont des parties by mesu- rables (événements). Une sous-tribu is de by est une tribu incluse dans by au sens où A E S - AE.9. Remarque : Toute inersection de tribus sur 2 est une tribu sur 2. Mais une réunion de tribus n'est pas forcément une tribu;en effet si A1,A2 C 2, alors 91 = 0,A1,AS,2) et 91 = (0,A2,Ag,2)","Une sous-tribu s'est lorsqu'on va ler d'une mesure de probabilté, et les éléments"
41,"sont des tribus, mais pas 91 U.92 en général, par exp si A1 NA2 £ 91 J.92 Définition 2.1.2 Soit a une partie quelconque de 9P(2). On appelle tribu engendrée par d, et on note C(G) l'intersection de toutes les tribus contenant d. Définition 2.1.3 On appelle tribu des boréliens de R, on note 9B( (R), la tribu engendrée par les intervalles ouverts Ja,bla<be ER.",Soit une partie quelconque de 9P(2).
42,"Exemples d'espace mesurable : 1. (2, 9P(2)) si 2 est fini. 2. (R, B(R). Remarque : 1. Dans cette définition, on peut remplacer ""ouvert"" par ""fermé"" ou par ""ouvert à droite"" ou :","Dans ""ouvert"" par ""ouvert  droite"
43,"2. 9B(R) $ 9(R). 2.1.2 Mesurabilité Définition 2.1.4 Soit (2,9F) et (E,6) deux espaces mesurables. Une application X :  E est dite bg 6 mesurable si X 4) € g pour tout A € 6 où X (A) - fo E 2: X(O)EA; autrement dit si X (6)c9.","2.1.2 Mesurabilité Définition 2.1.4 Soit (2,9F) et ("
44,"Remarque : La mesurabilité d'une application est conservée chaque fois que l'on diminue (au sens de l'inclusion) la tribu de l'espace d'arrivée ou que l'on agrandit celle de l'espace de départ. Une fonction X : R 1 R est dite borélienne si elle est 9B(R) 9B(R) mesurable, c-à-d si X (A) € 9B(R) pour tout A € B(R). En fait, il suffit que cette propriété soit vérifiée pour les intervalles ouverts A.",Une fonction X : R 1 R est dite borélienne si elle est 9B(R) 9B(R) mesurable
45,"En particulier, les focntions continues sont boréliennes car l'image réciproque d'un intervalle ouvert par une fonction continue est une réunion d'intervalles ouverts, donc un borélien. Définition 2.1.5 Soit (2,9) un espace mesurable. Une variable aléatoire réelle (v.a.r.) X est une application mesurable de (2, 9) dans (R,B(R). Si X(2) est au plus dénombrable et X est y 9(R) mesurable, X est dite variable aléatoire",Une mesurable aléatoire mesurable s'est lors d'un intervalle ouvert par une fon
46,"discrète réelle. Définition 2.1.6 Soit (2,9) un espace mesurable. Un vecteur aléatoire X est une application mesurable de (,9) dans (R4,B(R4) (d > 1). Si X(2) est au plus dénombrable et X est by R4) mesurable, X est dit vecteur aléatoire","Une application mesurable de (,9) dans (R4,B(R4)"
47,"discret. Exemple : Un exemple simple, mais important, d'application mesurable de 2 dans R (et de v.a. 2.1 Espace probabilisé 9 discrète) est La fonction indicatrice d'un ensemble A € by :","Un exemple simple, important disc d'application mesurable de"
48,"1 SIOEA 1A 0) 0 sinon En effet pour tout B C R, on a 0 siOfBetlgB",The BBC Sport website reports on the results of the
49,A siOfBetlEB (1A)-'(B) = Ac si0EBetlfB 2 si0EBetlEB Remarquons bien que (1A)-'(9(R) = C(A)). Ceci montre que 1A est C(fA)) - 9P(R) mesu-,A siOfBetlEB (1A)-
50,"rable. De même, on vérifie aisément qu'une fonction constante de 2 1 R est mesurable pour toute tribu sur 2 et toute tribu sur R. Remarquons également que les v.a.r. sont stables par composition avec les fonctions boréliennes : si X est une v.a.r. is mesurable et f une fonction borélienne, alors f(X) est une v.a.r 9 mesurable.",Une v.a.r. sont stables par composition avec les fonctions boréliennes : si X
51,"Les v.a.r. peuvent être approchées en limite croissante simple par des v.a.r. ""en escalier"" qui sont du n type X a;lA, avec ai € R, Ai E. Définition 2.1.7 La tribu engendée par une variable aléatoire X définie sur (2, 9) est l'en- semble (X) = £x-(A), A € B(R).","La tribu engendée par une variable aléatoire X définie sur (2, 9)"
52,"c(X) est la plus petite tribu sur 2 rendant X mesurable. En plus, (X) est une tribu contenue dans 9. Définition 2.1.8 Soit (2, 9) un espace mesurable. Une mesure de probabilité ou simplement une probabilité sur (2, 9) est une application P de 9 dans [0, 1] vérifiant . P(2) = 1","Une application P de 9 dans [0, 1] vérifiant ."
53,"e Si (An)meN est une suite d'éléments de by deux à deux disjoints, alors (UA)-EPA) (propriété de o - additivité). n-0 n-0 On utilise souvent l'écriture avec les fonctions indicatrices :","Mae fonctionstrices (An)meN,"
54,P(A) dP 1AdP. A ,The winning numbers in Saturday evening's drawing of
55,"Le triplet (,9,P) est appelé espace de probabilité ou espace probabilisé. Proposition 2.1.1 On a (i) P(A) + P(AP) - 1 VA € by (ii) A C B 1 P(A) < P(B) (iii) P(AUB) + P(ANB) = P(A) + P(B) VA, B € 9.",Prop 2.1.1 on a (i) P(A) + P(AP) - 1
56,"Exemple : (Lancé de dés) On considère l'expérience aléatoire du lancer de deux dés. L'espace d'échantillonnage est 22 = ((1,1),(1,2), (6,6)). Soit X la variable aléatoire définie par la somme des résultats des deux dés. Dans ce cas on a P(X = 1) = P(0) = 0; P(X = 3) = P(fo € /X(0) = 3) = P(1,2),2,1)D)- S 18",Dans ce cas on a P(X = 1) = P(0) = 0; P(X = 3) = P(fo 
57,"10 Chapitre 2. Notions générales de probabilités P(X < 2) = P(X = 2) = P(fo € 2/X(0) = 2) = P((1,1))) = 36 P(X S 4) = 44,1,2.0.32.226.0) 6 50 Remarque : II existe une unique mesure de probabilité 2 sur ([0,",Remarque : II existe une unique mesure de probabilité 2 sur (
58,"[0, 1) telle que 2(a,bD = b a, pour tous a, b € [0,1,a < b; appelée mesure de Lebesgue sur [0, 1). Définition 2.1.9 Soit (2, 9,P) un espace de probabilité. Un ensemble A € by est dit négli- geable pour P si P(A) = 0. Par exemple les singletons, les ensembles dénombrables de [0, 1] sont négilgeables pour la mesure","Par exemple les singletons, les ensembles dénombrables de [0, 1] sont négilgeables pour la me"
59,"de Lebesgue. 2.1.3 Probabilités conditionnelles : Si A et B sont deux événements tels que P(B) > 0, la probabilité conditionnelle de A sachant B, notée P(AIB) est définie par P(ANB)",Les tats-Unis de France s'
60,"P(AIB) = P(B) BCA 1 P(AIB) = 1 eA et B indépendants 1 P(AIB) = P(A). Rappel : Un système d'événements (Bi)iel est dit exhaustif ou complet si 2 = UieiBi, union",The results of the first round of voting in the French presidential election are
61,"au plus dénombrable (I C N) et disjointe (B;B; = 0, Vij). Formule des probabilités totales Si (Bi)iel est un système exhaustif (fini ou dénombrable) d'événements, et si ViEI,P(Bi) * 0, alors pour tout événement A on a P(A) - EP(AIB,)P(B) = EP(ANB).",Alors tout événement A on a P(A) - EP(A
62,"iEl El Formule de Bayes Avec les notations précédentes, si P(A) > 0 on a P(AIB;)P(B;)",El Formule de Bayes Avec les notations
63,"P(B,IA) = Ee,PAB)P(B) 2.1.4 Mesure image; Lois Les applications mesurables permettent de < transporter > la mesure d'un espace à un autre. Dans le cas d'une variable (ou d'un vecteur) aléatoire, la "" mesure image "" ainsi obtenue s'appelle la loi",Dans le cas d'une variable (ou d'un vecteur)
64,"de la variable (ou du vecteur) aléatoire. Définition 2.1.10 Soit une v.a.r. définie sur (2,9,P). La loi de X (sous P) est la probabilité Px sur(R, 9B(R) définie comme mesure-image de P par X : pour tout A € 9B( (R), Px(A) = P(X (A)) = Pfo € 2/X(0) E Aj. Pour simplifier, on note Px(A) = P(X EA).","La loi de X (sous P) est la probabilité Px sur(R, 9B(R))"
65,"2.1 Espace probabilisé 11 2.1.5 Variables aléatoires discrètes La loi d'une v.a discrète X est la donnée de toutes les valeurs P(X = xk) = Pk (Pk € [0, 1) lorsque Xk prend toutes les valeurs possibles dans X( (2).",1.1 Espace probabilisé 11 2.1.5 Variables aléatoires
66,"Dans ce cas VB € B(R), P(X € B) = E P(X - xk) = Z Pk- XKEB XKEB Définition 2.1.11 Soit (21,91,P1) et (22,92,P2) deux espaces de probabilité. Soit X (resp. Y) une variable aléatoire discrète définie sur 21 (resp. sur 22). On dit que. X et Y ont même loi","Dans ce cas VB  B(R), P(X  B) = E P(X -"
67,"si X(21) = Y(22) et si pour tout x € X(21) on a PI(X - x) = P2(Y = x). On note X 2 Y. Définition 2.1.12 e Soient X et Y deux v.a. discrètes. La loi conjointe du couple (X,Y) est la donnée de toutes les valeurs de P(X =x,Y = y) pour (x,y) € X(2) X Y(2). Les lois de X et de Y sont les lois marginales de X et de Y. e Soit x € X(2) tel que P(X = x) > 0. On appelle loi conditionnelle de Y sachant (X - x) la",Les lois de X et de Y sont marginales de X et de Y.
68,"probabilité Px définie sur Y( (2) par P(X - VyEY(2), Px(b!) - P(Y P(X - x Proposition 2.1.2 = - v.a. discrètes indépendantes. Deux v.a. discrètes X et Y sont indépen-",Voters in the UK go to the polls on 8 May to choose
69,"dantes ssi pour tout A C X(2) et tout B C Y(2), on a P(X E A,Y € B) = P(X EA)P(Y € B). Définition 2.1.13 - Espérance. Soit X une v.a. prenant ses valeurs dans fxk K N 1). Si la famille (P(X = xK))k est sommable c-à-d si 5 xyP(X =. xk) < +oo, on dit que X est K21",Soit X une v.a. prenant ses valeurs dans fxk K N 1).
70,"intégrable et on définit l'espérance de X par E(X) = L xP(X = xk). K21 Formule de transfert : Soit 9 R R telle que (X) est intégrable, alors E((X))= Z (x)P(X = xk).",Alors d'une Formule de transfert : K21
71,"K21 Définition 2.1.14 - Variance. Si E(X2) < 00, on appelle . variance de X le réel Var(X) = E[(x - E(X))2]= L (xk E(X))?P(X =. xk) ( Var(X) = E(X2)-(E(X). K21",variance de X le réel Var(X) = E[(
72,"e écart-type de X le réel c(X) = VVar(X). covariance de X et de Y (si les moments d'ordre 2, E(X2) < 0o et E(Y2) < co) le réel Cov(X,Y)= E((X - E(X))(Y-L E(Y)) = E(XY) - E(X)E(Y). 2.1.6 Lois discrètes usuelles : Loi de Bernoulli. On dit qu'une v.a. X suit une loi de Bernoulli de paramètres P E [0, 1] et on note",La loi de Bernoulli s'agitre  l'arrivée et s'agitre  l
73,"X 2 B(p) si X est à valeurs dans f0, 1 et que P(X = 1)=p et P(X (=0)=1-P 12 Chapitre 2. Notions générales de probabilités On a E(X) = P et Var(x) = P(1 P).",On a E(X) = P et Var(x) = P(1
74,"Dans ce cas, la v.a. X vaut 1 s'il y'a succès et 0 s'il y'a échec. Loi uniforme. On dit qu' une v.a. X suit une loi uniforme sur f1, ,nj et on note. X 7 ,ny) si X prend ses valeurs dans £1, ,nj et P(X = k)","Dans ce cas, la v.a. X vaut 1 s'il y'a succ"
75,"pour tout k E f1, ny. n n + 1 n - On a E(X)",n n n n n n n n n n
76,"et Var(x) - 2 12 Loi binomiale. On dit qu'une V.a. X suit une loi binomiale de paramètres n N let € [0,1] et on note X 2 B(n,p) si X prend ses valeurs dans f0, nj et",Loi binomiale de paramtres  l'occasion d'un
77,"P(X = k) p""(1 - p)""-k Vk € f0, ,nj. On a E(x) = np et Var(X) - np(1 p ). La loi binomiale donne le nombre de succès qu'on peut obtenir en répétant n épreuves simi- laires (de même loi) et indépendantes ayant pour issues un échec et un succès (c-à-d n épreuves de Bernoulli).",La loi binomiale donne le nombre de succs qu'on peut obtenir en répé
78,Loi de Poisson. On dit qu'une v.a. X suit une loi de Poisson de paramètre A > 0 et on note X 2 9P(2) si X prend ses valeurs dans N et Pk P(X Vk € N. k!,La loi de Poisson de paramtre A > 0 et on peut
79,"On a E(X) = A et Var(X) = 2. La loi de Poisson décrit le comportement du nombre d'événements se produisant dans un in- tervalle de temps donné, lorsque la probabilité de réalisation d'un événement est très faible et que le nombre d'essais est très grand. Si le nombre moyen d'événements dans un intervalle de temps fixé est A, alors la probabilté qu'il","On a E(X) = A et Var(X) = 2, alors la probabilité de réalisation d'"
80,"existe exactement k événements (k entier naturel, k = 0, 1,2...) est donnée par Pk- Exemples : 1. Si un événement se produit en moyenne 5 fois par seconde, pour étudier le nombre d'événements se produisant pendant 60 secondes, on choisit comme modèle une loi de Poisson de paramètre 2 = 60 X 5 = 300.",The following table shows the number of seconds taken by Poisson modle de paramtre 2 = 60 X 5 = 300
81,"2. Soit X la variable aléatoire du nombre de personnes réservant un billet d'avion pour la Mecque le 20 mars à 9H30. X suit une loi binomiale dont l'effectif est très grand, tous les clients potentiels, des millions; et le paramètre P est très petit, la probabilité pour qu'un individu choisi au hasard ait envie de se rendre à la Mecque le 20 mars par le vol de 9H30. On approxime en général la loi de X par la loi de Poisson de paramètre np.","La loi de Poisson de paramtre np ainsi que l'effectif est trs grand, tous les clients potentiels, des millions; et le param"
82,"3. Soit X la variable aléatoire égale au nombre d'appels reus par un standard téléphonique dans un intervalle de temps [0,T), la loi de X est une loi de Poisson. 2.1 Espace probabilisé 13 4. Les clients arrivent à une banque ou un supermarché de manière aléatoire, ce qui signifie","Les clients arrivent une banque ou un limite de manimar, ce n'a pas "
83,que l'on ne peut pas prédire quand certains arriveront. La loi ou la fonction de densité de probabilité décrivant de telles arrivées durant une une période spécifique est la loi de Poisson. Soit X la V. a. désignant le nombre d'événements c-à-d d'arrivées qui prend place durant une unité de temps spécifique par exp. une minute ou une heure.,Le nombre d'événements c--d d'arrivées c'-d d'arri
84,"Etant donné que 2 est une constante connue, la loi de probabilité est définie par : 2k P(X - k=0, 1,2, k!",La loi de probabilité s'
85,"La moyenne E(X) - A signifie que 2 doit représenter le taux ou la vitesse avec laquelle l'événe- ment se produit. On rencontre la distribution de Poisson dans l'étude des files d'attente. Par exp., les travaux de réparation arrivent à un petit atelier de réparation de moteurs de facon totalement aléatoire à un taux de 10 par jour.",Une taux de réparation  un taux de taux  laquelle les ateliers
86,"1. Quel est le nombre moyen de travaux reçus par jour? 2. Quelle est la probabilité qu'aucun travail de réparation n'arrive pendant une heure, en suppo- sant que l'atelier est ouvert 8 heures par jour. En effet, 1. le nombre moyen de travaux reçus par jour est égal à A = 10.",Le nombre de aux reus par jour?
87,"2. Pour calculer la probabilité qu'aucun travail de réparation n'arrive pendant une heure, on a besoin 10 de calaculer le taux d'arrivées par heure, c-à-d Aheure = - 1.25 travaux par heure. Ainsi 8",Pour calculer la probabilité qu'aucun travail de ré
88,hheure - (Apam)e Paucune arrivée par heure) = P(X = 0) - 0.2865,Paucune arrivée par heure
89,"0! Loi géométrique. On dit que X suit une loi géométrique dej paramètre pE [0, 1] et on: note X 2 9(p) si X elle est à valeurs dans N* et si P(X p(1",Loi géométrique.
90,Vn E N*. I On a E(X) et Var(x) P,The winning numbers in Saturday evening's drawing of
91,"- la loi géométrique représente le nombre d'essais nécessaires de réalisation d'une expérience jusqu'à son premier succès. En d'autres termes, cette loi correspond à la loi du temps d'attente dans un processus de Bernoulli. Remarque : Pour chacune des lois précédentes, on a EP(X=n)=1.",La loi du temps d'attente dans un processus de Bernoulli s'
92,"Proposition 2.1.3 On a e Une loi de Bernoulli de paramètre P est une loi binomiale de paramètres 1 et p. Las somme de 2 v.a. indépendantes suivant des lois binomiales de paramètres (ni,P) et (2,P) respectivement suit une loi binomiale de paramètres (ni + n2,P). e La somme de deux v.a. indépendantes suivant respectivement des lois de Poisson de paramètres",On a e Une loi de Bernoulli de paramtre P est une loi binomiale de paramtres
93,"A1 et A2 suit une loi de Poisson de paramètre A1 + A2. 14 Chapitre 2. Notions générales de probabilités Proposition 2.1.4 Si (Xn)n est une suite de variables aléatoires de Bernoulli indépendantes de même paramètre P, et si X est la variable aléatoire qui donne le rang du premier succès dans cette",Une loi de Poisson de paramtre A1 + A2 mme paramtre P
94,"successions d'épreuves, alors X suit une loi géométrique de paramètre p. Théorème 2.1.5 = Caractérisation comme loi sans mémoire. Soit X une variable aléatoire à valeurs dans N. X est sans mémoire c-à-d que Vn, k € N, P(X > n + k)X > n) = P(X > k) si et seulement si X suit une loi géométrique. L'unique loi de probabilité discrète à perte de mémoire est la loi géométrique.",La loi géométrique de paramtre p. Théorme 2.1.5 = Caractérisation loi sans mé
95,Cette propriété est le plus souvent exprimée en termes de < temps d'attente >. Supposons qu'une variable aléatoire X soit définie comme le temps passé dans un magasin de l'heure d'ouverture (disons 9H du matin) à l'arrivée du premier client. On peut donc voir X comme le temps qu'un serveur attend avant l'arrivée du premier client. La propriété de perte de mémoire fait une comparaison entre les lois de probabilité du temps,Cette propri est le plus souvent exprimée en termes de  temps d'attente >.
96,"d'attente du serveur de 9H à l'arrivée du premier client, et celle du temps d'attente du serveur pour qu'un client arrive à compter d'un délai arbitraire après l'ouverture (disons, par exp, une heure après l'ouverture soit à partir de 10H du matin) sachant qu'aucun client n' 'est arrivé de l'ouverture àl l'écoulement de ce délai arbitraire. La propriété de perte de mémoire affirme que ces lois sont les mêmes.",L'aucun client n'est arrivé de l'ouverture l l'écoulement de ce délai arbi
97,"Ainsi, dans cet exemple, ce n'est pas parce que le serveur a déjà attendu, pendant une heure l'arrivée d'un premier client qu'il peut espérer que le délai avant qu'arrive effectivement son premier client soit plus faible qu'au moment de l'ouverture. Attention : La perte de mémoire d'une loi de probabilité d'un nombre d'essais X jusqu'au premier succès signifie",L'aprs-sous-de-la-crme d'un premier succse s
98,"P(X > 50/x > 40) = P(X > 10), cependant, elle n'implique pas P(X > 50] > 40) = P(X > 50) à moins que les événements (X > 50) et (X > 40) sont indépendants, ce qui n'est pas le cas. 2.1.7 Variables aléatoires réelles et Lois à densité",2.1.7 Variables aléatoires réelles et Lois  densité
99,"Proposition 2.1.6 - v.a. indépendantes. Soient X et Y deux v.a.r. sur 2. On dit que. X et Y sont indépendantes si pour tout A, B C R, P(X E A,Y € B) = P(X EA)P(Y € B). Définition 2.1.15 = Lois à densité. Soit f: R 1 R une fonction positive d'intégrale 1 f(x)dx - On dit qu'une variable aléatoire X : 22 1 R a pour densité si sa loi",Soit f: R 1 R une fonction positive d'intégrale 1 f(x) - On dit qu'une variable aléatoire
100,"00 f est définie par VB C R, P(X € B) F(x)dx. B",C'est un peu t-
101,"2.1 Espace probabilisé 15 Dans ce cas, pour a < b réels on a P(a<X<b) = P(X Ela,bD = f(r)dx = f(x)dx ja,bl",Espace probabilisé 15 Dans ce
102,"a Lois usuelles : 1 Loi uniforme sur [a,b) (a < b), notée A ([a,b)), de densité f(x) a,b(x).","L'autre Lois, selon"
103,"a Loi exponentielle de paramètre 2 (2 > 0), notée 8(2), de densité f(x) - Ae -Ax1R-(). La loi exponentielle est l'analogue dans le cas continu de la loi géométrique : c'est la seule loi à densité continue sans mémoire, c-à-d vérifiant P(X > S +t)X > t) = P(X > s), Vs,t>0.",La loi exponentielle f(x) - Ae - Ax1R - La loi exponentielle est
104,"Loi gaussienne ou normale de moyenne m et de variance G2 > 0, notée N (m, 0 ). de densité 1 n f(x) exp",C'est un peu t-
105,"Vx E R. GV2r 202 Proposition 2.1.7 Si Xi et X2 sont deux v.a. indépendantes suivant respectivement des lois N (mi,o?) et N (m2, G2), alors X1 + X2 - N (mi + m2,0? + 02).",Si Xi et X2 sont deux v.a.
106,"Définition 2.1.16 = Fonction de répartition. Soit X une v.a. réelle sur 2. On appelle fonction de répartition de X, et on note Fx, la fonction définie par Vt € R, Fx(t) = P(X S1). Proposition 2.1.8 On a e Va, b € R, P(a <X < b) = Fx(b) - Fx(a).","Proposition 2.1.8 On a e Va, b  R, P(a X  b)"
107,"e Fx est croissante et satisfait lim Fx(t)=0, lim Fx(t)=1. 1-- t-+o0 La loi d'une v.a.r. X est entièrement déterminée par la fonction de répartition Fx de X. Si X est une v.a.r. admettant une densité f, alors",La loi d'une v.a.r.
108,"Fx() = f(x)dx, Vt € R. Exemple : (Lancé de dés) Soit X la somme des résultats des deux dés. On a Fx(1) = P(X K 1) = 0; Fx(4) = P(X < 4) 36 ; Fx(12) = 1. Proposition 2.1.9 Si (X,Y) est un couple de variables aléatoires qui admet pour densité fix,y) : R2 1 R, et si fx, fr sont les densités marginales de X et Y respectivement, X et Y sont indépen-","Fx = f(x) fx, Vt  R, Exemple : (Lancé de dés) Soit X la somme des résult"
109,"dantes ssi foxp(x,y) = fx()fr0) pour presque tout (x,y) € R?. Proposition 2.1.10 SiX et Y sont deux V.a.r. indépendantes et de carré intégrable alors elles sont décorrélées, c-à-d E(XY) = E(X)E(Y). Définition 2.1.17 = Espérance. Soit X une variable aléatoire admettant pour densité f. Si",Proposition 2.1.10 SiX et Y sont deux V.a.r.
110,"xlf(r)dx < +oo, on dit que X est intégrable et on définit l'espérance (moment d'ordre 1) de JR 16 Chapitre 2. Notions générales de probabilités X par",C'est tre  l'
111,"E(X) = xf(x)dx. JR Proposition 2.1.11 Soient X et Y deux v.a.r. intégrables sur 2. Alors, on a 1. Va, b E R, E(ax + b) = dE(X)+b; 2. E(X +Y) = E(X) + E(Y).",Soient X et Y deux va.
112,"Proposition 2.1.12 Soit X :-7 R une v.a.r. admettant pour densité f. Soit 9 : R 7 R telle que (X) est intégrable ( (x)lf(r)dx < +oo), alors R E((x)) = J (x)f()dx. La variance de X est le réel positif",Proposition 2.1.12 Soit X :-7 R une v.a.r.
113,"Var(X) = E[(X - E(X))2]= - (-E(X)F()dx TR Proposition 2.1.13 Soit X une v.a.r. de carré intégrable. On a 1. Var(X) = E(x2) + (E(X))2; 2. Va, b E R, Var(ax +b) = a?Var(X).",On a 1. Var(X) = E(x2) +E(
114,"2.2 Théorèmes de convergence 2.2.1 Lois des grands nombres : Théorème 2.2.1 - Loi forte des grands nombres. Soit (X)nx1 une suite de variables aléa- toires réelles 2 a 2 indépendantes, de même loi (i.i.d.) d'espérance m. Pour tout n, on note X1 + + Xn",Une suite de variables aléa- toires réelles 2 a 2 indépendantes
115,"X, n Alors pour tout E > 0, on les estimations et E((X,-m))s n","Alors pour tout E > 0,"
116,"(K.-me)se De plus, la suite (Xn)n2i converge presque sûrement vers m. Remarque : La loi des grands nombres exprime que la moyenne des n premiers termes d'une suite de variables aléatoires converge vers une constante qui est leur moyenne, c-à-d que la moyenne empirique de cette suite se concentre autour de son espérance.",La suite (Xn)n2i avait  l'arrivée et  l'
117,"2.2.2 Théorème central limite : On rappelle ici l'importance de la loi normale, comme étant la loi limite apparaissant lorsqu'on examine les fluctuations de la moyenne empirique d'une suite de v.a. i.i.d. autour de l'espérance. L'on sait que la moyenne empirique d'une suite de variables aléatoires undépendantes se concentre autour de son espérance; la question suivante est naturelle : que peut-on dire des fluctuations de la","2.2.2 Théorme central limite : On rappelle ici l'importance de loi normale, étant la loi"
118,"2.2 Théorèmes de convergence 17 moyenne empirique autour de l'espérance, c'est-à-dire de la distribution de X, - m? La réponse à cette question, est le théorème Central Limite, il affirme que : 1-Xn m est de l'ordre de 1/n",C'est--dire de l'ordre de 1/n
119,"2- La distribution de Km approche la même distribution, lorsque n devient grand, quelle que soit la distribution des X; : tant que ceux-ci ont une variance G2 finie. Théorème 2.2.2 -= Théorème central limite. Soit (Xn)neN- une suite de v.a.r. i.i.d. de carré X1 Xn",Théorme 2.2.2 -= Théorme central limite.
120,"intégrable, de moyenne m = E(X1) et de variance G?. Soit Xn alors pour tout n n € N* on a m",C'est un peu tre 
121,0. 2 % O o,BBC Sport takes a look back at some of the
122,- G 9 / - 1,BBC Sport takes a look back at some of the
123,2 % S96 G 5 3. Simulation de Variables aléatoires,The following are some of the key points from the
124,"La majorité des langages informatiques permettent de générer des nombres pseudo-aléatoires. Ce sont en fait des suites récurrentes entières initialisées avec des paramètres liés à l'ordinateur puis renormalisées pour être à valeurs dans [0,1). Via sa fonction ""random"", un langage fournit une suite de v.a. indépendantes de loi uniforme sur [0,1). A partir de cette suite de v.a. indépendantes, on doit être capable de construire une suite de","A fonction ""random"", un langage fournit indépendantes de loi uniforme sur une suite de v.a indépendant"
125,"v.a. indépendantes suivant n'importe quelle loi. 3.1 Simulation de Variables aléatoires discrètes Simulation d'une V.a. de Bernoulli. Pour simuler une v.a. de Bernoulli de paramètre P E10,1,il suffit de simuler une v.a. U uniforme sur J0, 1[ et de poser X = lusp)ic-à-d X =1 si U < P avec probabilité P et 0 sinon.","Une v.a. de Bernoulli de paramtre P E10,1,il suffit de"
126,"Simulation d'une v.a. binomiale. Pour simuler une v.a. binomiale de paramètres n > let E0,15, il suffit de sommer n v.a. de Bernoulli de paramètre p indépendantes. Simulation d'une V.a. discrète. Pour simuler une v.a. discrète à valeurs dans fx/xk € NJ de loi Pk = P(X = xk), k € N, il suffit de simuler une v.a. U uniforme sur J0,1[et de poser X = Xk si Pk-1 <U<P où Pk est le cumul des Pk : Pk - P2 + + Pk avec Po - 0.","Pour simuler une v.a. binomiale de paramtres n > let E0,15, il suffit de sommer n v.a."
127,"3.2 Simulation de Variables aléatoires à densité 3.2.1 Simulation d'une v.a. admettant une densité continue par inversion de la fonction de répartition Proposition 3.2.1 Soit. X une v.a. de densité f continue et de fonction de répartition F inversible. On pose Y = F(X). Alors Y suit une loi uniforme sur [0,1 1).",3.2 Simulation de Variables aléatoires  densité 3.2.1 Simulation d'une v.
128,"20 Chapitre 3. Simulation de Variables aléatoires Pour simuler la v.a. X, si F -1 est connue analytiquement, il suffit donc de simuler une loi uniforme sur J0, 1[ et de calculer F-1 du résultat obtenu. 3.2.2 Simulation d'une v.a. bornée à densité bornée par la méthode de rejet",Le Simulation de Variables aléatoires pour simuler la v.a.
129,"Proposition 3.2.2 Soit X une v.a. presque sûrement bornée de densité f presque partout bornée. On suppose pour simplifier que X € [0, 1] et on choisit m tel que f K m p-p. Soient U et V deux v.a. indépendantes de lois uniformes sur [0, 1 et [0,m) respectivement. Alors, conditionnellement à [V<F(U), U a même loi que X. A partir de ce résultat, on construit la procédure de simulation suivante : étant donnée une v.a. X","A partir de ce résultat, on construit la procédure de simulation suivante : étant don une v.a."
130,"comme dans la proposition, on procède à la simulation de U et V indépendantes. Si les réalisations u et V de ces v.a. satisfont V < f(u) la réalisation de la simulation de X est u, sinon on procède à un autre tirage des v.a. U et V. 3.2.3 Simulation d'une loi normale : Danns certains cas, la connaissance des v.a. repose sur un changement de variable; ainsi si une v.a.",La simulation de U et V  f(u)  f(u)  un autre tirage des v.a.
131,"s'exprime comme une fonction de v.a. indépendantes il est plus simple de simuler ces v.a. et de reconstituer la v.a. de départ à travers une fonction que de chercher à la simuler directement. La loi normale est un exemple de ce principe : Proposition 3.2.3 Soient U et V deux v.a. indépendantes de loi uniforme sur J0, 1. On pose X = V-2lnU COS 2rV, Y = V-2InU sin 2mV.",La loi normale est un exemple de ce principe : Proposition 3.2.3 Soient U et V deux v.a.
132,"Alors X et Y sont des V.a. indépendantes de loi (0,1). Pour générer une V.a. (0, 1), il faut générer deux V.a. indépendantes de loi uniforme sur J0,1. puis utiliser l'une des deux formules précédentes. 3.3 Méthode de Monte Carlo pour le calcul des intégrales et des moments : La méthode de Monte Carlo pour faire des calculs d'intégrales est une méthode basée sur la",Une méthode de Monte Carlo s'agit  l'arrivée de la Coupe de la Ligue.
133,"simulation aléatoire, elle repose sur la loi des grands nombres. Sous certaines hypothèses, on peut approcher presque sûrement l'espérance d' une v.a. (une intégrale dans le cas à densité) par la moyenne arithmétique de v.a. de même espérance. Le principe de la méthode est alors de simuler ces v.a. et de considérer la réalisation de leur moyenne arithmétique comme une approximation numérique de leur espérance commune.",Le principe de la méthode est alors de simuler ces v.a.
134,"3.3.1 Méthode On suppose que l'on cherche à calculer numériquement I dx pour une fonction f intégrable. A l'aide de changements de variables, on se ramène au calcul de g(u)du pour une certaine",A l'on  calculerériquement Icher
135,"0 fonction g. Etant donnée U une v.a. de loi uniforme sur ]0, 1. Alors la formule de transfert s'écrit E(g(U)) = g(w)lpoa(u)du = g(u)du R",La formule de transfert s'écrit
136,Jo 3.3 Méthode de Monte Carlo pour le calcul des intégrales et des moments : 21 1 Ainsi en posant g(u) = I () )+F(-1-),"All photographs courtesy of AFP, EPA, Getty Images"
137,"u) pour u €]0, 1on a f(x)dx = g(u)du R 0 Soit (U)m21 une suite de v.a. indépendantes suivant une loi uniforme sur J0, 1. La loi des grands",La loi des grandsdépendantes suivant
138,nombres affirme que 1 E8(U) g(u)du p.s. quand n * +oo. n i-1 Jo,Nombres affirme que 1 E8(
139,Une approximation numérique (estimateur) de l'intégrale I est alors donnée par la réalisation de cette somme. 3.3.2 Précision (estimation d'erreur) : On approche donc une constante (l'intégrale ou l'espérance) par la réalisation d'une v.a. Remarquons qu'il n'est pas possible de caractériser avec certitude l'erreur d'approximation que,Une approximation numérique (estimateur) de l'intégrale I est alor
140,"l'on commet, cependant on a les informations suivantes sur la v.a. : Ui) U g(u)du, Jo","L'ont, cependant on a les"
141,"ceci veut dire que l'on approche la valeur de I sans erreur en moyenne. d'autre part, Var E8(U)) Var(g(U)), n ce qui montre que l'erreur quadratique moyenne tend vers 0 en O(1/n) quand n tend vers +oo.",Var E8(U) Var(g) Var(g(U)
142,"3.3.3 Inérêts de la méthode Cette méthode converge assez lentement et repose sur la qualité du générateur de nombres aléatoires dont on dispose mais elle est très robuste. En particulier, en grandes dimensions @ 3), elle est plus rapide et plus simple d'utilisation que les intégrateurs numériques. 3.3.4 Calcul approché des moments",3.3.3 Inérts de la méthode Cette méthode converge assez
143,"On peut aussi utiliser cette méthode pour l'approximation des moments des V.a. En effet, toujours par la loi des grands nombres, l'espérance El(x)] d'une v.a. X pour une fonction à valeurs réelles 9 telle que P(X) est intégrable, peut être approchée p.s. quand n grand par E(p(x)) & - (X:) n =",Une fonction  valeurs réelles 9 telle que P(X) est intégrable
144,"pour toute suite (Xi)i21 iid à X. Même lorsque X admet une densité f, cette approximation peut être beaucoup plus précise àn petit que le calcul direct de la valeur (x)f(x)dx R","All photographs  AFP, EPA, Getty Images and Reuters"
145,"car les points alors choisis par la méthode de Monte Carlo se font selon la densité f et non unifor- mément. 22 Chapitre 3. Simulation de Variables aléatoires 3.4 Méthode de ""simulation Monte Carlo""",La méthode de Monte Carlo se font selon la den
146,"La méthode de ""simulation Monte Carlo"" est un moyen qui permet de calculer une approximation numérique de la fonction de répartition, mais également de calculer des probabilités faisant interve- nir des phénomènes dont on ne connaît pas la distribution explicitement (phénomènes complexes dépendant de variables multiples). On suppose que l'on sait simuler une v.a. X (par exemple X est fonction d'un vecteur aléatoire dont","La thode de "" Monte Carlo"" est un qui permet de calculer une approximationérique de la fonction de répartition"
147,"on connait la loi). Pour calculer la probabilité P(X € B), on remarque que, si X a pour densité f, P(X E B) f(x)dx = 1p(x)f(r)dx = E(1B(X)). B /R",Pour calculer la probabilité P(X 
148,"Cette valeur, d'après la section précédente, peut s'approcher en simulant un grand nombre de fois la variable X et en faisant la moyenne arithmétique des valeurs des réalisations obtenues de 1B(X), c-à-d : P(X € B) & X 1B(X:) n","Cette valeur, d'aprs la section cédente,"
149,"pour des v.a. X; iid de même loi que X. Autrement cardi/X,EB) P(X E B) & n c-à-d la probabilité que X soit dans B est approchée par la proportion de simulations observées",D'aprs-mme  l'
150,"dans B. Enfin, grâce à cette méthode, on peut également calculer une distribution approchée de phé- nomènes aléatoires complexes. Dans le cas d'une variable aléatoire X à densité, par exemple, il s'agit d'obtenir une densité approchée, par exemple par une fonction constante par morceaux.","Dans le cas d'une variable aléatoire X  densité, par"
151,"Pour cela, on discrétise l'ensemble des valeurs prises par X en intervalles I "" assez petits "" : puis on approche P(X EI) par la méthode de Monte Carlo pour chaque intervalle I. On obtient ainsi une approximation discrète de la densité de X. Exercice (calcul de T par méthode de Monte Carlo) On cherche à évaluer T en remarquant que T -",Exercice (calcul de T par méthode de Monte Carlo) on cherche  évaluer T en re
152,"lp(x,y)dxdy, par une méthode de Monte Carlo. 1. Montrer que T = 4.E (1p(x,Y)),ou X et Y sont deux variables aléatoires indépendantes uniformes sur [-1,1). 2. Proposer un estimateur P, de T par la méthode de Monte Carlo.",Une méthode de Monte Carlo peut--tre
153,3. Quelle est l'espérance et la variance de PA. 2 % O o,A look at some of the key findings from the
154,- G 9 / - 1,BBC Sport takes a look back at some of the
155,2 % %  5,BBC Sport takes a look back at some of the
156,"4. Modèles stochastiques 1 4.1 Processus stochastiques 4.1.1 Introduction . Dans un modèle déterministe, partant d'un état initial, on a un seul état final. Ce modèle est","Dans un modle déterministe,"
157,"parfaitement connu et il n'ya aucun phénomène aléatoire. e Un modèle stochastique intégre une part d'aléa. En fait dans la réalité, il y'a de l'aléa : ( une panne de réveil , un retard à la SNCF, rencon- trer un ami, qui va gagner un match de foot-ball ? ...) Dans la réalité, partant d'un état initial, il y'a plusieurs états finaux;1 la connaissance est imparfaite en plus il y'a des phénomène aléatoires.","Dans la réalité, il y'a de l'aléa : ( une panne de réveil , un retard  la"
158,"On parle alors de stochasticité pour tout ce qui échappe au déterminisme. Un système stochastique est un système évoluant de manière probabiliste dans le temps. On trouve plusieurs exemples tels que la température quotidienne ou un centre d'appels téléphoniques. Un modèle stochastique est une représentation mathématique d'un système stochastique. Parmi les modèles stochastiques, on peut citer les processus stochastiques tels que les proces-",Un modle modle stochastique d'un centre d'appelsphoniques peut-on tre
159,"sus de Wiener, de Poisson, de comptage, de Markov, les processus gaussiens et les files d'attente. 4.1.2 Processus stochastiques Définition 4.1.1 Soit (,9,P) un espace probabilisé. Unj processus stochastique est une famille de variables aléatoires X = (X:)IET, T appelé ensemble d'indices souvent assimilé au temps (T = R+,N, , définies sur (2,9,P) et à valeurs dans un espace métrique S muni de la tribu","Processus stochastiques Définition 4.1.1 Soit (,9,P) un espace probabilisé."
160,"borélienne. Pour chaque 0 fixé dans , l'application qui à t associe X,(0) est appelée trajectoire ou réalisa- tion du processus. 24 Chapitre 4. Modèles stochastiques",C'est un peu t--
161,"S est appelé espace des états ou des phases. Par exp. S peut être R, C, un ensemble fini ou dénom- brable. Si T est fini ou dénombrable, on dit que le processus est discret; si T n'est pas dénombrable, on parle de processus continu. Exemple : Marche aléatoire","Si T n'est pas dénombrable, on dit que le processus est discret;"
162,"la marche aléatoire discrète à une dimension sur le réseau périodique Z est le modèle de marche aléatoire le plus simple. Imaginons un individu (ou "" particule "") sur un escalier, qui tire à pile ou face pour décider si le prochain pas sera vers le haut ou vers le bas. A chaque étape, il n'y a que deux possibilités : un pas en avant ou un pas en arrière. Notons :",Une marche aléatoire s'il avait  l'occasion d'un escalier et  l'occasion d'
163,"P(O<P < 1) la probabilité que la particule fasse un saut en avant (plutôt qu'un saut en arrière); c'est le seul paramètre libre du problème; q 1 - P la probabilité que la particule fasse un saut en arrière. Le cas le plus simple (mouvement brownien), consiste à faire considérer que les directions ""avant / arrière"" sont équivalentes (équiprobabilité) : P 9 - 1/2.","C'est le cas le plus simple (mouvement brownien), consiste  faire considérer que les directions ""avant /"
164,"Chacun des tirs au hasard pour choisir le mouvement constitue une épreuve de Bernoulli avec issues équiprobables. Après n pas, si N désigne le nombre de fois où on tire ""pile"", alors N suit la loi binomiale B(n,1/2) et on a 1","Alors d'un autre spécifique, alors"
165,"P(N = k) 2"" Notons Xn la position de la particule après n pas. En prenant Xo = 0 au départ et en ajoutant 1 pour chaque pas en avant (pile) et en retranchant 1 pour chaque pas en arrière (face), on a X, = N (n - N) = 2N, n.",Chaque pas en avant (pile) et en retranchant 1 pour chaque pas en arrire
166,"Ceci montre que par rapport à la loi binomiale classique, Ça revient à décaler les résultats de n/2 et de multiplier par 2. Dans ce cas, Xn = k si 2N, n - k, nécessairement n + k est pair; ainsi N = n+k avec n et k de même parité; On en déduit que P(X si n etk ont même parité","Dans ce cas, Xn = k si 2N, n - k, nécessairement n + k est pair; a"
167,"0 sinon Définition 4.1.2 = Processus à accroissements indépendants. Un processus (X:)IER+ est dit à accroissements indépendants si Vn > 1,0<11 < In, les accroissements Xi2 X,",Processus  accroissements indépendant
168,"X, Xin-1 sont indépendants. Définition 4.1.3 = Processus à accroissements stationnaires. Soit un processus (X:)IER+. La propriété de statonnarité des accroissements signifie que la loi de X; - X; ne dépend que de la longueur t - S de l'intervalle de temps : pour t N S, X - Xs est de même loi que X-s. 4.2 Processus de Wiener ou mouvement brownien","Mae tte tte,  , , , , , "
169,"Un processus (W:)IER+ réel est un processus de Wiener ou mouvement brownien standard s'il est issu de 0 c-à-d W(0) 0 et s'il est à accroissements indépendants et stationnaires tel que Wi 2 N(0,t) pour tout t > 0. Le mouvement brownien est le nom donné aux trajectoires irrégulières du pollen en suspension dans l'eau, observé par le botaniste Robert Brown en 1828.",Un processus (W:)IER+ réel est un processus de Wiener ou mouvement brownien standard s'il
170,"4.3 Processus gaussien 25 4.3 Processus gaussien Unj processus (X:)ET à valeurs dans R est dit processus gaussien si toute combinaison linéaire finie de (X)IET est une variable gaussienne (suit une loi normale), c-à-d",Processus gaussien Unj processus (X:)ET 
171,"Vn , 11 in Vai, d2, Cn € R : a,X + d2Xi2 + + CyXin 2 N (my,o,). 4.4 Processus de Poisson Définition 4.4.1 - Processus de comptage. Désignons par N(t) le nombre de ""tops"" se produisant dans l'intervalle de temps [0,t). Le processus (N(t)r20 est dit processus de comptage",Processus de Poisson Définition 4.4.1 - Processus de comptage.
172,"s'il vérifie : e N(O) = 0; Vt N 0, N() € N; H N(t) est croissante. Pour tout OKa<b, N(b) - N(a) représente le nombre de tops se produisant dans l'inervalle de","Pour tout OKab, N(a) res"
173,"temps Ja,b). Si (TA)meN est une suite de v.a.r. positives, alors le processus (N(t)20 défini par N() = L 1s) n>1 est un processus de comptage. et on a 0 si t<Ti",Si (TA)meN est une suite de v.a.
174,N() n si IKi<Tt Définition 4.4.2 = Processus de Poisson. Un processus de Poisson de densité A > 0 est un processus de comptage (N(t)20 tel que : e le processus est à accroissements indépendants;,Un processus de Poisson de densité A > 0 s'
175,"e le nombre de tops se produisant dans un intervalle de temps de longueur t > 0 suit une loi de Poisson de paramètre At, c-à-d (Ar)"" Vs 0, > 0, Vn € N, P( V N(s)",De temps de longueur t > 0 suit une loi de Poisson
176,"n! Remarque : Les processus de Poisson sont souvent utilisés pour modéliser des files d'attente, chaque top représentant l'appel d'un client au guichet. 4.5 Chaines de Markov 4.5.1 définitions et propriétés",Remarque : Les processus de Poisson sont so
177,"Définition 4.5.1 - Processus de Markov. Un processus de Markov est un processus stochas- tique satisfaisant la propriété de Markov, c-à-d Vti 12 In / 1, VA € B(R), P(X E AIXi, Xi2, Ain = P(X EAIX.) p.s. Cette propriété (de Markov) exprime que le passé et le futur sont incondtionnelement indépendants,",Un processus de Markov est un processus stochas- tique satisfaisant la propriette
178,"le présent étant donné. En d'autres termes, pour un processus de Markov l'information utile pour la prédiction du futur est entièrement contenue dans l'état présent et ne dépend pas des états antérieurs; un tel processus est sans mémoire. Un processus de Markov en temps discret est une suite (Xn)n de variables aléatoires vérifiant la",Un processus de Markov en temps discret est suite (Xn)n de variables aléatoires v
179,propriété précédente. La valeur X étant l'état du processus à l'instant n. 26 Chapitre 4. Modèles stochastiques Définition 4.5.2 = Chaine de Markov. Une chaine de Markov est un processus de Markov à temps discret ou à temps discret et à espace d'états fini ou dénombrable.,Une chaine de Markov est un processus de Markov  temps discret ou 
180,"Dans ce cas, la loi conditionnelle de Xn+1 sachant le passé s'exprime par Vn € N, Vio, i1, in-1,i,, P(Xn+1 = jXo i0, X1 11 - X, P(Xn+1 jX=i). Exemple : Soit R, n N 0 des variables indépendantes à valeurs dans E - N. Alors Sn","Dans ce cas, la loi conditionnelle de Xn+1 sachant le passé "
181,"R; et P L R; sont des chaines de Markov. i-1 Une chaine de Markov peut être vue comme un système dynamique, ce qui veut dire que Xn+1 = fn(Xn), où fn est une ""transformation aléatoire"" indépendante du passé. Dans l'exemple",Une chaine de Markov peut tre vue un syst
182,"précédent, fn(X) est la somme (ou le produit) de X, (ici X, = Sn ou P) avec Rn+1- Si la transformation aléatoire fn ne dépend pas de n, c-à-d si Xn+1 = f(Xn) pour tout n pour une certaine transformation f, on dit que X est une chaine de Markov homogène. Définition 4.5.3 Soit X = (X)n2o une chaine de Markov à valeurs dans E fini. On appelle probabilité de transition de l'état ià l'état j en un pas (ou simplement probabilité de transition",Une transforme de Markov homogne s'est  l'auteur de l'étranger fn(X).
183,"de l'état ià l'état j) le nombre P(Xn+1 = jXn = i). On note Pij = P(Xn+1 = jX, = i). Définition 4.5.4 Une chaine de Markov est dite homogène si ses probabilités de transition ne dépendent pas de n, c-à-d si Vn N 1, V(i,j)EE?, P(X+1 = jX,=1)=P(X, = jX-1=i). Remarque : Une chaine de Markov est homogène si","Une chaine de Markov est homogne si ses probabilités de transition ne dépendent pas de n, c"
184,"Vn > 1, V(i,j) € E?, P(X+1 = jX, P(X1 jXo i). Proposition 4.5.1 Soit une suite Y = (Kn)m21 de variables aléatoires indépendantes et de même loi, à valeurs dans un espace F, et soit f: E X. F E une application mesurable. Soit la suite X = (Xn)n2o définie par : Vn € N, Xn-1 = f(Xn,En+1). On suppose que la suite Y est indépendante","Soit une suite Y = (Kn)m21 de variables aléatoires indépendantes et de mme loi,"
185,"de Xo. Alors X est une chaine de Markov homogène. Remarque : Toute chaine de Markov homogène peut être simulée via une relation de récurrence de la forme Xn-1 = f(Xn,In-1) pour une fonction f bien choisie. Dans la suite, toutes les chaines de Markov sont supposées homogènes. 4.5.2 Probabilités et matrice de transition",Alors X est une chaine de Markov homogne.
186,"Il s'agit de modéliser une chaine de Markov à l'aide de représentations synthétiques afin de connaitre l'évolution des états du système. On utilisera les matrices ou les graphes. Définition 4.5.5 On appelle matrice de transition (quand E est fini) ou noyau de transition ou opérateur de transition (quand E est infini) la famille des nombres P = (Pij)G.)EE2. Proposition 4.5.2 La matrice de transition P = (Pij)G.)EE2 est stochastique, c-à-d",On s'agit de modéliser une chaine de Markov  l'aide de resentations synthétiques afin de con
187,"V(i,j) EE2,Pij20 ViEE, Epij = 1 (la somme des termes de chaque ligne de P est égale à 1). jEE 4.5 Chaines de Markov 27","V(i,j) EE2,Pij"
188,"Remarque : La matrice d'une chaine de Markov est forcément stochastique et inversement, toute matrice stochastique est la matrice d'une chaine de Markov. Pour k N 1, la probabilité de transition de l'état ià l'état j en k pas (sur k étapes), P(Xntk = jXn =i) ne dépend pas de n, c-à-d P(Xntk = jXn i P(Xk - jXo i). On note Pi9 - P(Xk - jXo - i).","La matrice d'une chaine de Markov est forcément stochastique et inversement, toute matrice d'une chaine de"
189,"Exemple : Une grenouille monte sur une échelle. Chaque minute, elle peut monter d'un barreau avec probabilité 1/2, ou descendre d'un barreau avec probabilité 1/2. L'échelle a 5 barreaux. Si la grenouille arrive tout en haut, elle saute en bas de l'échelle avec probabilité 1/2 ou redescend d'un barreau. On appelle. X, la position de la grenouille sur l'échelle. L'espace d'états est donc E = (0,1,2,3,4,5).",L'échelle a 5 barreaux.
190,"Si a un instant n la grenouille est au niveau i € £1,2,3,4) de l'échelle, alors à l'instant n + 1 elle sera : au barreau i+ 1 avec probabilité 1/2 au barreau i- 1 avec probabilité 1/2. ce qui s'exprime par",Alors  l'instant n + 1 elle sera : au barre
191,"P(Xn+1 =i+ 1Xn = 1/2 = P(Xi Xo P(Xn+1 =i- Xn - = 1/2 (= P(Xi 1 1Xo Ces probabilités ne dépendent pas de n, il parait qu'il s'agit d'une chaine de Markov homogène. Si",P(Xi Xo P(Xn+1 =i+ 1X
192,"c'est le cas, la matrice de transition se traduit par : * * * * * * 1/2 0 1/2 0 0 0 0 1/2 0 1/2 0 0 P=","C'est le cas, la matrice de transition se trad"
193,"0 0 1/2 0 1/2 0 0 0 0 1/2 0 1/2 * * * * * * Si la grenouille se retrouve à l'état 5, alors elle peut soit passer à l'état 4, soit passer à l'état 0. La dernière ligne de la matrice est donc","Si la grenouille se retrouve  l'état 5, alors "
194,"( 1/2 0 0 0 1/2 0), là encore cela ne dépend pas de l'instant n. Si la grenouille est à l'état 0, elle ne peut que passer à l'état 1. La première ligne de la matrice est alors 0 I 0 0 0 0",La premire de la matrice est  l'instant n.
195,"(Xn) est bien une chaine de Markov homogène, avec matrice de transition P. 4.5.3 Propriétés des chaines de Markov homogènes Soit X = (Xn)m2o une chaine de Markov. Soit n € N. II ne faut pas confondre ""loi de X"" et ""loi conditionelle de Xn sachant Xn- 1 "". La seconde se calcule facilement à l'aide de la matrice de transition.",Les chaines de Markov homognes Soit X = (Xn)m2o une chaine de Markov s'
196,"28 Chapitre 4. Modèles stochastiques Définition 4.5.6 On appelle loi initiale de X la loi de Xo, c-à-d la donnée des valeurs : T'(io) = P(Xo = io), io € E. Proposition 4.5.3 - loi de dimension finie d'une chaine de Markov - La loi d'une chaine de","On appelle loi initiale de X la loi de Xo, c--d la"
197,"Markov X = (Xn)n2o est caractérisée par sa matrice de transition P et sa loi initiale : pour tout n N 1, la loi jointe de (Xo,X1, Xn) est donnée par P(Xo i0, X1 X0 - io), Pioi Piiz Pin-in I Définition 4.5.7 Un chemin est simplement la donnée d'un vecteur fini (xo,X1 Xn) € E"".","Une vecteur fini (xo,X1 Xn)  E, Pioi Piiz Pin-"
198,"Dans la prop. précédente, la probabilté P(Xo - io, X1 1- Xn In) est la probabi- lité de suivre le chemin (io,i1,. ,in). Dém. : En effet, en posant ak = P(Xo = io, X1 Xk - ik); par récurrence sur n",C'est un peu t- t- t- 
199,"on a : dn+1 = P(Xn+1 = in+1Xo = io, Xi - 1. Xn-1 in-1,Xn - n) dn - P(Xn+1 = in+1Xn - in)P(Xo = io) Piphl Piyiz Pin-in (pté de Markov) - P(Xo = io) PiphPiiz Pin-1iPinintl"" Proposition 4.5.4 = Propriété de Chapman-Komogorow la matrice de transition en k pas,",Proposition.4 = Propri+1 de Chapman-Komogo la matrice de transition en k pas.
200,"(P(Xntk = jXn )) (,J)EE2 est égale à la puissance kème de la matrice de transition P. En notant PiR = P(Xntk = jXn =i) et p(K) = (Pij (k) G)EE2 ? on a p(K) - pk. Proposition 4.5.5 = lois marginales. La loi de Xn+k est donnée par P-D-LPX. -Dp IEE",Voters in the UK go to the polls on 8 June to choose their preferred candidate to run in the forthcoming general election.
201,"En particulier, on a P-D-EPA. = i)pij iEE et par récurrence, on a P(X, = j) X P(Xo )Pi (n)",= i)pij iEE et par ré
202,"IEE Forme matricielle : On note T"" la loi ou distribution de Xn;c'est le vecteur-ligne T"" = (T)IEE où T = P(X, = i). Les deux formules précédentes se réecrivent ""tk-m""pk et T""+1 = T""P, ainsi T""=p"" (équations de Chapman-kalmogorow,",Le vecteur de Chapman-kalmogorow et le vecteur
203,"Exercice : Une chaine de Markov avec états E = £1,2) a la matrice de transition a P a EJ0,1. I - a a",Une chaine de Markov avec état
204,"1. Diagonaliser P. 2. Calculer P"" pour n N 1. 4.5 Chaines de Markov 29 3. Calculer la loi de X pour tout n, sachant que l'on part de l'état Xo = 1.","Calculer la loi de X pour tout n, sa"
205,"Exemple 1: Doudou le hamster Exemple 2: The Gardener Problem (Problème du jardinier : [voir H. Taha] Every year, during the Marcth-tmroughsepember growing season, a gardener uses a chemical test to check soil condition. Depending on the outcome of the test, productivity for the new sea- son can be one of three states : (1) good, (2) fair, and (3) poor. Over the years, the gardener has observed that","Exemple 1: Doudou le hamster Exemple 2: The Gardener Problem (Problme duier : [voir H. Taha] Every year,"
206,last year's soil condition impacts current year's productivity and that the situation can be described by the following Markov chain : State of the system next year,"In our series of letters from African journalists, filmmaker and"
207,1 2 3 State of 1 .2 .5 3 P = the system 2 0 5 5 this year 0 0 1,The winning numbers in Saturday evening's drawing of
208,"FIGURE 4.5.1 The transition probabilities show that the soil condition can either deteriorate or stay the same but never improve. For example, if this year's soil condition is good (state 1), there is a 20% chance it will not change next year, a 50% chance it will be fair (state 2), and a 30% chance it will deteriorate to a poor",The graph shows the transition probabilities between good and poor soil conditions.
209,"condition (state 3). The gardener alters the transition probabilities by using organic fertilizer. In this case, the transition matrix becomes : 1 2 3 1/ .30 .60 .10 PI = 2 .10 .60 .30","In this case, the gardener alters the transition probabilities by using organic fertilizer."
210,3 .05 .40 .55 FIGURE 4.5.2 The use of fertilizer can lead to improvement in soil condition. II y'a 10% de chance que l'état du sol change de moyen vers bon c-à-d de l'état 2 vers l'état 1; 5% de faible vers bon et 40% faible vers moyen.,"Fertiliser use has been shown to improve soil condition, but how effective is it?"
211,"Exercice 1 : An engineering professor acquires a new computer once every two years. The professor can choose from three models : M1, M2, and M3. If the present model is M1, the next computer can be M2 with probability .25 or M3 with probability .1. If the present model is M2, the probabilities of switching to M1 and M3 are .5 and .15, respectively. And, if the present model is M3, then the probabilities of purchasing M1 and M2 are .7 and .2, respectively. Represent",Exercice 1 : An engineering professor acquires a new computer once every two years.
212,"the situation as a Markov chain. Exercice 2: A police car is on patrol in a neighborhood known for its gang activities. During a patrol, there is a 60% chance of responding in time to the location where help is needed; else regular patrol will continue. Upon receiving a call, there is a 10chance for cancellation (in which 30",Exercice 2: A police car is on patrol in a neighbourhood known for its gang activities.
213,"Chapitre 4. Modèles stochastiques case normal patrol is resumed) and a 30% chance that the car is already responding to a previous call. When the police car arrives at the scene, there is a 10% chance that the instigators will have fled (in which case the car returns back to patrol) and a 40% chance that apprehension is made immediately. Else, the officers will search the area. If apprehension occurs, there is a 60% chance","There is a 40% chance that the police car is already on its way to the scene (i.e. When the police car arrives at the scene, there is a 10%"
214,of transporting the suspects to the police station; else they are released and the car returns to patrol. Express the probabilistic activities of the police patrol in the form of transition matrix. Retour au problème du jardinier avec les engrais : Dans la suite la matrice Pi est notée P. 1 2 3 1/.30 .60 .10,A police car is parked in the middle of a street.
215,"P = 2] .10 .60 .30 3 .05 .40 55/ FIGURE 4.5.3 The initial condition of the soil is good that is (0) - ( 1 0 0 ). Determine the absolute probabilities of the three states of the system after 1, 8 and 16 gardening seasons.","The probabilities of the three states of the system after 1, 8 and 16 gardening seasons."
216,.30 .60 .10/ 30 .60 .10) .1550 5800 .2650 p2 = .10 .60 .30 .10 .60 .30 = .1050 .5400 3550 .05 .40 .55 - - .05 .40 .55/ .0825 .4900 .4275,.30 .60 - - - - - - - -
217,.1550 .5800 .2650 .1550 .5800 .2650 P* = .1050 5400 .3550 .1050 5400 .3550 .0825 .4900 .4275/ .0825 .4900 .4275 .10679 .53295 .36026) = .10226 .52645 .37129,P* = .1050 .3550 
218,.09950 .52193 .37857) .10679 53295 36026) .10679 .53295 36026) ps = .10226 .52645 .37129 .10226 .52645 37129 .09950 .52193 .37857/ .09950 .52193 .37857/ .101753 525514 .372733,The BBC's science and technology correspondent Nick Triggle
219,.101702 .525435 .372863 .101669 525384 .372863 ArtiveWindawe FIGURE 4.5.4 4.5 Chaines de Markov,.101702 .525435 .
220,31 .101753 525514 372733 .101753 .525514 372733 p16 = .101702 .525435 .372863 .101702 525435 372863 .101669 .525384 372863/ .101669 525384 372863/ .101659 .52454 .372881,The winning numbers in Saturday evening's drawing of
221,.101659 52454 372881 .101659 52454 .372881 FIGURE 4.5.5 Ainsi x(1) vaut .30 .60 .10),BBC Sport takes a look at some of the key
222,(1 0 0) .10 .60 .30 = (.30 .60 .1) .05 .40 .55 FIGURE 4.5.6 .101753 525514 .372733) (1 0 0) .101702 .525435 .372863 = (.101753 525514 372733),BBC Sport takes a look at some of the key statistics behind
223,.101669 .525384 .372863/ x(8) vaut FIGURE 4.5.7 .101659 .52454 372881 (1 0 0) .101659 .52454 372881 = (.101659 52454 372881),The following table shows the value of the US dollar
224,".101659 .52454 .372881 x(16) vaut FIGURE 4.5.8 The rows of p8 and the vector of absolute probabilities a x(8) are almost identical. The result is more evident for p16 . It demonstrates that, as the number of transitions increases, the absolute",The absolute probabilities of p8 and p16 are given as vectors.
225,"probabilities become independent of the initial x(0). The resulting probabilities are known as the steady-state probabilities. 4.5.4 Classification des états d'une une chaine de Markov On s'intéresse à la ""dynamique"" des chaines de Markov. les états d'une chaine de Markov peuvent être classés sur la base de la probabilité de transition Pij",The Markov chain of Monte Carlo is a set of Monte Carlo distributions in which the coefficients of the initial x(0) and the
226,"de P. Etats accessibles à partir d'un état donné : On considère une chaine de Markov homogène. Soit (,j)EE2: Définition 4.5.8 On dit qu'un état j est accessible à partir d'un état i (ou qu'un état i conduit à un état j) si il existe n N 0 tel que P(Xn = jXo - i) > 0, c-à-d si In 20 tel que pi9 > 0. On",Etats accessibles  partir d'un état donné : Onidre une chaine de Markov homog
227,"note j tij. En particulier, un état j est toujours accessible depuis lui même, puisque Pjj (0) > 0. Classes de communication : 32 Chapitre 4. Modèles stochastiques",C'est un peu t--t
228,"Définition 4.5.9 On dit que iet j communiquent si j est accessible à partir de i et i accessible à partir de j, C- à-d s'il existe (n,m) € N2 tels que Pi >Oet Pm > 0. On note tj 47 ij. Proposition 4.5.6 La relation communiquer 47 ij"" est une relation d'équivalence. Ceci signifie que l'on peut regrouper. les éléments de E en paquets, chaque paquet regroupant tous les éléments qui communiquent entre eux. Ainsi on a une partition de E en classes d'équivalence",Les Etats-Unis ont t t t t t t t t t t 
229,"appelées classes de communication. Définition 4.5.10 Une chaine X est irréductible s'il existe une seule classe d'équivalence, c-à-d si tous les états communiquent. La notion d'iréductibilité est liée à la matrice de transition de la chaine et non à la loi initiale. Pour montrer qu'une chaine de Markov est irréductible, il peut être utile de tracer un graphe orienté dont",Une chaine de Markov est irréductible s'il existe une seule classe d'équival
230,"les sommets sont les états de la chaine et où une arête représente une transition possible (l'arête (i,j) existe uniquement si Pij > 0). La chaine est alors irréductible si et seulement s'il existe un chemin fermé passant au moins une fois par tous les états de la chaine. La relation être accessible, notée $ s'étend aux classes d'équivalence : pour deux classes Cet C' on a",Les sommets de la chaine et o une arte resente resente une transition possible (l'arte
231,"C+-C $ J(i,j) ECx C. it. $ fV(i,j) ECxC,it-j) Une classe C est dite finale ou fermée si elle ne conduit à aucune autre c-à-d si la classe est minimale pour la relation t; ceci s'exprime par : pour tout WeEIeCal-y-yec Sinon, elle est dite transitoire ou transiente. Si C= fioj est fermée, io est dit état absorbant.","C+-C $ J(i,j) ECx C."
232,"Définition 4.5.11 On dit que e un état j est absorbant s'il retourne à lui-même avec certitude dans une transition c-à-d si Pij = 1. e un état j est transient s'il peut atteindre un autre état mais ne peut pas lui-même être atteint à partir d'un autre état, c-à-d si lim PIR = 0 pour tout i.",E un état j est transient s'il peut atteindre un autre état PIR ne
233,"n-0o e un état j est récurrent si la probabilité d'être revisité à partir d' autres états est 1 (c-à-d partant de j, la chaine repasse en j presque sûrement). Cela peut se produire ssi l'état j n'est pas transient. e un état j est périodique de période t > 1 si un retour n'est possible qu'en t, 2t, 3t, étapes,",C'est un état d'tre revisité  partir d'autres états d'
234,"dans ce cas la période est d(j) = t = pgedm/p) > 0j >1 1. Ceci signifie que p9 = 0 chaque fois que n n'est pas divisible par t. Si d(j) = 1,l'état j est dit apériodique. Proposition 4.5.7 Les états d'une même classe ont même période. on peut donc parler de la période d'une classe d'états.",Une période d'une mme classe ont mme période  l'
235,Définition 4.5.12 Une chaine de Markov est e irréductible récurrente si elle est irréductible et si tous les états sont récurrents; irréductible transiente si elle est irrééuctible et si tous les états sont transitoires. Proposition 4.5.8 Une chaine de Markov irréductible sur un espace E fini est irréductible récur- rente.,Proposition 4.5.12 Une chaine de Markov irréductible sur un espace E fini est
236,"Par exp., considérons la chaine de Markov définie par la matrice de transition 4.5 Chaines de Markov 33 0 1 0 0 0 0 1 0",Les Chains de Markov peut-tre
237,"P - 0 0 3 .7 0 0 .4 .6 les états 1 et 2 sont transients parce qu'ils ne peuvent pas être atteints à nouveau une fois que le système se trouve dans les états 3 et 4. En plus, les états 3 et 4 peuvent être à la fois des états absorbants si P33 = P44 = 1. Dans un tel cas,",Les états 1 et 2 sont transients parce qu'ils ne peuvent pas re atteints 
238,chaque état forme un ensemble fermé. Example 17.3-1 (Absorbing and Transient States) Consider the gardener Markov chain with no fertilizer. 5 3 P -,C'est un peu tre 
239,"5 5 0 1 States 1 and 2 are transient because they reach state 3 but can never be reached back. State 3 is absorbing because P33 = 1. These classifications can also be seen when lim Pi) = Ois computed. For example,",The following table shows the different classifications of states.
240,"-00 p(100) = 1 which shows that in the long run, the probability of ever reentering transient state 1 or 2 is zero, whereas the probability of being ""trapped"" in absorbing state 3 is certain.","The probability of ever absorbing state 1 or 2 is zero, whereas the probability"
241,"FIGURE 4.5.9 Example 17.3-2 (Periodic States) We can test the periodicity of a state by computing P"" and observing the values of pl) for n = 2,3,4, These values will be positive only at the corresponding period of the state. For FIGURE 4.5.10",The periodicity of a state can be tested by computing P and observing the values of pl)
242,"34 Chapitre 4. Modèles stochastiques example, in the chain P - 1 0","Modles stochastiques examples,"
243,4 we have .24 .76 0 .904 .0960) .0576 .9424 0,BBC Sport takes a look at some of the best
244,p2= 0 1 0 P- 0 1 0 P* = 0 1 0,The winning numbers in Saturday evening's drawing of
245,0 .76 24 .144 .856 0 0 .9424 .0576) .97696 .02304 =,Match reports from the weekend's Premier League games
246,U 1 0 .03456 .96544 0,BBC Sport takes a look back at some of the
247,"Continuing with n = 6,7, P"" shows that Pil and P33 are positive for even values of n and zero otherwise. This means that the period for states 1 and 3 is 2. FIGURE 4.5.11 Exemple 1: Considérons la chaine de Markov définie par le graphe : 0,5",The following graph shows the Markov chain of states:
248,"0,9 2 0,25 0,4 0,25",BBC Sport looks back at some of the key moments
249,"0,1 0,5 0,5 1 0,6 0,5",BBC Sport takes a look back at some of the
250,"0,5 FIGURE 4.5.12 cette chaine de Markov est réductible. On a 3 classes : £1,3), f2), (4,5j. Létat 2 est transitoire.","On a 3 classes : £1,3), f"
251,"Les classes £1,3), et (4,5) sont finales. Quitte à renuméroter les états, on peut écrire : 1 3 4",Une pédagogique s'
252,"5 2 1 0,5 0,5 0 0 0 3",The winning numbers in Saturday evening's drawing of
253,"0,4 0,6 0 0 0 P = 4",The winning numbers in Saturday evening's drawing of
254,"0 0 0,1 0,9 0 5 0 0 0,5 0,5 0",BBC Sport takes a look at some of the key
255,"2 0,25 0,25 0 0 0,5/ 4.5 Chaines de Markov 35",Match reports from the weekend's Premier League games
256,"FIGURE 4.5.13 Exemple 2: Soit le graphe 0,4 0,3 1",Figure 4.5.13 Exemple 2: Soit le
257,"2 0,3 0.8 0,2 FIGURE 4.5.14",BBC Sport looks at some of the key statistics behind
258,"Les classes sont f1,2) et £3). Une seule classe finale : (3). L'état 3 est absorbant. Exemple 3 : Considérons cette fois-ci la chaine de Markov définie par le graphe : 0.4","Une classe finale : (1), (2), (3), (4)"
259,"0,2 1 2 3 0,8",The winning numbers in Saturday evening's drawing of
260,"0,6 0,2 0,25 0,8 0,9 0,25 0,1",BBC Sport takes a look at some of the key
261,"6 5 4 0,75 0,75",The winning numbers in Saturday evening's drawing of
262,"FIGURE 4.5.15 Ici, la chaine est irréductible. Chaque état est périodique de période 2. 1 3",Chaque état périodique
263,5 2 4 6 1,The winning numbers in Saturday evening's drawing of
264,"0 0 0 0,8 0 0,2 3 0 0",The winning numbers in Saturday evening's drawing of
265,"0 0,2 0,8 0 P= 5 0 0 0",The winning numbers in Saturday evening's drawing of
266,"0 0,75 0,25 2 0,4 0,6 0 0 0",Match reports from the weekend's Premier League games
267,"0 4 0 0,9 0,1 0 0 0",The winning numbers in Saturday evening's drawing of
268,"6 (0,25 0 0,75 0 0 0 FIGURE 4.5.16",Find out more at www.bbc.co.
269,"36 Chapitre 4. Modèles stochastiques Exercice : On considère la chaine de Markov sur 5 états 1, 2, 3, 4 et 5 de matrice de transition P: 1/2 0 1/2 0 0 1/3 2/3 0 0 0",C'est un peu t--tre  l
270,P= 0 1/4 1/4 1/4 1/4 0 0 0 3/4 1/4 0 0 0 1/5 4/5 1. Dessiner la chaine de Markov. 2. Classifier les états.,"All photographs courtesy of AFP, EPA, Getty Images and"
271,"3. Quelle est la probabilité en partant de 1 et en 4 étapes d'arriver en 5? Définition 4.5.13 Une chaine de Markov fermée (irréductible) est dite ergodique si tous les états sont récurrents et apériodiques (c-à-d si la matrice P admet une puissance Ph > 0). Dans ce cas les probabiltés absolues, après n étapes, T"" = mopn convergent vers une distribution limite, quand n 1 0o, qui est indépendante de la loi initiale T.",Une chaine de Markov fermée (irréductible) est dite ergodique si tous les états sont récurrents et
272,4.5.5 Lois ou probabilités stationnaires; Temps moyen de premier retour Lois ou probabilités stationnaires : L'une des problématiques les plus courantes concernant les chaines de Markov homogènes consiste à déterminer leurs distributions (ou probabilités) invariantes (ou stationnaires) et à étudier la convergence éventuelle de la chaine vers ces distributions.,Lois et Lois et Lois et Lois et Lois et Lois et Lois et Lois et Lois et Lois et Lois et
273,"Définition 4.5.14 On appelle probabilté invariante ou loi stationnaire pour P (pour la chaine) toute mesure T = (T)IEE sur l'espace d'états E vérifiant : e T = TP, ViEE, Ti >0, - Z=1.",On appelle probabilté invariante ou loi stationnaire pour
274,"IEE Définition 4.5.15 Une chaine de Markov, de matrice de transition P, est stationnaire si et seulement si sa loi initiale To est une probabilté stationnaire, c-à-d si elle vérifie TP-. Dans ce cas pour tout n, la loi de X, vérifie T"" = T. Proposition 4.5.9 1. Si l'espace d'états est fini, il existe au moins une mesure stationnaire.","Dans ce cas pour tout n, la loi de X, vérifie T"" = T. Proposition 4.5.9"
275,"2. Si la chaine est irréductible, il existe au plus une mesure stationnaire. Proposition 4.5.10 Si la chaine est irréductible sur un espace d'états fini, il existe une unique probabilité stationnaire. Proposition 4.5.11 Soit (Xn)eN une chaine de Markov à états finis f1, Nj. On suppose que pour tout iE f1,. ,Nj, (P(Xn = i))NeN converge et on note Ti sa limite. Alors T = (T1, TN)","Proposition 4.5.11 Soit (Xn)eN une chaine de Markov  états finis f1, Nj."
276,est une distribution invariante pour la chaine. 4.5 Chaines de Markov 37 Temps moyen de premier retour d'une chaine ergodique : STEADY-STATE PROBABILITIES AND MEAN RETURN TIMES,Une chaine ergodique s'est
277,"OF ERGODIC CHAINS In an ergodic Markov chain, the steady-state probabilities are defined as ""i = lim a""), j=0,1,2,... n 00 These probabilities, which are independent of (ao)), can be determined from the",The steady-state probabilities of an ergodic Markov chain are defined as
278,"equations la = mP Sm-1 (One of the equations in a = mP is redundant.) What 1 = mP says is that the prob- abilities a remain unchanged after one transition, and for this reason they represent",What is the difference between a = mP and a = mP?
279,"the steady-state distribution. A direct by-product of the steady-state probabilities is the determination of the expected number of transitions before the systems returns to a state j for the first time. This is known as the mean first return time or the mean recurrence time, and is com- puted in an n-state Markov chain as",A steady-state distribution is a Markov chain of systems in which the systems return to a state j for the first time.
280,Pij .. FIGURE 4.5.17 38 Chapitre 4. Modèles stochastiques,FIGURE 4.5.17 38 Chapitre 4.
281,"Example 17.4-1 To determine the steady-state probability distribution of the gardener problem with fertilizer (Example 17.1-3), we have .3 .6 1 (""i ""2 W3) = (i ""2 ""3) 1 .6 3",The coefficients of the coefficients of the coefficients of the coefficients of the
282,.05 4 55 which yields the following set of equations: T1 = 3m1 + .172 + .05m3 T2 = .6m + .672 + .4m3 T3 = .Imi + .3m2 + .55m3,Here is a simple equation which shows how the distance between two points
283,"7I + 12 73 = 1 Recalling that one (any one) of the first three equations is redundant, the solution is 71 = 0.1017, 2 = 0.5254, and ""3 = 0.3729. What these probabilities say is that, in the long run, the soil condition approximately will be good 10% of the time, fair 52% of the time, and poor 37% of the time.",The solution to this problem is as follows:
284,FIGURE 4.5.18 The mean first return times are computed as 1 1 1,The following table shows the mean first return times for
285,"Pi1 = - 9.83, #22 = - 1.9,33 = .3729 = 2.68 .1017 .5254",Pi 1 / 2 / 3 / 4 / 5
286,"This means that, depending on the current state of the soil, it will take approximately 10 garden- ing seasons for the soil to return to a good state, 2 seasons to return to a fair state, and 3 seasons to return to a poor state. These results point to a more ""bleak"" than ""promising"" outlook for the soil condition under the proposed fertilizer program. A more aggressive program should im- prove the picture. For example, consider the following transition matrix in which the probabili-","The following table shows that, on average, it will take a decade for the soil to return to a good state."
287,ties of moving to a good state are higher than in the previous matrix: 35 .6 .05 P .6 1 25 4 .35,The relationship between moving to a good state and moving
288,"In this case, Ti = 0.31, 72 = 0.58, and T3 = 0.11, which yields P11 - 3.2,22 = 1.7, and H33 = 8.9,a reversal of the ""bleak"" outlook given previously. 4.6 Application : 39 FIGURE 4.5.19",The following table shows the expected growth rates for the UK over the next five years
289,Example 17.4-2 (Cost Model) Consider the gardener problem with fertilizer (Example 17.1-3). Suppose that the cost of the fer- tilizer is $50 per bag and the garden needs two bags if the soil is good. The amount of fertilizer is increased by 25% if the soil is fair and 60% if the soil is poor. The gardener estimates the annual yicld to be worth $250 if no fertilizer is used and $420 if fertilizer is applied. Is it worthwhile to,Is it worthwhile to estimate the annual value of a plant?
290,"use the fertilizer? Using the steady state probabilities in Example 17.4-1,we get Expected annual cost of fertilizer = 2 x $50 x Ti + (1.25 x 2) x $50 x T2 + (1.60 x 2) X $50 x T3 = 100 X .1017 + 125 x .5254 + 160 x .3729",How much fertilizer do you need to grow a crop?
291,"= $135.51 Increase in the annual value of the yield = $420 = $250 = $170 The results show that, on the average, the use of fertilizer nets 170 - 135.51 = $34.49. Hence the use of fertilizer is recommended. FIGURE 4.5.20",The following table shows the results of a study on the effect of fertilizer on the annual yield of
292,"4.6 Application : On dispose de deux machines identiques fonctionnant indépendamment et pouvant tomber en panne au cours d'une journée avec la probabilité 9 - A On note Xn le nombre de machines en panne au début de la n-ième journée. 1. On suppose que, si une machine est tombée en panne un jour, elle est réparée la nuit suivante",The following is a description of the invention:
293,"et qu'on ne peut réparer qu'une machine dans la nuit. Montrer que l'on peut définir ainsi une chaine de Markov dont on déterminera le graphe, la matrice de transition et éventuellement les distributions stationnaires. 2. Même question en supposant qu'une machine en panne n'est réparée que le lendemain, le réparateur ne pouvant toujours réparer qu'une machine dans la journée.",C'est tre l'auteur de l'étranger de l'étranger de l'
294,"3. Le réparateur, de plus en plus paresseux, met maintenant 2 jours pour réparer une seule machine. Montrer que (Xn) n'est plus une chaine de Markov, mais que l'on peut construire un espace de 5 états permettant de décrire le processus par une chaine de Markov dont on donnera le graphe des transitions. Calculer la probabilité que les 2 machines fonctionnent après n jours (n = 1,n =2etn = 3) si elles fonctionnent initialement.",Une seule machine a fonctionnent  l'autre d'un espace de 5 états permettant de décri
295,"Corrigé : 1. L'ensemble des états est E - f0, 1 car si le soir il y a une ou aucune machine en panne, le lendemain matin, il y en aura 0; et si le soir, il y en a 2, le lendemain matin, il y en aura une seule. Le nombre de machines en panne le matin ne dépend que de celui de la veille au matin et de",Le nombre de machines en panne le matin ne dépend que de celui de la veille au matin et de le lendemain mat
296,"ce qu'il s'est passé dans la journée, ceci indépendamment de la période de l'année. On a 40 Chapitre 4. Modèles stochastiques donc bien une chaine de Markov dont il faut déterminer la matrice de transition. On a",Une période de l'Institut National de la Recherche Scient
297,Poo - 2q(1 q)=1-gou (1-q) correspond à aucune panne dans la journée et 2q(1 - q) à une seule panne qui peut provenir d'une machine ou de l'autre; e P01 = 9 (les 2 machines tombent en panne dans la journée et ces pannes sont indépendantes,The winning numbers in Saturday's drawing of the French national lottery are:
298,"entre elles); P10 - I 9 (la machine en panne est réparée et l'autre fonctionne tjs); P11 - 9 (la machine en panne est réparée et l'autre est tombée en panne). 1-4 4 Ainsi, on a la matrice : P =",P8 - I 9 (la machine en panne est réparée et l'
299,1 - 9 9 La distribution stationnaire est déterminée en résolvant (To Ti) = (To Ti) avec To + T1 - 1. 1 9 9,Watch the highlights of the Euro 2016 final between France
300,"On obtient TO = 1-q+g 1-9 et Ti 4+4 or 4 4 ainsi TO 13 et TI 13 2. Dans ce cas, l'ensembles des états est E' - 0, 1,2) car aucune machine n'est réparée la nuit. On a Poo = (1-4 q)2 (c-à-d aucune panne dans la journée);","Dans ce cas, l'ensembles des états est E' - 0, 1,2"
301,"Por = 2q(1 q) (une des 2 machines est tombée en panne); Pio =1-q (la machine qui fonctionne ne tombe pas en panne); Phi = 9 (la machine qui fonctionne tombe en panne, l'autre est réeparée; Pi2 = 0 (la machine en panne est sûre de refonctionner le lendemain); P20 = P22 = 0;","The answer to the question: ""Which of the following is the most accurate?"""
302,P21 = 1 (une seule des 2 machines en panne est réparée). D'ou la matrice (1-q)2 2q(1-4) 4? P' = 1-4 9 0,P21 = 1 (une seule des 2
303,0 1 0 La distribution stationnaire est déterminée en résolvant (1-4)2 2q(1-4) 9,A selection of photos from around the world this week
304,(o TI T2) =(To TI T2) 1-4 9 0 avec TO + T1 T2 1. 0,Watch highlights of the final day of the T2
305,1 0 On obtient TO - 1+q+ 1-4 Ti 1+q+ 24-4 et T2 - 1+q+ ; d'oû TO = 48 79 TI et,The winning numbers in Saturday evening's drawing of
306,"T2 3. Si on garde l'ensemble des états E', on n'a pas une chaine de markov car, lorque l'on a 2 machines en panne par exemple, on ne peut pas savoir si le lendemain, on en aura 1 ou 2 en panne : tout dépend si c'est le premier jour de réparation ou le second. On est donc amené à introduire 2 états supplémentaires :","Si on garde l'ensemble des états E', on n'a pas une chaine de markov car, "
307,"1': 1 machine en panne dont c'est le deuxième jour de réparation, 2': 2 machines en panne et deuxième jour de réparation pour la première. D'oû l'espace de 5 états E"" = f0, 1, 2, 1', 2'. Dans ce cas, on a 00 2q(1","D'o l'espace de 5 états E"" = f0,"
308,"01 = p""o2 = 0; 11 12 - 0; P'1'1 4, 2 21' - 1; les autres probabilités sont nulles.",Nos tte--t
309,"Laj probabilité que les 2 machines fonctionnent après n jours, ceci revient à calculer les puissances de la matrice de transition P"" : 4.6 Application : 41 en = 1 cette proba est égale P(X1) = OXo = 0) = P'00",Les machines fonctionnent fonctionnent de la matrice de transition
310,"(le premier terme ds la matrice de transition P""); an = 2 cette proba est égale P(X2) = OXo = 0) 00P (1-q)4 (le premier terme ds la","The Oxford English Dictionary defines the word ""transition"" as """
311,"matrice de transition P""2); . n - 3 cette proba est égale P(X3) = OXo = 0) 'oop 00p 00 + PuplwPo-l-q""t 24(1-4)3 (le premier terme ds la matrice de transition P""3). 2","'oop 00p 00 + PuplwPo-l-""t"
312,% O o - G 9,The winning numbers in Saturday evening's drawing of
313,/ - 1 2 %,BBC Sport takes a look back at some of the
314,"S96 O) / 5. Processus de décision markoviens 5.1 Processus de décision markoviens Définition 5.1.1 Un processus de décision markovien (Markov decision process, ou MDP) est",The following table lists the key dates and events in the
315,"un processus stochastique contrôlé satisfaisant la propriété de Markov et défini par : un ensemble d'états S (incluant un étant initial so) dans lequel évolue le processus; un ensemble d'actions A qui contrôlent la dynamique de l'état; un espace des temps T, ou axe temporel; une fonction ou modèle de transition P(s, a,s') P(s's,a) (les probabilités de transition entre",Un processus stochastique contrlé satisfaisant la propri de Markov et défini par : un
316,"états), où a € A(s); cette fonction définit l'effet des actions de l'agent sur l'environnement; e une fonction de récompense r qui permet de déterminer le(les) but(s) à atteindre et les éventuelles zones dangereuses de l'environnement. Cette fonction r peut être définie de différentes manières, suivant le problème à résoudre : * r:SXAX, S X R 1 [0, 1] cas général. r(s,a,s',v) désigne la probabilité d'obtenir une récompense","Cette fonction s'est tre difficile de différentes manires : * r:SXAX, S"
317,"Vj pour être passé de l'état S às en ayant effectué l'action a; *r:SxAx: S * R, récompense déterministe; *r:SxA 1 R, récompense déterministe ratachée à l'action en ignorant son résultat; *r:S 1 R, récompense déterministe ratachée à un état donné. Pour une action a fixée, P(s's,a) représente la probabilité que le système passe dans",C'est tre passé de l'état S s en ayant effectué l
318,"l'état s après avoir exécuté l'action a dans l'état S. Evidemment, on a EP(Is,a) =1. 44 Chapitre 5. Processus de décision markoviens i*,",L'étranger de l'ét
319,"- - % pslspa, a",The winning numbers in Saturday evening's drawing of
320,"FIGURE 5.1.1 - Processus décisionnel de Markov Dans le MDP représenté par la figure ci-dessus, à chaque instant t de T l'action di est appliquée dans l'état courant St, influençant le processus dans sa transition vers l'état S1+1. La récompense Ti est émise au cours de cette transition. Un MDP est un modèle général pour un environnement stochastique dans lequel un agent peut","Processus décisionnel de Markov Dans le MDP resenté par la figure ci-dessus,  chaque instant "
321,"prendre des décisions (et où les résultats de ses actions sont aléatoires) et reçoit des récompenses. On suppose A et S finis. Comme résultat d'avoir choisi l'action a dans l'état S à l'instant t, l'agent décideur re- Çoit une récompense, ou revenu Ti = r(s,a) € R. Les valeurs de Ti positives peuvent être considéerés comme des gains et les valeurs négatives",Les valeurs de Ti peuvent tre considées des gains et les valeurs négatives.
322,"comme des côuts. . La représentation vectorielle de la fonction de récompense r(s,a) consiste en AI (=card(A)) vecteurs Ta de dimension s. Remarques : 1. Les MDP sont utilisés pour étudier des probèmes d'optimisation à l'aide d'algorithmes de","La resentation vector de récompense r(s,a) consiste"
323,"programmation dynamique ou d'apprentissage par renforcement qui consiste, pour un agent autonome, à apprendre les actions à prendre à partir d'expériences, de façon à optimiser une récompense quantitative au cours du temps. 2. On peut également considérer des récompenses aléatoires r(s,a) et dans ce cas on considère la valeur moyenne Tt = F(s,a). En particulier, T peut dépendre de l'état d'arrivée s' selon",D'aprs-mme  l'arrivée  l'aprs-mme
324,"r(s,a,s'). On considère alors la valeur moyenne F(s,a) = Es Pls,a)r(s,4,s). 5.1 Processus de décision markoviens 45 Grille (occupancy grid) Rdorh 1",On considre alors la val
325,Rboms Actions: A E: Go east W: Go west,BBC Sport takes a look back at some of the
326,S: Go south N: Go north Si positions sur la grille 1 Ro on 3,S: Go south N: Go north Si positions
327,Degré de désirabilité 2 0 0.4 But But +1,    
328,o 0 1 2 3 Actions(s) R(s) (fonction de récompense) FIGURE 5.1.2,The following table shows the results of a survey carried
329,"Actions aux effets incertains Go South (S) État courant Action P(s' s,a) (modèle de transition)",Action P(s) tat courant
330,États successeurs possibles (25%) (50 %,More than half of French football fans say they would
331,(25 % FIGURE 5.1.3 46 Chapitre 5. Processus de décision markoviens Décision,The following table lists the key steps in the process
332,"Une décision est un choix d'une action dans un état c'est une règle < if state then action > Exemples: (21,12) e W (21,13)E",C'est un choix d'une action
333,"ou (20,13) W (20,12)5 (21,12) E",The winning numbers in Saturday evening's drawing of
334,0.2 0.3 W W 0.5 N,BBC Sport takes a look back at some of the
335,"(21,12) S (20,11) S 0.9",The winning numbers in Sunday evening's drawing of
336,"S 0.2 0.1 0.8 (20,10)",The winning numbers in Saturday evening's drawing of
337,"(1,1) FIGURE 5.1.4 Plan (politique) Un plan est une stratégie: choix d'une action (décision) pour chaque état un plan est également appelé une politique (policy)",The French government has drawn up a plan to tackle
338,"Exemples: c'est un ensemble de règles if state then action Plan n1 (21,13)5 £(21,12) * W,",The BBC's economics editor Robert Peston looks
339,"120,135 s (20,12) (20,13) e S, 0.2 Wi0.3 W",BBC Sport takes a look at some of the key
340,"W (21,13) * S, 0.5 (21,12) S",The winning numbers in Saturday evening's drawing of
341,"(20,11) N. j S 0.9 S",A selection of photos from around the world this week
342,"S 0.2 0.1 0.8 (20,10) - (1,1)",The winning numbers in Saturday evening's drawing of
343,FIGURE 5.1.5 5.1 Processus de décision markoviens 47 Plan (politique) Un plan est une stratégie: choix d'une action (décision) pour chaque état,The French government has announced a major overhaul of its
344,"un plan est également appelé une politique (policy) Exemples: c'est un ensemble de règles if state then action Plan n1 (21,13)5",C'est un ensemble de rgles if
345,"((21,12) e W, (20,13) W (20,12)5 (20,13) S,",BBC Sport takes a look back at some of the
346,"W 0.2 0.3 W (21,13) S, 0.5 (20,11) N",The winning numbers in Saturday evening's drawing of
347,"(21,12) 1 S 0.9 40.2",BBC Sport takes a look back at some of the
348,"Plan T2 0.1 0.8 ((21,12)5, (20,10)","Plan T2 0.1 0.8 ((21,12)"
349,"(20,11) * S, (1,1) (21,10) * E, . FIGURE 5.1.6",The following table shows the value of the pound against
350,Exécution d'un plan (politique) Notons T(s) l'action désignée par le plan TL dans l'état S voici un algorithme d'exécution ou d'application d'un plan While (1) f,Exécution d'un plan (politique)
351,1. S = état courant de l'environnement; 2. a=n(s); 3. execute a; ) L'étape 1 peut impliquer de la détection (sensing),L'étape 1 peut impliquer
352,et de la localisation L'état résultant de l'exécution de l'action à l'étape 3 est stochastique FIGURE 5.1.7 48,"All photographs  AFP, EPA, Getty Images"
353,"Chapitre 5. Processus de décision markoviens Interpretation/application d'un plan L'application d'un plan dans un MDP résulte en une chaîne de Markov le modèle de transition est donné par P(s']s, T(s)) Exemples:",Un processus de décision markoviens Interpretation/application d
354,"Plan T1 (21,13)5 ((21,12) e W, (20,13) (20,13)-S,",BBC Sport looks at some of the key statistics behind
355,"0.2 W 0.3 (20,12)5 W W (21,13) S,",The winning numbers in Saturday evening's drawing of
356,"0.5 (20,11) N (21,12) ) A",",,,,"
357,0.9 - $40.2 Plann2 0.1,BBC Sport takes a look at some of the best
358,"0.8 ((21,12) S, (20,10) E(1,1) - Activer Windows (20,11) S,",BBC News takes a look back at some of the
359,"Arrédez arv naramàtrec (21,10) Erindid FIGURE 5.1.8 5.1 Processus de décision markoviens 49 Exemple 1 de MDP :",Arrédez arv naramt
360,"Cet exemple représente un processus de Décision Markovien à trois états distincts (s0,51,52) représentés en vert. Depuis chacun des états, on peut effectuer une action de l'ensemble fao,ai). Les noeuds rouges représentent donc une décision possible (le choix d'une action dans un état donné). Les nombres indiqués sur les flèches sont les probabilités d'effectuer la transition à partir du noeud",Les nombres rouges indiquées sur les flches sont les probabilités d'effectuer la transition
361,"de décision. Enfin, les transitions peuvent générer des récompenses (dessinées ici en jaune). +5 0.10 So 0.70",C'est un peu tre 
362,S1 a1 ao (1.0 0.20,BBC Sport takes a look back at some of the
363,0.5 0.95 ao a1 0.5,The winning numbers in Saturday evening's drawing of
364,0.4 0.05 ao 0.40 0.30,The winning numbers in Saturday evening's drawing of
365,0.6 (a1) 0.30 S2 -,BBC Sport takes a look at some of the key
366,Exemple de processus de Décision Markovien à trois états et à 6 deux actions. FIGURE 5.1.9 50,    
367,Chapitre 5. Processus de décision markoviens La matrice de transition associée à l'action ao est la suivante : 0.50 0 0.50 0.70 0.10 0.20,La matrice de transition associée 
368,0.40 0.60 La matrice de transition associée à l'action ai est la suivante : 0 0,La matrice de transition associée 
369,"1.0 0 0.95 0.05 0.30 0.30 0.40 En ce qui concerne les récompenses,","All photographs courtesy of AFP, EPA, Getty Images"
370,on perçoit une récompense de +5 lorsque l'on passe de l'état 81 à l'état S0 en accomplissant l'action ao on perçoit une récompense de -1 (aussi appelée pénalité) lorsque l'on passe de l'état 82 à l'état S0 en accomplissant l'action ai FIGURE 5.1.10,On peroit une récompense de +5 lorsque l'on pass
371,"5.1 Processus de décision markoviens 51 Exemple 2: Schéma d'interaction entre agent (intelligence artificielle) et son environnement; cas du contrôle d'un drone : Dans le cadre du contrôle d'un drone, l'idée de l'apprentissage par renforcement est de concevoir","All photographs courtesy of AFP, EPA, Getty Images and Reuters"
372,"un agent implanté au sein de notre drone et dictant à ce dernier les décisions rationnelles à prendre dans chacune des situations rencontrées. A chaque instant, ces décisions seront prises selon les informations disponibles sur l'environnement : le signal de perception sur l'environnement (signal d'observation) et le signal de récompense. En intégrant l'environnement dans notre schéma, il apparait que nous avons décrit une boucle",Un agent implanté de notre drone et dictant  ce dernier les décisions rations  prendre dans chac
373,"d'interaction entre, d'une part, l'agent et, d'autre part, l'environnement dans lequel évolue l'agent. Obs., Récompense Action FIGURE 5.1.11 St,Ar,7i, S+1,A,+1,741, Si+2, A1+2, T+2,","St,Ar,7i, S+1,"
374,"Processus de génération des données d'apprentissage Ainsi, à chaque instant, l'agent exécute une décision, laquelle influence les informations (signaux de perception et de récompense) transmises à l'agent par le biais de l'environnement. Ce proces- sus se: répète encore et encore. Cette boucle d'interactions définit une série temporelle, composée de : - décisions (ou actions), notées A; ;","Une série temporelle  Ainsi, selon l'Institut National de la Recherche Scientifique"
375,"signaux d'observations, notées S; signaux de récompenses, notées T. C'est cette série qui engendre nos données d'apprentissage. e Description du processus stochastique à contrôler : Question : sur quels modèles formels et sur quelles représentations de l'environnement",Le processus stochastique  contrler : d'
376,"s'appuie un agent afin qu'un drone autonome puisse interpréter une sche complexe et y agir de faon rationnelle ? 52 Chapitre 5. Processus de décision markoviens Les MDP constituent un de ces modèles, capable de décrire formellement les interactions","A l'arrivée de ces modles,  l'"
377,entre un environnement et un drone autonome. Un MDP peut représenter de nombreux problèmes de prise de décisions séquentielles traités en apprentissage par renforcement. . Etat d'un processus de Markov : On rappelle que la boucle d'interactions entre un agent et son environnement produit deux signaux : un signal de récompense et une observation.,Un agent et sonnement d'un processus de Markov : On rappelle que la boucle d'interaction
378,"Si l'observation contient toutes les données sur le système (composé de l'environnement et l'agent) nécessaires et suffisantes pour décrire l'évolution future de celui-ci, alors l'observation décrit parfaitement l'état du système et aucune autre information n'est nécessaire pour son contrôle. Cette propriété est appelée observation complète de l'état. Un système doté d'une telle propriété est dit totalement observable.",L'observation du systme de celui-ci s'est  l'occasion d'
379,"En fait, Un état est la totalité de l'information nécessaire et suffisante pour prédire l'évolution future d'un système. Question : Comment un drone passe t-il d'un état à un autre ? On note bien que rien dans la définition d'un processus de Markov ne permet de commander un drone. En effet, on peut tout au plus suivre l'évolution de celui-ci au travers des états dans les-",Un drone passe t-il d'un état  un autre ?
380,"quels il passera. Le contrôle d'un processus de Markov requiert l'introduction de la notion d'actions. e Actions d'un MDP : Jusqu'ici, on a défini une chine de Markov comme un modèle capable de décrire la dynamique d'un système non contrôlé. Pour permettre la commande d'un système, on doit ajouter à une chaine de Markov un ensemble d'actions.",Un modle capable de décrire la dynamique d'un systme non contrlé.
381,"On remarque bien que la fonction de transition d'un MDP contient autant de matrices de transition qu'il y a d'actions. Cela signifie que l'état successeur S1+1 = s' dépend de l'état courant Si = S mais aussi de l'action courante A; = a à travers la probabilité conditionnelle P(s,a,s'). Prenons un exemple trivial afin d'illustrer un MDP. Considérons un drone déployé dans une grille 3 X 3. Initialement, en bas à gauche de la grille, le drone souhaite se rendre en haut à","Mae Cela, rapporteur de l'tat de l'étranger de l'tat de l'tat de France, nous avons tre "
382,"droite de celle-ci. II dispose pour cela de 4 actions : gauche, droite, bas et haut. L'a action haut réussit avec une probabilité de 0,8 et échoue en allant soit à gauche soit à droite avec probabilité 0,1 1. Question : Quel est la séquence d'actions qui offre la probabilité d'être en haut à droite ? Si les actions sont déterministes, leurs effets sont prédictibles avec certitude, alors la séquence recherchée est celle dont le chemin, allant de la cellule en bas à gauche jusqu'à la cellule en","L'a action haut réussit avec une probabilité de 0,8 et échoue en allant soit  gauche soit  droite avec probabilité 0"
383,"haut à droite, est le plus court, par exemple haut, haut, gauche, gauche. Si au contraire les actions sont stochastiques, leurs effets sont tirés suivant une loi de proba- bilité; alors la séquence haut, haut, gauche, gauche est peut-être l'une de celles recherchées. 5.2 Problèmes décisionnels de Markov 53","Une gauche  gauche, haut, gauche alors laquence haut"
384,FIGURE 5.1.12 5.2 Problèmes décisionnels de Markov 5.3 Politiques d'actions Une politique ou stratégie notée T (pour contrôler un MDP) décrit la procédure suivie par l'agent pour choisir dans chaque état (à chaque instant) l'action à exécuter. II s' agit d'une fonction,Une politique d'actions Une politique ou stratégie not
385,"1:S- A dans le cas déterministe; T:SxA 1 [0, 1] dans le cas stochastique. Une politique déterministe définit précisément l'action à effectuer; Une politique stochastique est une famille de distributions de probabilité selon laquelle une action a doit être sélectionnée pour chaque état (ou historique observé h).",Une politique stochastique est une famille de distributions de probabilité s
386,"54 Chapitre 5. Processus de décision markoviens On obtient ainsi quatre familles distinctes de stratégies, comme indiqué sur le tableau : politique Tt déterministe",Les familles distinctes de stratégies
387,"aléatoire markovienne St dt at, St [0, 1]",Aléatoire markovienne St dt
388,"histoire-dépendante he at he, St 0, 1 Tableau 1.1. Différentes familles de politiques pour les MDP. FIGURE 5.3.1",Différentes familles de poli
389,"Une politique déterministe est notée T = f(s), Vt, Wsi) ou T = f(hr), Vt, Vhi; T(h) définit l'action a choisie àl l'instant t si on a observé l'historique h. . Une politique stochastique est notée T = fm(a,st),t, VSt, Vaj ou T = f(a,h:), Vt, Vhy, Vaj; où m(a,s) = T(a/s:) = P(a: - as: = s) représente la probabilité de choisir l'action a à l'instant t sachant que l'état à l'instant t est S.","Une politique stochastique est notée T = fm(a,st),t, VSt, Vaj ou T = f(a,h"
390,Ces quatre familles de politique définissent les quatre ensembles suivants : = ITHS l'ensemble des politiques histoire-dépendantes stochastiques (aléatoires) cà-d l'en- semble le plus général des politiques; - ITHD l'ensemble des politiques histoire-dependantes déterministes; - ITMS l'ensemble des politiques markoviennes stochastiques (aléatoires);,Le politiques histoire-dépendantes stochastiques (alé
391,- IIMD l'ensemble des politiques markoviennes déterministes. histoire-dépendante aléatoire histoire-dépendante markovienne aléatoire déterministe,La histoire-dépendante aléa
392,markovienne déterministe Figure 1.2. Relations entre les différentes familles de politiques FIGURE 5.3.2 Remarque : La définition des politiques peut ou non dépendre explicitement du temps. Définition 5.3.1 - Politique stationnaire. Une politique est stationnaire si elle ne dépend pas,All figures are approximations and subject to change.
393,"du temps c-à-d si Vt,t, = Ty. Parmi ces politiques stationnaires, les politiques markoviennes déterministes sont centrales dans l'étude des MDP. II s'agit du modèle le plus simple de stratégie décisionnelle, on nomme leur 5.4 Politique markovienne et chaine de Markov valuée 55",Les politiques markoviennes déterministes sont centrales dans
394,"ensemble D. Définition 5.3.2 - Politiques markoviennes déterministes stationnaires. D est l'ensemble des fonctions T qui à tout état de S associent une action de A : T:SES T(s) EA Un autre ensemble important, noté D4 est constitué des politiques markoviennes aléatoires","Un ensemble important, noté D4 est constitué des politiques markov"
395,"stochastiques) stationnaires. Les politiques de D et D4 sont très importantes car, comme on le verra, D et D4 contiennent les politiques optimales pour les principaux critères. 5.4 Politique markovienne et chaine de Markov valuée Un MDP et une politique markovienne T forment une chaine de Markov dont la matrice de transition",Politique markovienne et chaine de Markov valuée Un MDP et une politique markovienne
396,"est définie par Ws,s Pr(s,s') = P(S11 S'St s) - L (a,s)P(ss,a) dEA Dans le cas où T est déterministe, Pr(s,s') = P(ss,T(s)). La matrice Pr est construite simplement en retenant pour chaque état S la ligne correspondante dans la matrice Pr avec a = T(s).","Dans le cas o T - L (a,s)P(ss,a)"
397,"De même, on note Rt le vecteur de composantes R(s,(s)) pour T markovienne détermi- niste et L T(a,s)R(s,a) pour T markovienne stochastique. dEA Le triplet (S,Pa,R) est appelé un processus de Markov valué, ou chaine de Markov va- luée. II s'agit simplement d'une chaine de Markov avec des revenus associés aux transitions.","Le triplet (S,Pa,R) appelé un processus de Markov valué, ou chaine"
398,"Exemple : Imaginons un sujet qui, pendant l'hiver, souhaite décider chaque jour de prendre ou non un médicament pour lutter contre le rhume. On considère un horizon de temps infini (T = N). L'état du sujet, chaque matin, est soit malade (1) soit en bonne santé (0) : S = £0, 1. Et il peut choisir de prendre un traitement (1) ou non (0) :A = £0,1). On suppose que l'état du sujet au jour j+ 1 dépend uniquement de l'état du sujet le jour j et du fait",C'est un peu t--tre un peu t--tre un peu t--tre un peu t--t
399,qu'il ait pris ou non un traitement le jour j. L'évolution de la maladie est aléatoire et le traitement n'est pas systématiquement efficace. On suppose que la fonction de transition est connue et donnée par la matrice : a 0 1 S,Une fonction de transition s'il avait tre 
400,"P(S1+1 = 1st = a) P(1, a 0 0.3 0.1 1",The winning numbers in Sunday evening's drawing of
401,"0.9 0.2 FIGURE 5.4.1 Chaque jour, le sujet obtient une récompense réelle modélisant son niveau de satisfaction de son état de santé, et prenant en compte le coût du traitement : 56",Le sujet de l'Institut National de la Recherche Scientif
402,"Chapitre 5. Processus de décision markoviens 1 S R(s,a) = 0","All photographs courtesy of AFP, EPA, Getty Images"
403,1 0.9 1 0 -0.1 FIGURE 5.4.2 une politique markovienne stationnaire déterministe possible est de choisir de prendre le traitement,Une politique markovienne stationnaire
404,"uniquement si on est malade : Vt € T,(s) =Osis, = 0, T(s) = 1 si St = 1. L'état de santé suit alors une chaine de Markov de matrice de transition : s 0 1 s",C'est un peu t--tre selon
405,Pr(s']s) = 0 0.7 0.8 1 0.3 0.2,The winning numbers in Sunday evening's drawing of
406,"FIGURE 5.4.3 5.5 Critères de performance Se poser un problème décisionnel de Markov, c'est rechercher parmi une famille de politiques celles qui optimisent un critère de performance donné pour le processus décisionnel markovien considéré. Ce critère a pour ambition de caractériser les politiques qui permettront de générer des",Une politiques celle s'est recherchercher parmi une famille de
407,"séquences de récompenses les plus importantes possibles. En termes formels, cela revient toujours à évaluer une politique sur la base d'une mesure du cumul espéré des récompenses instantanées le long d' une trajectoire, comme on peut le voir sur les critères les plus étudiés au sein de la théorie des MDP, qui sont respectivement : e le crière fini : E(Ro + Ri + R2 + + RN-1So)",Cette politique de France (MDP) avait tre  difficile et tre  difficile diffusés
408,"e le critère y-pondéré : E(Ro + yRi + yR2 +yR,so) e le critère total : E(Ro + R + R2 Ri 0 e le critère moyen : lim",E-mail: news@bbc.co.uk
409,"+ R2 + + Kn So). n-oo Les deux caractéristiques communes à ces 4 critères sont en effet d'une part leur formule additive en R, qui est une manière simple de résumer l'ensemble des récompenses reçues le long d'une trajectoire et, d'autre part, l'espérance E(.) qui est retenue pour résumer la distribution des",L'espérance E(.) s'est une manire simple de résumer l'
410,"récompenses pouvant être reçues le long des trajectoires, pour une même politique et un même état de départ. Ce choix d'un cumul espéré est bien sêr important, car il permet d'établir le principe d'optimalité de Bellman ("" les sous-politiques de la politique optimale sont des sous-politiques optimales ""), à la base des nombreux algorithmes de programmation dynamique permettant de",La politique optimale sont des sous-politiques optimales sont des nombreux algorithmes de program
411,résoudre efficacement les MDP. On va maintenant caractériser successivement les politiques optimales et présenter les algorithmes permettant d'obtenir ces politiques optimales pour chacun des critères précédents. 5.6 Fonctions de valeur 57,Les politiques optimales et senter leses
412,"5.6 Fonctions de valeur Les 4 critères qu'on viens de voir permettent de définir une fonction de valeur qui, pour une politique T fixée, associe à tout état initial S € S la valeur du critère considéré en suivant T à partir de s: pour T fixée, V.S-R. On note W l'espace des fonctions de S dans R, identifiable à l'espace vectoriel RISI. L'en-",Fonctions de valeur Les 4 critres qu'on viens de voir permettent de définir une fon
413,"semble  est muni d'un ordre partiel naturel : VU,VEY USV $ VSES U(s) S V(s) L'objectif d'un problème décisionnel de Markov est alors de caractériser et de rechercher = si elles existent - les politiques optimales m* € ITHS telles que VI € ITHS VSES V""(s) < V'(s)","Une politique optimale m*  ITHS VSES V""(s) "
414,"c-à-d a* € argmaxerws V On note V* = maxrerHs V = V. Dans le cadre des MDP, on recherche donc des politiques optimales meilleures que toute autre politique, quel que soit l'état de départ. Remarquons que l'existence d'une telle politique optimale n'est pas en soi évidente.","Dans le cadre des MDP, on recherche donc des politiques optimales"
415,"La spécificité des problèmes décisionnels de Markov est alors de pouvoir être traduits en terme d'équations d'optimalité portant sur les fonctions de valeur, dont la résolution est de complexité moindre que le parcours exhaustif de l'espace global des politiques de ITHS (la taille du simple ensemble D est déjà de AISl). 5.6.1 1- cas du crière fini",La spécificité des problmes décisionnels de Markov est alors de pouvoir
416,"On suppose ici que l'agent doit contrôler le système en N étapes, avec N fini. Le critère fini conduit naturellement à définir la fonction de valeur (à horizon fini) qui associe à tout état S l'espérance de la somme des N prochaines récompenses obtenues en suivant la politique T à partir de s: Définition 5.6.1 - Fonction de valeur pour le critère fini. Si T = f0, 1, N y, on pose VSES VN( (s) - E"" X R,(St,a:)so =s)",La politique T  partir de s: Définition 5.6.1 - Fonction de valeur pour le critre fini.
417,"-0 Dans cette définition, E""() dénote l'espérance mathématique sur l'ensemble des réalisations du MDP en suivant la politique T. ET est associée à la distribution de probabilité Pr sur l'ensemble de ces réalisations. Notons qu'il est parfois utile d'ajouter au critère une récompense terminale TN fonction",C'est tre tre et tre tre 
418,"du seul état final SN. II suffit pour cela de considérer une étape artificielle supplémentaire où Vsi, ai, Rw(St,a;) = Rw(SN) (récompense terminele). C' est le cas par exemple lorsqu'il s'agit de piloter un système vers un état but en N étapes et à moindre coût. 5.6.2 2- cas du crière moyen Lorsque la fréquence des décisions est importante, avec un facteur d'actualisation proche de 1,",Lorsque lorsqu'il s'agit de piloter un systme vers un état but en N
419,"ou lorsqu'il n'est pas possible de donner une valeur économique aux récompenses, on préfère 58 Chapitre 5. Processus de décision markoviens considérer un critère qui représente la moyenne des récompenses le long d'une trajectoire et non plus leur somme pondérée. On associe ainsi à une politique l'espérance du gain moyen par étape.",Une politique spécifique ainsi  une tte d'un
420,"On définit alors le gain moyen p""(s) associé à une politique particulière T et à un état S : Définition 5.6.2 Le gain moyen est VSES P""(s) = lim E"" X R;(St,a)lso = S n-oo n Pour le critère moyen, une politique T* est dite gain-optimale si p"" (S) > p"" (s) pour toute politique",Une particulire particulire  une politique particulire 
421,"T et tout état S. Ce critère est particulièrement utilisé dans des applications de type gestion de file d'attente, de réseau de communication, de stock etc. 5.7 Politiques markoviennes 5.7.1 Equivalence des politiques hisioire-dépendantes et markoviennes",Les politiques hisioire-dépendantes et markovienne
422,"En voici une propriété fondamentale des MDP pour ces différents critères, qui est d'accepter comme politiques optimales des politiques simplement markoviennes, sans qu'il soit nécessaire de considérer l'espace total ITHA des politiques histoire-dépendantes. Proposition 5.7.1 Soit T € ITHS une politique aléatoire histoire-dépendante. Pour chaque état initial x € S, il existe alors une politique stochastique markovienne m' € IIMS telle que",Une politique aléatoire histoire-dépendante peut-tre tre  l'
423,"1. V() = V(x), 2. = V V7() 7 (x), 3. v X = V""(r),","V = V(x), V7 = V"
424,"4. P""(s)=p""(x). Ce résultat permet d'affirmer que lorsque l'on connaît l'état initial (ou une distribution dej probabilité sur l'état initial), toute politique histoire-dépendante aléatoire peut être remplacée par une politique markovienne aléatoire ayant la même fonction de valeur. 5.8 Caractérisation des politiques optimales",Caractérisation des politiques optimales ayant tre re
425,"5.8.1 Cas du critère fini Equations d'optimalité : Supposons que l'agent se trouve dans l'état S lors de la dernière étape de décision, confronté au choix de la meilleure action à exécuter. II est clair que la meilleure décision à prendre est celle qui maximise la récompense instantanée à venir, qui viendra s'ajouter à celles qu'il a déjà percues. On",La meilleure action  exécuter peut-tre tre  tre 
426,"a ainsi : TEN-1(s) € argmaxaeA RN-1(s,a), et Vi(s) - max. Rw-1(s,a), dEA",    
427,"où TN-1 est la politique optimale à suivre à l'étape N - 1 et Vi la fonction de valeur optimale pour un horizon de longueur 1, obtenue en suivant cette politique optimale. 5.8 Caractérisation des politiques optimales 59 Supposons maintenant l'agent dans l'état S à l'étape N - 2. Le choix d'une action a va",L'Institut National de la Recherche Scientifique (Institut National de la Recherche Scientifique) a déj
428,"lui rapporter de façon sûre la récompense RN-2(s,a) et l'aménera de manière aléatoire vers un nouvel état s' à l'étape N - 1. Là, il sait qu'en suivant la politique optimale TN-1 il pourra récupérer une récompense moyenne Vi(s). Le choix d'une action a à létape N - 2 conduit donc au mieux en moyenne à la somme de récompenses Rw-2(s,a) + Es PN-2(s,4)V7(6).",L'Institut National de la Recherche Scientifique (Institut National de la Recherche Scientifique-Scientifique-Scientif
429,"Ainsi, le problème de l'agent à l'étape N = 2 se ramène simplement à rechercher l'action qui maximise cette somme, soit : TN-2(s) € argmaxdeA K-16ALmVBAA et V(s) - max RN-2(s,a) + X PN-2('s,4)V7(6).",Le problme de l'agent  l'étape
430,"dEA Ce raisonnement peut s'étendre jusqu'à la première étape de décision, où l'on a donc : m(s) € argmaxaeA RZmia-0 et V(s) = max REmikaw.-0)",Une raisonnement s'étend
431,"dEA L'évaluation d'une politique à horizon fini se fait donc en partant de la fin, en résolvant des problèmes à un pas de temps, ce qui est à la base de la programmation dynamique. D'oû le Théorème 5.8.1 - - Equations d'optimalité pour le critère fini. Soit N < 00. Les fonctions de",L'évaluation d'une politique  horizon fini se fait donc en partant de la
432,"valeurs optimales V* = (VN Vi) sont les solutions uniques du système d'équations Vs ES V41(s) = max RW-1-ls,4)+2 PN-1-6,0,6), dEA avec n = J, N - 1 et Vo = 0. Les politiques optimales pour le critère fini T* = (o,,..",Les politiques optimales pour le critre fini T* = (o
433,"1 sont alors déterminées par : Vs ES (s) € argmaxaeA W6ZmwM--0) pour t = 0, ,N-1. On voit donc ici dans le cadre du critère fini que les politiques optimales sont de type markovien déterministe, mais non stationnaire (le choix de la meilleure décision à prendre dépend de l'instantt).","Les politiques optimales sont de type markovien déterministe, non stationnaire ("
434,Evaluation d'une politique markovienne déterministe : Soit une politique T markovienne déterministe. La même démarche permet alors de caractériser sa fonction de valeu VA. 60 Chapitre 5. Processus de décision markoviens,La mme démarche permet alors de
435,"Théorème 5.8.2 = Caractérisation de VA. Soient N < 00 et T = (To, T1, ,TN-1) une po- litique markovienne. Alors VA = V avec V = (Vw,Vw-1--.,V) sont solutions du système d'équations linéaires VSES Vn+1(s) = RN-1-n(s, -1-0)+ZP--1-0)G, pour n = 0, N I et Vo = 0.","Aujourd'hui, le systme d'équations linéaires VSES"
436,"5.8.2 Cas du critère moyen On se limite ici au cadre des MDP récurrents (pour toute politique markovienne déterministe, la chaine de Markov correspondante est constituée d'une unique classe récurrente), unichaines (chaque chaine de Markov est constituée d'une unique classe récurrente plus éventuellement quelques états transitoires) ou multichaines (il existe au moins une politique dont la chaine de",Les politiques markoviennes (MDP) peut-tre tre tre
437,"Markov correspondante soit constituée de deux classes récurrentes irréductibles ou plus). On suppose de plus ici que pour toute politique, la chaîne de Markov correspondante est apériodique. Evaluation d'une politique markovienne stationnaire : Soit T € DA une politique stationnaire et (S,P,R) le processus de Markov associé. Rap- pelons que le gain moyen ou critère moyen est défini par :",La chane de Markov correspondante a s'il s'il s'il s'il s'
438,"N-1 Vs ES P""(s) - lim E"" > Rx(S:)so=s N-4+oo M t-0 Sous forme matricielle, on a N-1",BBC Sport takes a look at some of the key
439,p lim Rr- N-+oo N t-0 Soit P = limw-too N EN Pt la matrice limite de Pr. On montre que P existe et est une matrice,P existe et est une matrice.
440,"stochastique pour tout S fini. De plus, Pr vérifie PPx = P. Le coefficient Prss peut être interprété comme la fraction de temps que le système passera dans l'état So en étant parti de l'état S. Pour des MDP apériodiques, on a de plus P = limw-too PN et Prss peut être interprété comme la probabilité à l'équilibre d'être dans l'état SO en étant parti de S.","Pour des MDP apériodiques, on a de plus P = limw-too PN et Prss peut t"
441,"Enfin, pour un MDP unichaine, P est alors la matrice dont toutes les lignes sont identiques et égales à la mesure invariante Hz de la chaine contrôlée par la politique T. Ainsi Pas,s = Ha(s'). De la définition précédente de p"" on déduit que p"" = PR. Pour un MDP unichaine on établit ainsi que p(s) = P est constant pour tout s, avec p =LH(S)Rx(s).",C'est quoi p(s) et p(s) et p(s) et p(s) et p(s) et p
442,"SES Dans le cas général d'un MDP multichaine, p(s) est constant sur chaque classe de récurrence. Cette première caractérisation de p"" fait intervenir P qu'il n'est pas facile de calculer. II est toutefois possible d'obtenir autrement p"", en introduisant une nouvelle fonction de valeur pour le critère moyen, dite fonction de valeur relative :",Une tte tte  l'occasion d'un tte t
443,"5.9 Algorithmes de résolution des MDP 61 Définition 5.8.1 = Fonction de valeur relative pour le critère moyen. VSES UG)=E""(ER, - P"")so=s t-0",The following table shows the results of a new study
444,"Théorème 5.8.3 Soit (S,P,R) un processus de Markov valué assicié à une politique T € DA. Alors Si p"" et UT sont le gain moyen et la fonction de valeur relative de T, on a 1. (I-Pr)p""-0; 2. p""+( I-Pr)U = Rr. On retiendra que la fonction de valeur relative U"" est l'unique solution de (I Pr)U= I P)R",Un processus de Markov valué assicié  une politique T  DA.
445,"telle que PU = 0. Dans le cas simplifié d'un processus unichaine, la première équation se simplifie en p""(s) p""et la seconde peut s'écrire VSES U(s)+p = Rr(s) + EPssU(s) (1) S'ES",Dans le cas simplifié d'un processus unichain
446,"Théorème 5.8.4 - Equations d'optimalité uniichaine. II existe une solution p,U au système d'équations définies pour tout S E S: P(s) = max E p('s,a)p(s), dEA SES U(s)+p=1 max R60+Epkaue) (2)",Une solution d'optimalité uniichaine s'
447,dEA S'ES On a alors p = p*. 5.9 Algorithmes de résolution des MDP 1. Cas du critère fini :,"On a alors p = p*,"
448,"Le cas de l'horizon fini est assez simple. Les équations d'optimalité permettent en effet de calculer récursivement à partir de la dernière étape les fonctions de valeur optimales Vi, VN selon l'algorithme du premier théorème. 2. Cas du critère moyen : On présente ici deux principaux algorithmes de programmation dynamique pour calculer des",Le cas de l'horizon fini est assez simple.
449,"politiques gain-optimales, dans le cas simplifié de MDP unichaines (toutes les politiques sont unichaines) pour lesquels le gain moyen p est constant. Le test d'arrêt est ici basé sur l'emploi de la semi-norme span sur Y: : WV € Y, span(V) - max V( (s) - min V (s). SES","Dans le cas politiques gain-optimales, dans le cas simplifié de MDPchain"
450,"SES Contrairement à IIVII qui mesure l'écart de V à 0, la semi-norme span(V) mesure l'écart de V à un vecteur constant. Algorithme d'itération sur les valeurs relatives : C'est un algorithme d'itération sur les valeurs relatives U(s) = V(s)-p.",C'est un algorithme d'itération sur les valeurs relatives U(s
451,62 Chapitre 5. Processus de décision markoviens Algorithme d'itération sur les valeurs relatives - Critère moyen initialiser Uo € N choisir s* € S,"Anywayanyday, anywayanyday.com, anywayanyday"
452,"n +-0 répéter Pn+1 - maxacA R(s*, a +L-p6,QU.6)) pour S E S faire Un+1(s) = maxaeA R(s,a)+) Epl's,a)U.6)) - Pn+1",Pn+1 - maxacA R(s
453,nt-n+1 jusqu'à span(Un+1 - Un) <E pour S € S faire T(s) € argmaxdeA A6LPVAL) retourner Pns Uns T.,nt-n+1 jusqu' span
454,"Sous différentes hypothèses techniques, on peut montrer sa convergence pour E 1 0 vers une solution (p*,V*) des équations d'optimalité (E) et donc vers une politique optimale T*. Algorithme modifié d'itération sur les politiques : L'algorithme ci-dessous est un algorithme modifié d'itération sur les politiques, qui ne né- cessite pas la résolution de l'équation (1) pour évaluer la fonction de valeur relative.","L'algorithme ci-dessous est un algorithme modifié d'itération sur les politiques,"
455,Algorithme modifé d'itération sur les politiques - Critère moyen initialiser Vo € Y flag +-0 n +-0 répéter,Nos tte--t
456,"pour S € S faire T+1(s) € argmaxdeA £R(s,a) + LyPl1s,4)V.(6)) (T+1(s) = (s) si possible) VP(s) = 4DAHEACRaM m +-0",T+1(s)  argmax
457,"si span(V, Vn) < E alors flag +-1 sinon répéter pour S € S faire Vm+I(s) = (R(s,T+1(s)): + Erp0G)","si span(V, Vn) "
458,m t-m+1 jusqu'à span(m+l Vm) <8 Vn-1 + V n t-n+1 jusqu'à flag = 1,M t-m+1 jusqu'
459,"retourner Vns Tn+1- Pour 8 élevé, l'algorithme est équivalent à l'itération sur les valeurs (non relative car on ne gère pas ici explicitement le revenu moyen Pn). Pour 8 proche de 0, on retrouve une itération sur les politiques classiques. Sous les mêmes conditions techniques précédentes, on montre que cet algorithme converge pour tout 8 vers une politique optimale pour E 1 0. Plus précisément, lorsque","Retourner Vns Tn+1- Pour 8 élevé, l'algorithme est équiration  l'ité sur"
460,"l'algorithme s' arrête, on a min(V,(s) Vn(s)) / 1n+1 / p"" / max Vn(s)), SES SES",L'algorithme s' a
461,5.10 Etude de cas et Application Industrielle; cas d'un MDP 63 qui assure p-P'se. 5.10 Etude de cas et Application Industrielle; cas d'un MDP 5.10.1 Exemple de système de production,5.10 Etude de cas et Application Industrielle; cas d
462,"Voir fichier joint ""Application indust. MDP"". Analyse du système de décision; Objectif du problème : déterminer une politique optimale relativement au côut moyen par unité de temps; Identification d'une politique déterministe optimale par résolution exhaustive;",Délégués  l'Institut National de la Recherche Scient
463,"On rappelle que e Les états E de la chaine peuvent être partitionnés en classes d'équivalence appelées classes irréductibles. Si E est réduit à une seule classe, la chaîne de Markov est dite irréductible. Une classe d'équivalence C est dite fermée si, pour tout x, tels que x € Cet fx 1 y, y € C. Autrement dit, Vx € C, Vn € N, LyecP(s,y) = 1, c-à-d encore C est une classe dont on",Une classe d'équivalence C peuvent tre partitionnés en classes d'équivalence appelées classes ir
464,"ne peut pas sortir. Une classe fermée réduite à un point C x est appelée un état absorbant. Un état X est absorbant ssi p(x,x) = 1. Si deux états communiquent alors ils ont même période. La période d'une classe est la période de chacun de ses éléments. Une classe est dite",Une classe fermée réduite  un point C x est appelée un état
465,"apériodique si sa période est 1. Exercices corrigés. Exercice 1. Considérons une chaine de Markov définie sur un espace E = f0, 9 formé de 10 états, dont la matrice de transition est : 64",Considéron une chaine de Markov définie sur un e
466,Chapitre 5. Processus de décision markoviens 0.7 0 0 0.3 1 1 0,"All photographs courtesy of AFP, EPA, Getty Images"
467,0 0 FIGURE 5.10.1 1. Tracer son graphe. 2. Déterminer les classes de la chaine et leurs périodes. Solution :,The solution to this problem is presented in the following
468,"0.3 0.7 1 1. FIGURE 5.10.2 2. On constate qu'il y a deux cycles donc les états de chacun des deux cycles communiquent. De plus,",The following table shows the relationship between the number of
469,"ces deux cycles contiennent le même état 0, ce qui implique qu'ils communiquent. II n'y a donc qu'une seule classe; la chaine est irréductible. Afin de déterminer la période de cette chaine de Markov, il suffit de déterminer celle d'un de ses états; prenons x = 0. Partant de 0 nous sommes de nouveau en 0 au bout de 4 transitions en passant par le petit cycle et au bout de 7 transitions en passant par le grand cycle, on a:","Partant de 0 nous sommes de nouveau en 0 transitions en passant par le petit cycle et au bout de 7 transitions en passant par le grand cycle, on a:"
470,"pl4)(0,0) = 0.7 et pl4)(0,0) = 0.3. On en déduit que le PGCD est égal à 1;1 la chaine est donc apériodique. En résumé, cette chaine est formée d'une seule classe apériodique. Exercice 2. On reprend la chaine précédente avec un état de moins dans le second cycle. C'est-à-dire, considérons la chaine de Markov définie sur un espace E = f0, ,8 formé de 9","C'est--dire, Markovid la chaine de définie sur un espace E = f0, ,8"
471,"5.10 Etude de cas et Application Industrielle; cas d'un MDP 65 états, dont la matrice de transition est : 0.7 0 0 0.3 =",Etude de cas et Application Industrielle; cas d
472,FIGURE 5.10.3 1. Tracer son graphe. 2. Déterminer les classes de la chaine et leurs périodes. Solution : 0.3,The solution to this problem is shown in Figure 5.1
473,6 1 0.7 1 1. FIGURE 5.10.4,BBC Sport takes a look at some of the key
474,"2. Comme précédemment cette chaine de Markov ne contient qu'une seule classe et est donc irréduc- tible. étudions la période en étudiant celle du sommet 0. Partant de 0, nous sommes de nouveau en 0 au bout de 4 transitions en passant par le petit cycle et au bout de 6 transitions en passant par le grand cycle. De plus, pour revenir en 0 on est obligé de passer par le petit ou par le grand cycle (""ou"" non exclusif). Ainsi, nous avons p(0,0) > 0 ssi n = 4p + 6q pour (P,9) * (0,0). Donc",Une période d'une seule classe irréduc- tible s'est tudiant d'un tudiant chaine de Markov.
475,"pgcdin N 1p(0,0) > 0j = pgcd4p + 6ql(p,4) * (0,0)) = pgcd/4, 6j = 2. Ainsi la chaine est périodique de période 2. Exercice 3. Soit la chaine de Markov à 10 états E = f0, 9 de matrice de transition : 66 Chapitre 5. Processus de décision markoviens",The following table shows the results of a study on the Markov chain:
476,0 0 - . FIGURE 5.10.5,"All photographs courtesy of AFP, EPA, Getty Images"
477,1. Tracer son graphe. 2. Déterminer les classes de la chaine et leurs périodes. Solution : 0 1,How to solve the following problem in Haskell?
478,4 $ & 1 1 1,The winning numbers in Saturday evening's drawing of
479,L 7 I 3 2,A look back at some of the most memorable moments
480,3 6 9 1 1.,BBC Sport takes a look back at some of the
481,"FIGURE 5.10.6 5.10 Etude de cas et Application Industrielle; cas d'un MDP 67 2. Nous constatons l'existence de 4 classes : la classe f0) qui est fermée et réduite à un point, et forme donc un état absorbant 0;",Nous constatons l'existence de 4 classes : la classe f
482,"la classe £1,5) qui est fermée et apériodique en raison des boucles en 1 et 5; la classe [2, 3,6,7,9) qui est fermée et de période 3, les 3 sous-classes étant dans l'ordre(2y, £3,6) et (7,9; enfin la classe [4, 8), que nous pouvons quitter vers les classes f0) et £1,5) et qui n'est donc pas fermée.",Une sous-classes sous-tant dans l'ordrey peut-tre tre
483,"Nous pouvons voir que nous quittons la classe non-fermée [4, 8) vers les classes fermées f0) et £1,5). Or, nous voyons que nous quittons la classe [4, 8) de 4 ou de 8, et que nous avons toujours deux fois plus de chance de nous retrouver en 0 que dans la classe (1,5). Par suite la probabilité 2 2","Nous voyons que nous quittons la classe non-fermée [4, 8] vers les classes fermé"
484,"de se retrouver en 0 est de et en £1, 55 de 3 3",BBC Sport takes a look back at some of the
