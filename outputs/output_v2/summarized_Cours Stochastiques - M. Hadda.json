{
    "chunks": [
        "Polycopié RO pour le GI Modèles stochastiques pour la prise de décision Mohammed Hadda 4ème année GI-IADS (2022-2023) ENSAM de Meknès Université Moulay Ismail Table des matières 1 Introduction 5 2 Notions générales de probabilités 7 2.1 Espace probabilisé 7 2.1.1 Espace mesurable 7 2.1.2 Mesurabiité 8",
        "2.1.3 Probabilités conditionnelles : 10 2.1.4 Mesure image; Lois 10 2.1.5 Variables aléatoires discrètes 11 2.1.6 Lois discrètes usuelles : 11 2.1.7 Variables aléatoires réelles et Lois à densité 14 2.2 Théorèmes de convergence 16 2.2.1 Lois des grands nombres : 16 2.2.2 Théorème central limite : 16 3",
        "Simulation de Variables aléatoires 19 3.1 Simulation de Variables aléatoires discrètes 19 3.2 Simulation de Variables aléatoires à densité 19 3.2.1 Simulation d'une v.a. admettant une densité continue par inversion de la fonction de répartition 19 3.2.2 Simulation d'une va. bornée à densité bornée par la méthode de rejet 20",
        "3.2.3 Simulation d'une loi normale : 20 3.3 Méthode de Monte Carlo pour le calcul des intégrales et des moments : 20 3.3.1 Méthode 20 3.3.2 Précision (estimation d'erreur) : 21 3.3.3 Inérêts de la méthode 21 3.3.4 Calcul approché des moments 21 3.4 Méthode de \"simulation Monte Carlo\" 22",
        "4 Modèles stochastiques 23 4.1 Processus stochastiques 23 4.1.1 Introduction 23 4.1.2 Processus stochastiques 23 4.2 Processus de Wiener ou mouvement brownien 24 4.3 Processus gaussien 25 4.4 Processus de Poisson 25 4.5 Chaines de Markov 25 4.5.1 définitions et propriétés 25 4.5.2 Probabilités et matrice de transition 26",
        "4.5.3 Propriétés des chaines de Markov homogènes 27 4.5.4 Classification des états d'une une chaine de Markov 31 4.5.5 Lois ou probabilités stationnaires: : Temps moyen de premier retour 36 4.6 Application : 39 5 Processus de décision markoviens 43 5.1 Processus de décision markoviens 43",
        "5.2 Problèmes décisionnels de Markov 53 5.3 Politiques d'actions 53 5.4 Politique markovienne et chaine de Markov valuée 55 5.5 Critères de pertormance 56 5.6 Fonctions de valeur 57 5.6.1 1-cas du crière fini 57 5.6.2 2- cas du crière moyen 57 5.7 Politiques markoviennes 58",
        "5.7.1 Equivalence des politiques nstolre-dépendontes et markoviennes 58 5.8 Caractérisation des politiques optimales 58 5.8.1 Cas du critère fini 58 5.8.2 Cas du critère moyen 60 5.9 Algorithmes de résolution des MDP 61 5.10 Etude de cas et Application Industrielle; cas d'un MDP 63 5.10.1 Exemple de système de production",
        "63 2 % O o - G 9 / - 1 2 % o 39 G - 5 1. Introduction 1 Dans le cas d'un problème d'optimisation déterministe, les valeurs de tous les paramètres sont supposées connues. Alors que dans un problème d'optimisation stochastique un certain nombre de paramètres peuvent",
        "être considérés comme inconnues et sont modélisés par des variables aléatoires. Plus généralement, l'incertitude est représentée par des expériences aléatoires dont l'issue est notée Q. L'ensemble de toutes les issues possibles est l'espace fondamental .",
        "Par exemple, si les éléments de 2 sont les conditions climatiques, ils permettent de décrire des variables aléatoires telles que la consommation éléctrique. Ainsi, L'optimisation stochastique est un cadre qui permet de formuler un problème d'optimisation",
        "dans lequel les données sont incertaines (par exp. les prix des énergies). C' est aussi un ensemble de méthodes et de techniques de résolution. Dans un problème d'optimisation stochastique, on cherche souvent à prendre une décision avant que les valeurs des réalisations des variables aléatoires ne soient observées. 2 %",
        "O o - G 9 / - 1 2 % S% - 2. Notions générales de probabilités Dans ce chapitre introductif nous présentons les notions basiques de théorie des probabilités qui seront utilisées dans la suite de ce cours. 2.1 Espace probabilisé 2.1.1 Espace mesurable",
        "On considère un ensemble quelconque 2, qui modélise le caractère aléatoire d'un certain phéno- mène et portera dans la suite le nom d'ensemble fondamental, ou d'espace d'issues (des chances, d'échantillons). On note 9P(2) l'ensemble des parties de 2.",
        "Rappelons que si Card. 9P(2) - n; alors Card 9P(2) = 2\" : Par exp si 2 = (1,2,3) alors 9P(2) = 0,0),12),1).0.2) (1,3),12,3),2). Quand 2 est fini ou dénombrable, l'ensemble 9P(2) décrit les événements aléatoires associés à l'expérience considérée.",
        "Quand 2 est infini non-dénombrable, l'ensemble 9P(2) est parfois trop gros pour pouvoir étudier mathématiquement la probabilité de ces événements. On a alors recours à la notion de tribu : Définition 2.1.1 Une tribu ou O-algèbre bg sur 2 est un sous-ensemble de 9(2) contenant",
        "2 et vériflant les propriétés suivantes : e SiA € by alors Ac € by (stabilité par passage au complémentaire). . Si (An)n est une suite de by alors UnAn € by (stabilité par réunion dénombrable). Remarque : by est stable par réunion finie et par intersection finie.",
        "II faut comprendre une tribu comme la quantité d'information assocée à une certaine expérience aléatoire. Remarquons qu'une tribu contient forcément 0. Exemples de tribus sur 2 : (0,2) tribu triviale (la plus petite) et 9P(2) tribu complète (la plus grande); ou 0,A,A,2).",
        "Si by est une tribu sur , on dit que (2, 9) est un espace mesurable, ou aussi probabilisable 8 Chapitre 2. Notions générales de probabilités lorsqu'on va le munir d'une mesure de probabilté, et les éléments de by sont des parties by mesu- rables (événements).",
        "Une sous-tribu is de by est une tribu incluse dans by au sens où A E S - AE.9. Remarque : Toute inersection de tribus sur 2 est une tribu sur 2. Mais une réunion de tribus n'est",
        "pas forcément une tribu;en effet si A1,A2 C 2, alors 91 = 0,A1,AS,2) et 91 = (0,A2,Ag,2) sont des tribus, mais pas 91 U.92 en général, par exp si A1 NA2 £ 91 J.92 Définition 2.1.2 Soit a une partie quelconque de 9P(2). On appelle tribu engendrée par d,",
        "et on note C(G) l'intersection de toutes les tribus contenant d. Définition 2.1.3 On appelle tribu des boréliens de R, on note 9B( (R), la tribu engendrée par les intervalles ouverts Ja,bla<be ER. Exemples d'espace mesurable : 1. (2, 9P(2)) si 2 est fini. 2. (R, B(R). Remarque :",
        "1. Dans cette définition, on peut remplacer \"ouvert\" par \"fermé\" ou par \"ouvert à droite\" ou : 2. 9B(R) $ 9(R). 2.1.2 Mesurabilité Définition 2.1.4 Soit (2,9F) et (E,6) deux espaces mesurables. Une application X : E",
        "est dite bg 6 mesurable si X 4) € g pour tout A € 6 où X (A) - fo E 2: X(O)EA; autrement dit si X (6)c9. Remarque : La mesurabilité d'une application est conservée chaque fois que l'on diminue (au sens",
        "de l'inclusion) la tribu de l'espace d'arrivée ou que l'on agrandit celle de l'espace de départ. Une fonction X : R 1 R est dite borélienne si elle est 9B(R) 9B(R) mesurable, c-à-d si X (A) €",
        "9B(R) pour tout A € B(R). En fait, il suffit que cette propriété soit vérifiée pour les intervalles ouverts A. En particulier, les focntions continues sont boréliennes car l'image réciproque d'un intervalle ouvert par une fonction continue est une réunion d'intervalles ouverts, donc un borélien.",
        "Définition 2.1.5 Soit (2,9) un espace mesurable. Une variable aléatoire réelle (v.a.r.) X est une application mesurable de (2, 9) dans (R,B(R). Si X(2) est au plus dénombrable et X est y 9(R) mesurable, X est dite variable aléatoire discrète réelle.",
        "Définition 2.1.6 Soit (2,9) un espace mesurable. Un vecteur aléatoire X est une application mesurable de (,9) dans (R4,B(R4) (d > 1). Si X(2) est au plus dénombrable et X est by R4) mesurable, X est dit vecteur aléatoire discret.",
        "Exemple : Un exemple simple, mais important, d'application mesurable de 2 dans R (et de v.a. 2.1 Espace probabilisé 9 discrète) est La fonction indicatrice d'un ensemble A € by : 1 SIOEA 1A 0) 0 sinon En effet pour tout B C R, on a 0 siOfBetlgB A siOfBetlEB",
        "(1A)-'(B) = Ac si0EBetlfB 2 si0EBetlEB Remarquons bien que (1A)-'(9(R) = C(A)). Ceci montre que 1A est C(fA)) - 9P(R) mesu- rable. De même, on vérifie aisément qu'une fonction constante de 2 1 R est mesurable pour toute tribu sur 2 et toute tribu sur R.",
        "Remarquons également que les v.a.r. sont stables par composition avec les fonctions boréliennes : si X est une v.a.r. is mesurable et f une fonction borélienne, alors f(X) est une v.a.r 9 mesurable. Les v.a.r. peuvent être approchées en limite croissante simple par des v.a.r. \"en escalier\" qui sont du",
        "n type X a;lA, avec ai € R, Ai E. Définition 2.1.7 La tribu engendée par une variable aléatoire X définie sur (2, 9) est l'en- semble (X) = £x-(A), A € B(R).",
        "c(X) est la plus petite tribu sur 2 rendant X mesurable. En plus, (X) est une tribu contenue dans 9. Définition 2.1.8 Soit (2, 9) un espace mesurable. Une mesure de probabilité ou simplement une probabilité sur (2, 9) est une application P de 9 dans [0, 1] vérifiant",
        ". P(2) = 1 e Si (An)meN est une suite d'éléments de by deux à deux disjoints, alors (UA)-EPA) (propriété de o - additivité). n-0 n-0 On utilise souvent l'écriture avec les fonctions indicatrices : P(A) dP 1AdP. A Le triplet (,9,P) est appelé espace de probabilité ou espace probabilisé.",
        "Proposition 2.1.1 On a (i) P(A) + P(AP) - 1 VA € by (ii) A C B 1 P(A) < P(B) (iii) P(AUB) + P(ANB) = P(A) + P(B) VA, B € 9. Exemple : (Lancé de dés) On considère l'expérience aléatoire du lancer de deux dés.",
        "L'espace d'échantillonnage est 22 = ((1,1),(1,2), (6,6)). Soit X la variable aléatoire définie par la somme des résultats des deux dés. Dans ce cas on a P(X = 1) = P(0) = 0; P(X = 3) = P(fo € /X(0) = 3) = P(1,2),2,1)D)- S 18 10",
        "Chapitre 2. Notions générales de probabilités P(X < 2) = P(X = 2) = P(fo € 2/X(0) = 2) = P((1,1))) = 36 P(X S 4) = 44,1,2.0.32.226.0) 6 50 Remarque : II existe une unique mesure de probabilité 2 sur ([0, [0, 1) telle que 2(a,bD =",
        "b a, pour tous a, b € [0,1,a < b; appelée mesure de Lebesgue sur [0, 1). Définition 2.1.9 Soit (2, 9,P) un espace de probabilité. Un ensemble A € by est dit négli- geable pour P si P(A) = 0.",
        "Par exemple les singletons, les ensembles dénombrables de [0, 1] sont négilgeables pour la mesure de Lebesgue. 2.1.3 Probabilités conditionnelles : Si A et B sont deux événements tels que P(B) > 0, la probabilité conditionnelle de A sachant B, notée P(AIB) est définie par P(ANB) P(AIB) = P(B)",
        "BCA 1 P(AIB) = 1 eA et B indépendants 1 P(AIB) = P(A). Rappel : Un système d'événements (Bi)iel est dit exhaustif ou complet si 2 = UieiBi, union au plus dénombrable (I C N) et disjointe (B;B; = 0, Vij). Formule des probabilités totales",
        "Si (Bi)iel est un système exhaustif (fini ou dénombrable) d'événements, et si ViEI,P(Bi) * 0, alors pour tout événement A on a P(A) - EP(AIB,)P(B) = EP(ANB). iEl El Formule de Bayes Avec les notations précédentes, si P(A) > 0 on a P(AIB;)P(B;) P(B,IA) = Ee,PAB)P(B) 2.1.4 Mesure image; Lois",
        "Les applications mesurables permettent de < transporter > la mesure d'un espace à un autre. Dans le cas d'une variable (ou d'un vecteur) aléatoire, la \" mesure image \" ainsi obtenue s'appelle la loi de la variable (ou du vecteur) aléatoire.",
        "Définition 2.1.10 Soit une v.a.r. définie sur (2,9,P). La loi de X (sous P) est la probabilité Px sur(R, 9B(R) définie comme mesure-image de P par X : pour tout A € 9B( (R), Px(A) = P(X (A)) = Pfo € 2/X(0) E Aj.",
        "Pour simplifier, on note Px(A) = P(X EA). 2.1 Espace probabilisé 11 2.1.5 Variables aléatoires discrètes La loi d'une v.a discrète X est la donnée de toutes les valeurs P(X = xk) = Pk (Pk € [0, 1) lorsque Xk prend toutes les valeurs possibles dans X( (2).",
        "Dans ce cas VB € B(R), P(X € B) = E P(X - xk) = Z Pk- XKEB XKEB Définition 2.1.11 Soit (21,91,P1) et (22,92,P2) deux espaces de probabilité. Soit X (resp.",
        "Y) une variable aléatoire discrète définie sur 21 (resp. sur 22). On dit que. X et Y ont même loi si X(21) = Y(22) et si pour tout x € X(21) on a PI(X - x) = P2(Y = x). On note X 2 Y.",
        "Définition 2.1.12 e Soient X et Y deux v.a. discrètes. La loi conjointe du couple (X,Y) est la donnée de toutes les valeurs de P(X =x,Y = y) pour (x,y) € X(2) X Y(2). Les lois de X et de Y sont les lois marginales de X et de Y.",
        "e Soit x € X(2) tel que P(X = x) > 0. On appelle loi conditionnelle de Y sachant (X - x) la probabilité Px définie sur Y( (2) par P(X - VyEY(2), Px(b!) - P(Y P(X - x",
        "Proposition 2.1.2 = - v.a. discrètes indépendantes. Deux v.a. discrètes X et Y sont indépen- dantes ssi pour tout A C X(2) et tout B C Y(2), on a P(X E A,Y € B) = P(X EA)P(Y € B).",
        "Définition 2.1.13 - Espérance. Soit X une v.a. prenant ses valeurs dans fxk K N 1). Si la famille (P(X = xK))k est sommable c-à-d si 5 xyP(X =. xk) < +oo, on dit que X est K21 intégrable et on définit l'espérance de X par",
        "E(X) = L xP(X = xk). K21 Formule de transfert : Soit 9 R R telle que (X) est intégrable, alors E((X))= Z (x)P(X = xk). K21 Définition 2.1.14 - Variance. Si E(X2) < 00, on appelle . variance de X le réel",
        "Var(X) = E[(x - E(X))2]= L (xk E(X))?P(X =. xk) ( Var(X) = E(X2)-(E(X). K21 e écart-type de X le réel c(X) = VVar(X). covariance de X et de Y (si les moments d'ordre 2, E(X2) < 0o et E(Y2) < co) le réel",
        "Cov(X,Y)= E((X - E(X))(Y-L E(Y)) = E(XY) - E(X)E(Y). 2.1.6 Lois discrètes usuelles : Loi de Bernoulli. On dit qu'une v.a. X suit une loi de Bernoulli de paramètres P E [0, 1] et on note X 2 B(p) si X est à valeurs dans f0, 1 et que",
        "P(X = 1)=p et P(X (=0)=1-P 12 Chapitre 2. Notions générales de probabilités On a E(X) = P et Var(x) = P(1 P). Dans ce cas, la v.a. X vaut 1 s'il y'a succès et 0 s'il y'a échec.",
        "Loi uniforme. On dit qu' une v.a. X suit une loi uniforme sur f1, ,nj et on note. X 7 ,ny) si X prend ses valeurs dans £1, ,nj et P(X = k) pour tout k E f1, ny. n n + 1 n - On a E(X)",
        "et Var(x) - 2 12 Loi binomiale. On dit qu'une V.a. X suit une loi binomiale de paramètres n N let € [0,1] et on note X 2 B(n,p) si X prend ses valeurs dans f0, nj et P(X = k) p\"(1 - p)\"-k Vk € f0, ,nj.",
        "On a E(x) = np et Var(X) - np(1 p ). La loi binomiale donne le nombre de succès qu'on peut obtenir en répétant n épreuves simi- laires (de même loi) et indépendantes ayant pour issues un échec et un succès (c-à-d n épreuves de Bernoulli).",
        "Loi de Poisson. On dit qu'une v.a. X suit une loi de Poisson de paramètre A > 0 et on note X 2 9P(2) si X prend ses valeurs dans N et Pk P(X Vk € N. k! On a E(X) = A et Var(X) = 2.",
        "La loi de Poisson décrit le comportement du nombre d'événements se produisant dans un in- tervalle de temps donné, lorsque la probabilité de réalisation d'un événement est très faible et que le nombre d'essais est très grand.",
        "Si le nombre moyen d'événements dans un intervalle de temps fixé est A, alors la probabilté qu'il existe exactement k événements (k entier naturel, k = 0, 1,2...) est donnée par Pk- Exemples :",
        "1. Si un événement se produit en moyenne 5 fois par seconde, pour étudier le nombre d'événements se produisant pendant 60 secondes, on choisit comme modèle une loi de Poisson de paramètre 2 = 60 X 5 = 300.",
        "2. Soit X la variable aléatoire du nombre de personnes réservant un billet d'avion pour la Mecque le 20 mars à 9H30. X suit une loi binomiale dont l'effectif est très grand, tous les clients potentiels,",
        "des millions; et le paramètre P est très petit, la probabilité pour qu'un individu choisi au hasard ait envie de se rendre à la Mecque le 20 mars par le vol de 9H30. On approxime en général la loi de X par la loi de Poisson de paramètre np.",
        "3. Soit X la variable aléatoire égale au nombre d'appels reus par un standard téléphonique dans un intervalle de temps [0,T), la loi de X est une loi de Poisson. 2.1 Espace probabilisé 13 4. Les clients arrivent à une banque ou un supermarché de manière aléatoire, ce qui signifie",
        "que l'on ne peut pas prédire quand certains arriveront. La loi ou la fonction de densité de probabilité décrivant de telles arrivées durant une une période spécifique est la loi de Poisson. Soit X la V. a. désignant le nombre d'événements c-à-d d'arrivées qui prend place durant une unité",
        "de temps spécifique par exp. une minute ou une heure. Etant donné que 2 est une constante connue, la loi de probabilité est définie par : 2k P(X - k=0, 1,2, k! La moyenne E(X) - A signifie que 2 doit représenter le taux ou la vitesse avec laquelle l'événe-",
        "ment se produit. On rencontre la distribution de Poisson dans l'étude des files d'attente. Par exp., les travaux de réparation arrivent à un petit atelier de réparation de moteurs de facon totalement aléatoire à un taux de 10 par jour.",
        "1. Quel est le nombre moyen de travaux reçus par jour? 2. Quelle est la probabilité qu'aucun travail de réparation n'arrive pendant une heure, en suppo- sant que l'atelier est ouvert 8 heures par jour. En effet,",
        "1. le nombre moyen de travaux reçus par jour est égal à A = 10. 2. Pour calculer la probabilité qu'aucun travail de réparation n'arrive pendant une heure, on a besoin 10 de calaculer le taux d'arrivées par heure, c-à-d Aheure = - 1.25 travaux par heure. Ainsi 8 hheure",
        "- (Apam)e Paucune arrivée par heure) = P(X = 0) - 0.2865 0! Loi géométrique. On dit que X suit une loi géométrique dej paramètre pE [0, 1] et on: note X 2 9(p) si X elle est à valeurs dans N* et si P(X p(1 Vn E N*. I",
        "On a E(X) et Var(x) P - la loi géométrique représente le nombre d'essais nécessaires de réalisation d'une expérience jusqu'à son premier succès. En d'autres termes, cette loi correspond à la loi du temps d'attente dans un processus de Bernoulli. Remarque : Pour chacune des lois précédentes, on a EP(X=n)=1.",
        "Proposition 2.1.3 On a e Une loi de Bernoulli de paramètre P est une loi binomiale de paramètres 1 et p. Las somme de 2 v.a. indépendantes suivant des lois binomiales de paramètres (ni,P) et (2,P) respectivement suit une loi binomiale de paramètres (ni + n2,P).",
        "e La somme de deux v.a. indépendantes suivant respectivement des lois de Poisson de paramètres A1 et A2 suit une loi de Poisson de paramètre A1 + A2. 14 Chapitre 2. Notions générales de probabilités Proposition 2.1.4 Si (Xn)n est une suite de variables aléatoires de Bernoulli indépendantes de",
        "même paramètre P, et si X est la variable aléatoire qui donne le rang du premier succès dans cette successions d'épreuves, alors X suit une loi géométrique de paramètre p. Théorème 2.1.5 = Caractérisation comme loi sans mémoire. Soit X une variable aléatoire",
        "à valeurs dans N. X est sans mémoire c-à-d que Vn, k € N, P(X > n + k)X > n) = P(X > k) si et seulement si X suit une loi géométrique. L'unique loi de probabilité discrète à perte de mémoire est la loi géométrique.",
        "Cette propriété est le plus souvent exprimée en termes de < temps d'attente >. Supposons qu'une variable aléatoire X soit définie comme le temps passé dans un magasin de l'heure d'ouverture (disons 9H du matin) à l'arrivée du premier client. On peut donc voir X comme le temps qu'un",
        "serveur attend avant l'arrivée du premier client. La propriété de perte de mémoire fait une comparaison entre les lois de probabilité du temps d'attente du serveur de 9H à l'arrivée du premier client, et celle du temps d'attente du serveur pour",
        "qu'un client arrive à compter d'un délai arbitraire après l'ouverture (disons, par exp, une heure après l'ouverture soit à partir de 10H du matin) sachant qu'aucun client n' 'est arrivé de l'ouverture àl l'écoulement de ce délai arbitraire. La propriété de perte de mémoire affirme que ces lois sont les",
        "mêmes. Ainsi, dans cet exemple, ce n'est pas parce que le serveur a déjà attendu, pendant une heure l'arrivée d'un premier client qu'il peut espérer que le délai avant qu'arrive effectivement son premier client soit plus faible qu'au moment de l'ouverture.",
        "Attention : La perte de mémoire d'une loi de probabilité d'un nombre d'essais X jusqu'au premier succès signifie P(X > 50/x > 40) = P(X > 10), cependant, elle n'implique pas P(X > 50] > 40) = P(X > 50)",
        "à moins que les événements (X > 50) et (X > 40) sont indépendants, ce qui n'est pas le cas. 2.1.7 Variables aléatoires réelles et Lois à densité Proposition 2.1.6 - v.a. indépendantes. Soient X et Y deux v.a.r. sur 2. On dit que. X et Y sont",
        "indépendantes si pour tout A, B C R, P(X E A,Y € B) = P(X EA)P(Y € B). Définition 2.1.15 = Lois à densité. Soit f: R 1 R une fonction positive d'intégrale 1",
        "f(x)dx - On dit qu'une variable aléatoire X : 22 1 R a pour densité si sa loi 00 f est définie par VB C R, P(X € B) F(x)dx. B 2.1 Espace probabilisé 15 Dans ce cas, pour a < b réels on a",
        "P(a<X<b) = P(X Ela,bD = f(r)dx = f(x)dx ja,bl a Lois usuelles : 1 Loi uniforme sur [a,b) (a < b), notée A ([a,b)), de densité f(x) a,b(x). a Loi exponentielle de paramètre 2 (2 > 0), notée 8(2), de densité f(x) - Ae -Ax1R-().",
        "La loi exponentielle est l'analogue dans le cas continu de la loi géométrique : c'est la seule loi à densité continue sans mémoire, c-à-d vérifiant P(X > S +t)X > t) = P(X > s), Vs,t>0.",
        "Loi gaussienne ou normale de moyenne m et de variance G2 > 0, notée N (m, 0 ). de densité 1 n f(x) exp Vx E R. GV2r 202 Proposition 2.1.7 Si Xi et X2 sont deux v.a. indépendantes suivant respectivement des lois",
        "N (mi,o?) et N (m2, G2), alors X1 + X2 - N (mi + m2,0? + 02). Définition 2.1.16 = Fonction de répartition. Soit X une v.a. réelle sur 2. On appelle fonction de répartition de X, et on note Fx, la fonction définie par",
        "Vt € R, Fx(t) = P(X S1). Proposition 2.1.8 On a e Va, b € R, P(a <X < b) = Fx(b) - Fx(a). e Fx est croissante et satisfait lim Fx(t)=0, lim Fx(t)=1. 1-- t-+o0",
        "La loi d'une v.a.r. X est entièrement déterminée par la fonction de répartition Fx de X. Si X est une v.a.r. admettant une densité f, alors Fx() = f(x)dx, Vt € R. Exemple : (Lancé de dés) Soit X la somme des résultats des deux dés. On a",
        "Fx(1) = P(X K 1) = 0; Fx(4) = P(X < 4) 36 ; Fx(12) = 1. Proposition 2.1.9 Si (X,Y) est un couple de variables aléatoires qui admet pour densité fix,y) :",
        "R2 1 R, et si fx, fr sont les densités marginales de X et Y respectivement, X et Y sont indépen- dantes ssi foxp(x,y) = fx()fr0) pour presque tout (x,y) € R?. Proposition 2.1.10 SiX et Y sont deux V.a.r. indépendantes et de carré intégrable alors elles sont",
        "décorrélées, c-à-d E(XY) = E(X)E(Y). Définition 2.1.17 = Espérance. Soit X une variable aléatoire admettant pour densité f. Si xlf(r)dx < +oo, on dit que X est intégrable et on définit l'espérance (moment d'ordre 1) de JR 16 Chapitre 2. Notions générales de probabilités X par E(X) = xf(x)dx. JR",
        "Proposition 2.1.11 Soient X et Y deux v.a.r. intégrables sur 2. Alors, on a 1. Va, b E R, E(ax + b) = dE(X)+b; 2. E(X +Y) = E(X) + E(Y).",
        "Proposition 2.1.12 Soit X :-7 R une v.a.r. admettant pour densité f. Soit 9 : R 7 R telle que (X) est intégrable ( (x)lf(r)dx < +oo), alors R E((x)) = J (x)f()dx. La variance de X est le réel positif Var(X) = E[(X - E(X))2]= - (-E(X)F()dx TR",
        "Proposition 2.1.13 Soit X une v.a.r. de carré intégrable. On a 1. Var(X) = E(x2) + (E(X))2; 2. Va, b E R, Var(ax +b) = a?Var(X). 2.2 Théorèmes de convergence 2.2.1 Lois des grands nombres :",
        "Théorème 2.2.1 - Loi forte des grands nombres. Soit (X)nx1 une suite de variables aléa- toires réelles 2 a 2 indépendantes, de même loi (i.i.d.) d'espérance m. Pour tout n, on note X1 + + Xn X, n Alors pour tout E > 0, on les estimations et E((X,-m))s n",
        "(K.-me)se De plus, la suite (Xn)n2i converge presque sûrement vers m. Remarque : La loi des grands nombres exprime que la moyenne des n premiers termes d'une suite de variables aléatoires converge vers une constante qui est leur moyenne, c-à-d que la moyenne",
        "empirique de cette suite se concentre autour de son espérance. 2.2.2 Théorème central limite : On rappelle ici l'importance de la loi normale, comme étant la loi limite apparaissant lorsqu'on examine les fluctuations de la moyenne empirique d'une suite de v.a. i.i.d. autour de l'espérance.",
        "L'on sait que la moyenne empirique d'une suite de variables aléatoires undépendantes se concentre autour de son espérance; la question suivante est naturelle : que peut-on dire des fluctuations de la 2.2 Théorèmes de convergence 17",
        "moyenne empirique autour de l'espérance, c'est-à-dire de la distribution de X, - m? La réponse à cette question, est le théorème Central Limite, il affirme que : 1-Xn m est de l'ordre de 1/n 2- La distribution de Km approche la même distribution, lorsque n devient grand, quelle que soit",
        "la distribution des X; : tant que ceux-ci ont une variance G2 finie. Théorème 2.2.2 -= Théorème central limite. Soit (Xn)neN- une suite de v.a.r. i.i.d. de carré X1 Xn intégrable, de moyenne m = E(X1) et de variance G?. Soit Xn alors pour tout n",
        "n € N* on a m 0. 2 % O o - G 9 / - 1 2 % S96 G 5 3. Simulation de Variables aléatoires La majorité des langages informatiques permettent de générer des nombres pseudo-aléatoires. Ce",
        "sont en fait des suites récurrentes entières initialisées avec des paramètres liés à l'ordinateur puis renormalisées pour être à valeurs dans [0,1). Via sa fonction \"random\", un langage fournit une suite de v.a. indépendantes de loi uniforme sur",
        "[0,1). A partir de cette suite de v.a. indépendantes, on doit être capable de construire une suite de v.a. indépendantes suivant n'importe quelle loi. 3.1 Simulation de Variables aléatoires discrètes Simulation d'une V.a. de Bernoulli. Pour simuler une v.a. de Bernoulli de paramètre P E10,1,il",
        "suffit de simuler une v.a. U uniforme sur J0, 1[ et de poser X = lusp)ic-à-d X =1 si U < P avec probabilité P et 0 sinon. Simulation d'une v.a. binomiale. Pour simuler une v.a. binomiale de paramètres n > let E0,15,",
        "il suffit de sommer n v.a. de Bernoulli de paramètre p indépendantes. Simulation d'une V.a. discrète. Pour simuler une v.a. discrète à valeurs dans fx/xk € NJ de",
        "loi Pk = P(X = xk), k € N, il suffit de simuler une v.a. U uniforme sur J0,1[et de poser X = Xk si Pk-1 <U<P où Pk est le cumul des Pk : Pk - P2 + + Pk avec Po - 0.",
        "3.2 Simulation de Variables aléatoires à densité 3.2.1 Simulation d'une v.a. admettant une densité continue par inversion de la fonction de répartition Proposition 3.2.1 Soit. X une v.a. de densité f continue et de fonction de répartition F inversible.",
        "On pose Y = F(X). Alors Y suit une loi uniforme sur [0,1 1). 20 Chapitre 3. Simulation de Variables aléatoires Pour simuler la v.a. X, si F -1 est connue analytiquement, il suffit donc de simuler une loi uniforme sur J0, 1[ et de calculer F-1 du résultat obtenu.",
        "3.2.2 Simulation d'une v.a. bornée à densité bornée par la méthode de rejet Proposition 3.2.2 Soit X une v.a. presque sûrement bornée de densité f presque partout bornée.",
        "On suppose pour simplifier que X € [0, 1] et on choisit m tel que f K m p-p. Soient U et V deux v.a. indépendantes de lois uniformes sur [0, 1 et [0,m) respectivement. Alors, conditionnellement à [V<F(U), U a même loi que X.",
        "A partir de ce résultat, on construit la procédure de simulation suivante : étant donnée une v.a. X comme dans la proposition, on procède à la simulation de U et V indépendantes. Si les réalisations",
        "u et V de ces v.a. satisfont V < f(u) la réalisation de la simulation de X est u, sinon on procède à un autre tirage des v.a. U et V. 3.2.3 Simulation d'une loi normale :",
        "Danns certains cas, la connaissance des v.a. repose sur un changement de variable; ainsi si une v.a. s'exprime comme une fonction de v.a. indépendantes il est plus simple de simuler ces v.a. et de",
        "reconstituer la v.a. de départ à travers une fonction que de chercher à la simuler directement. La loi normale est un exemple de ce principe : Proposition 3.2.3 Soient U et V deux v.a. indépendantes de loi uniforme sur J0, 1. On pose",
        "X = V-2lnU COS 2rV, Y = V-2InU sin 2mV. Alors X et Y sont des V.a. indépendantes de loi (0,1). Pour générer une V.a. (0, 1), il faut générer deux V.a. indépendantes de loi uniforme sur J0,1. puis utiliser l'une des deux formules précédentes.",
        "3.3 Méthode de Monte Carlo pour le calcul des intégrales et des moments : La méthode de Monte Carlo pour faire des calculs d'intégrales est une méthode basée sur la simulation aléatoire, elle repose sur la loi des grands nombres.",
        "Sous certaines hypothèses, on peut approcher presque sûrement l'espérance d' une v.a. (une intégrale dans le cas à densité) par la moyenne arithmétique de v.a. de même espérance. Le principe de la méthode est alors de simuler ces v.a. et de considérer la réalisation de leur",
        "moyenne arithmétique comme une approximation numérique de leur espérance commune. 3.3.1 Méthode On suppose que l'on cherche à calculer numériquement I dx pour une fonction f intégrable. A l'aide de changements de variables, on se ramène au calcul de g(u)du pour une certaine 0 fonction g.",
        "Etant donnée U une v.a. de loi uniforme sur ]0, 1. Alors la formule de transfert s'écrit E(g(U)) = g(w)lpoa(u)du = g(u)du R Jo 3.3 Méthode de Monte Carlo pour le calcul des intégrales et des moments : 21 1 Ainsi en posant g(u) = I () )+F(-1-)",
        "u) pour u €]0, 1on a f(x)dx = g(u)du R 0 Soit (U)m21 une suite de v.a. indépendantes suivant une loi uniforme sur J0, 1. La loi des grands nombres affirme que 1 E8(U) g(u)du p.s. quand n * +oo. n i-1 Jo",
        "Une approximation numérique (estimateur) de l'intégrale I est alors donnée par la réalisation de cette somme. 3.3.2 Précision (estimation d'erreur) : On approche donc une constante (l'intégrale ou l'espérance) par la réalisation d'une v.a. Remarquons qu'il n'est pas possible de caractériser avec certitude l'erreur d'approximation que",
        "l'on commet, cependant on a les informations suivantes sur la v.a. : Ui) U g(u)du, Jo ceci veut dire que l'on approche la valeur de I sans erreur en moyenne. d'autre part, Var E8(U)) Var(g(U)), n",
        "ce qui montre que l'erreur quadratique moyenne tend vers 0 en O(1/n) quand n tend vers +oo. 3.3.3 Inérêts de la méthode Cette méthode converge assez lentement et repose sur la qualité du générateur de nombres aléatoires",
        "dont on dispose mais elle est très robuste. En particulier, en grandes dimensions @ 3), elle est plus rapide et plus simple d'utilisation que les intégrateurs numériques. 3.3.4 Calcul approché des moments On peut aussi utiliser cette méthode pour l'approximation des moments des V.a.",
        "En effet, toujours par la loi des grands nombres, l'espérance El(x)] d'une v.a. X pour une fonction à valeurs réelles 9 telle que P(X) est intégrable, peut être approchée p.s. quand n grand par E(p(x)) & - (X:) n = pour toute suite (Xi)i21 iid à X.",
        "Même lorsque X admet une densité f, cette approximation peut être beaucoup plus précise àn petit que le calcul direct de la valeur (x)f(x)dx R car les points alors choisis par la méthode de Monte Carlo se font selon la densité f et non unifor- mément. 22",
        "Chapitre 3. Simulation de Variables aléatoires 3.4 Méthode de \"simulation Monte Carlo\" La méthode de \"simulation Monte Carlo\" est un moyen qui permet de calculer une approximation numérique de la fonction de répartition, mais également de calculer des probabilités faisant interve-",
        "nir des phénomènes dont on ne connaît pas la distribution explicitement (phénomènes complexes dépendant de variables multiples). On suppose que l'on sait simuler une v.a. X (par exemple X est fonction d'un vecteur aléatoire dont",
        "on connait la loi). Pour calculer la probabilité P(X € B), on remarque que, si X a pour densité f, P(X E B) f(x)dx = 1p(x)f(r)dx = E(1B(X)). B /R Cette valeur, d'après la section précédente, peut s'approcher en simulant un grand nombre de fois",
        "la variable X et en faisant la moyenne arithmétique des valeurs des réalisations obtenues de 1B(X), c-à-d : P(X € B) & X 1B(X:) n pour des v.a. X; iid de même loi que X. Autrement cardi/X,EB) P(X E B) & n",
        "c-à-d la probabilité que X soit dans B est approchée par la proportion de simulations observées dans B. Enfin, grâce à cette méthode, on peut également calculer une distribution approchée de phé- nomènes aléatoires complexes.",
        "Dans le cas d'une variable aléatoire X à densité, par exemple, il s'agit d'obtenir une densité approchée, par exemple par une fonction constante par morceaux. Pour cela, on discrétise l'ensemble des valeurs prises par X en intervalles I \" assez petits \" : puis on",
        "approche P(X EI) par la méthode de Monte Carlo pour chaque intervalle I. On obtient ainsi une approximation discrète de la densité de X. Exercice (calcul de T par méthode de Monte Carlo) On cherche à évaluer T en remarquant que T - lp(x,y)dxdy, par une méthode de Monte Carlo.",
        "1. Montrer que T = 4.E (1p(x,Y)),ou X et Y sont deux variables aléatoires indépendantes uniformes sur [-1,1). 2. Proposer un estimateur P, de T par la méthode de Monte Carlo. 3. Quelle est l'espérance et la variance de PA. 2 % O o - G 9 / - 1",
        "2 % % 5 4. Modèles stochastiques 1 4.1 Processus stochastiques 4.1.1 Introduction . Dans un modèle déterministe, partant d'un état initial, on a un seul état final. Ce modèle est parfaitement connu et il n'ya aucun phénomène aléatoire. e Un modèle stochastique intégre une part d'aléa.",
        "En fait dans la réalité, il y'a de l'aléa : ( une panne de réveil , un retard à la SNCF, rencon- trer un ami, qui va gagner un match de foot-ball ? ...) Dans la réalité, partant d'un état initial, il y'a",
        "plusieurs états finaux;1 la connaissance est imparfaite en plus il y'a des phénomène aléatoires. On parle alors de stochasticité pour tout ce qui échappe au déterminisme. Un système stochastique est un système évoluant de manière probabiliste dans le temps. On",
        "trouve plusieurs exemples tels que la température quotidienne ou un centre d'appels téléphoniques. Un modèle stochastique est une représentation mathématique d'un système stochastique. Parmi les modèles stochastiques, on peut citer les processus stochastiques tels que les proces-",
        "sus de Wiener, de Poisson, de comptage, de Markov, les processus gaussiens et les files d'attente. 4.1.2 Processus stochastiques Définition 4.1.1 Soit (,9,P) un espace probabilisé. Unj processus stochastique est une famille de variables aléatoires X = (X:)IET, T appelé ensemble d'indices souvent assimilé au temps",
        "(T = R+,N, , définies sur (2,9,P) et à valeurs dans un espace métrique S muni de la tribu borélienne. Pour chaque 0 fixé dans , l'application qui à t associe X,(0) est appelée trajectoire ou réalisa- tion du processus. 24 Chapitre 4. Modèles stochastiques",
        "S est appelé espace des états ou des phases. Par exp. S peut être R, C, un ensemble fini ou dénom- brable. Si T est fini ou dénombrable, on dit que le processus est discret; si T n'est pas dénombrable, on parle de processus continu. Exemple : Marche aléatoire",
        "la marche aléatoire discrète à une dimension sur le réseau périodique Z est le modèle de marche aléatoire le plus simple. Imaginons un individu (ou \" particule \") sur un escalier, qui tire à pile ou",
        "face pour décider si le prochain pas sera vers le haut ou vers le bas. A chaque étape, il n'y a que deux possibilités : un pas en avant ou un pas en arrière. Notons :",
        "P(O<P < 1) la probabilité que la particule fasse un saut en avant (plutôt qu'un saut en arrière); c'est le seul paramètre libre du problème; q 1 - P la probabilité que la particule fasse un saut en arrière.",
        "Le cas le plus simple (mouvement brownien), consiste à faire considérer que les directions \"avant / arrière\" sont équivalentes (équiprobabilité) : P 9 - 1/2. Chacun des tirs au hasard pour choisir le mouvement constitue une épreuve de Bernoulli avec issues équiprobables.",
        "Après n pas, si N désigne le nombre de fois où on tire \"pile\", alors N suit la loi binomiale B(n,1/2) et on a 1 P(N = k) 2\" Notons Xn la position de la particule après n pas. En prenant Xo = 0 au départ et en ajoutant",
        "1 pour chaque pas en avant (pile) et en retranchant 1 pour chaque pas en arrière (face), on a X, = N (n - N) = 2N, n. Ceci montre que par rapport à la loi binomiale classique, Ça revient à décaler les résultats de n/2",
        "et de multiplier par 2. Dans ce cas, Xn = k si 2N, n - k, nécessairement n + k est pair; ainsi N = n+k avec n et k de même parité; On en déduit que P(X si n etk ont même parité 0 sinon",
        "Définition 4.1.2 = Processus à accroissements indépendants. Un processus (X:)IER+ est dit à accroissements indépendants si Vn > 1,0<11 < In, les accroissements Xi2 X, X, Xin-1 sont indépendants. Définition 4.1.3 = Processus à accroissements stationnaires. Soit un processus (X:)IER+.",
        "La propriété de statonnarité des accroissements signifie que la loi de X; - X; ne dépend que de la longueur t - S de l'intervalle de temps : pour t N S, X - Xs est de même loi que X-s. 4.2 Processus de Wiener ou mouvement brownien",
        "Un processus (W:)IER+ réel est un processus de Wiener ou mouvement brownien standard s'il est issu de 0 c-à-d W(0) 0 et s'il est à accroissements indépendants et stationnaires tel que Wi 2 N(0,t) pour tout t > 0.",
        "Le mouvement brownien est le nom donné aux trajectoires irrégulières du pollen en suspension dans l'eau, observé par le botaniste Robert Brown en 1828. 4.3 Processus gaussien 25 4.3 Processus gaussien Unj processus (X:)ET à valeurs dans R est dit processus gaussien si toute combinaison linéaire finie",
        "de (X)IET est une variable gaussienne (suit une loi normale), c-à-d Vn , 11 in Vai, d2, Cn € R : a,X + d2Xi2 + + CyXin 2 N (my,o,). 4.4 Processus de Poisson Définition 4.4.1 - Processus de comptage. Désignons par N(t) le nombre de \"tops\" se",
        "produisant dans l'intervalle de temps [0,t). Le processus (N(t)r20 est dit processus de comptage s'il vérifie : e N(O) = 0; Vt N 0, N() € N; H N(t) est croissante. Pour tout OKa<b, N(b) - N(a) représente le nombre de tops se produisant dans l'inervalle de temps Ja,b).",
        "Si (TA)meN est une suite de v.a.r. positives, alors le processus (N(t)20 défini par N() = L 1s) n>1 est un processus de comptage. et on a 0 si t<Ti N() n si IKi<Tt",
        "Définition 4.4.2 = Processus de Poisson. Un processus de Poisson de densité A > 0 est un processus de comptage (N(t)20 tel que : e le processus est à accroissements indépendants;",
        "e le nombre de tops se produisant dans un intervalle de temps de longueur t > 0 suit une loi de Poisson de paramètre At, c-à-d (Ar)\" Vs 0, > 0, Vn € N, P( V N(s) n!",
        "Remarque : Les processus de Poisson sont souvent utilisés pour modéliser des files d'attente, chaque top représentant l'appel d'un client au guichet. 4.5 Chaines de Markov 4.5.1 définitions et propriétés Définition 4.5.1 - Processus de Markov. Un processus de Markov est un processus stochas-",
        "tique satisfaisant la propriété de Markov, c-à-d Vti 12 In / 1, VA € B(R), P(X E AIXi, Xi2, Ain = P(X EAIX.) p.s. Cette propriété (de Markov) exprime que le passé et le futur sont incondtionnelement indépendants, le présent étant donné.",
        "En d'autres termes, pour un processus de Markov l'information utile pour la prédiction du futur est entièrement contenue dans l'état présent et ne dépend pas des états antérieurs; un tel processus est sans mémoire. Un processus de Markov en temps discret est une suite (Xn)n de variables aléatoires vérifiant la",
        "propriété précédente. La valeur X étant l'état du processus à l'instant n. 26 Chapitre 4. Modèles stochastiques Définition 4.5.2 = Chaine de Markov. Une chaine de Markov est un processus de Markov à temps discret ou à temps discret et à espace d'états fini ou dénombrable.",
        "Dans ce cas, la loi conditionnelle de Xn+1 sachant le passé s'exprime par Vn € N, Vio, i1, in-1,i,, P(Xn+1 = jXo i0, X1 11 - X, P(Xn+1 jX=i). Exemple : Soit R, n N 0 des variables indépendantes à valeurs dans E - N. Alors Sn R; et",
        "P L R; sont des chaines de Markov. i-1 Une chaine de Markov peut être vue comme un système dynamique, ce qui veut dire que Xn+1 = fn(Xn), où fn est une \"transformation aléatoire\" indépendante du passé. Dans l'exemple",
        "précédent, fn(X) est la somme (ou le produit) de X, (ici X, = Sn ou P) avec Rn+1- Si la transformation aléatoire fn ne dépend pas de n, c-à-d si Xn+1 = f(Xn) pour tout n pour une",
        "certaine transformation f, on dit que X est une chaine de Markov homogène. Définition 4.5.3 Soit X = (X)n2o une chaine de Markov à valeurs dans E fini. On appelle probabilité de transition de l'état ià l'état j en un pas (ou simplement probabilité de transition",
        "de l'état ià l'état j) le nombre P(Xn+1 = jXn = i). On note Pij = P(Xn+1 = jX, = i). Définition 4.5.4 Une chaine de Markov est dite homogène si ses probabilités de transition ne dépendent pas de n, c-à-d si",
        "Vn N 1, V(i,j)EE?, P(X+1 = jX,=1)=P(X, = jX-1=i). Remarque : Une chaine de Markov est homogène si Vn > 1, V(i,j) € E?, P(X+1 = jX, P(X1 jXo i). Proposition 4.5.1 Soit une suite Y = (Kn)m21 de variables aléatoires indépendantes et de même",
        "loi, à valeurs dans un espace F, et soit f: E X. F E une application mesurable. Soit la suite X = (Xn)n2o définie par : Vn € N, Xn-1 = f(Xn,En+1). On suppose que la suite Y est indépendante de Xo. Alors X est une chaine de Markov homogène.",
        "Remarque : Toute chaine de Markov homogène peut être simulée via une relation de récurrence de la forme Xn-1 = f(Xn,In-1) pour une fonction f bien choisie. Dans la suite, toutes les chaines de Markov sont supposées homogènes. 4.5.2 Probabilités et matrice de transition",
        "Il s'agit de modéliser une chaine de Markov à l'aide de représentations synthétiques afin de connaitre l'évolution des états du système. On utilisera les matrices ou les graphes. Définition 4.5.5 On appelle matrice de transition (quand E est fini) ou noyau de transition ou",
        "opérateur de transition (quand E est infini) la famille des nombres P = (Pij)G.)EE2. Proposition 4.5.2 La matrice de transition P = (Pij)G.)EE2 est stochastique, c-à-d V(i,j) EE2,Pij20 ViEE, Epij = 1 (la somme des termes de chaque ligne de P est égale à 1). jEE 4.5 Chaines de Markov",
        "27 Remarque : La matrice d'une chaine de Markov est forcément stochastique et inversement, toute matrice stochastique est la matrice d'une chaine de Markov. Pour k N 1, la probabilité de transition de l'état ià l'état j en k pas (sur k étapes), P(Xntk =",
        "jXn =i) ne dépend pas de n, c-à-d P(Xntk = jXn i P(Xk - jXo i). On note Pi9 - P(Xk - jXo - i). Exemple : Une grenouille monte sur une échelle. Chaque minute, elle peut monter d'un barreau",
        "avec probabilité 1/2, ou descendre d'un barreau avec probabilité 1/2. L'échelle a 5 barreaux. Si la grenouille arrive tout en haut, elle saute en bas de l'échelle avec probabilité 1/2 ou redescend d'un barreau.",
        "On appelle. X, la position de la grenouille sur l'échelle. L'espace d'états est donc E = (0,1,2,3,4,5). Si a un instant n la grenouille est au niveau i € £1,2,3,4) de l'échelle, alors à l'instant n + 1 elle sera : au barreau i+ 1 avec probabilité 1/2",
        "au barreau i- 1 avec probabilité 1/2. ce qui s'exprime par P(Xn+1 =i+ 1Xn = 1/2 = P(Xi Xo P(Xn+1 =i- Xn - = 1/2 (= P(Xi 1 1Xo Ces probabilités ne dépendent pas de n, il parait qu'il s'agit d'une chaine de Markov homogène. Si",
        "c'est le cas, la matrice de transition se traduit par : * * * * * * 1/2 0 1/2 0 0 0 0 1/2 0 1/2 0 0 P= 0 0 1/2 0 1/2 0 0 0 0 1/2 0 1/2 * * * * * *",
        "Si la grenouille se retrouve à l'état 5, alors elle peut soit passer à l'état 4, soit passer à l'état 0. La dernière ligne de la matrice est donc ( 1/2 0 0 0 1/2 0), là encore cela ne dépend pas de l'instant n.",
        "Si la grenouille est à l'état 0, elle ne peut que passer à l'état 1. La première ligne de la matrice est alors 0 I 0 0 0 0 (Xn) est bien une chaine de Markov homogène, avec matrice de transition P. 4.5.3 Propriétés des chaines de Markov homogènes",
        "Soit X = (Xn)m2o une chaine de Markov. Soit n € N. II ne faut pas confondre \"loi de X\" et \"loi conditionelle de Xn sachant Xn- 1 \". La seconde se calcule facilement à l'aide de la matrice de transition. 28 Chapitre 4. Modèles stochastiques",
        "Définition 4.5.6 On appelle loi initiale de X la loi de Xo, c-à-d la donnée des valeurs : T'(io) = P(Xo = io), io € E. Proposition 4.5.3 - loi de dimension finie d'une chaine de Markov - La loi d'une chaine de",
        "Markov X = (Xn)n2o est caractérisée par sa matrice de transition P et sa loi initiale : pour tout n N 1, la loi jointe de (Xo,X1, Xn) est donnée par P(Xo i0, X1 X0 - io), Pioi Piiz Pin-in",
        "I Définition 4.5.7 Un chemin est simplement la donnée d'un vecteur fini (xo,X1 Xn) € E\". Dans la prop. précédente, la probabilté P(Xo - io, X1 1- Xn In) est la probabi- lité de suivre le chemin (io,i1,. ,in).",
        "Dém. : En effet, en posant ak = P(Xo = io, X1 Xk - ik); par récurrence sur n on a : dn+1 = P(Xn+1 = in+1Xo = io, Xi - 1. Xn-1 in-1,Xn - n) dn",
        "- P(Xn+1 = in+1Xn - in)P(Xo = io) Piphl Piyiz Pin-in (pté de Markov) - P(Xo = io) PiphPiiz Pin-1iPinintl\" Proposition 4.5.4 = Propriété de Chapman-Komogorow la matrice de transition en k pas,",
        "(P(Xntk = jXn )) (,J)EE2 est égale à la puissance kème de la matrice de transition P. En notant PiR = P(Xntk = jXn =i) et p(K) = (Pij (k) G)EE2 ? on a p(K) - pk. Proposition 4.5.5 = lois marginales. La loi de Xn+k est donnée par",
        "P-D-LPX. -Dp IEE En particulier, on a P-D-EPA. = i)pij iEE et par récurrence, on a P(X, = j) X P(Xo )Pi (n) IEE Forme matricielle : On note T\" la loi ou distribution de Xn;c'est le vecteur-ligne T\" = (T)IEE où T = P(X, = i).",
        "Les deux formules précédentes se réecrivent \"tk-m\"pk et T\"+1 = T\"P, ainsi T\"=p\" (équations de Chapman-kalmogorow, Exercice : Une chaine de Markov avec états E = £1,2) a la matrice de transition a P a EJ0,1. I - a a 1. Diagonaliser P. 2. Calculer P\" pour n N 1.",
        "4.5 Chaines de Markov 29 3. Calculer la loi de X pour tout n, sachant que l'on part de l'état Xo = 1. Exemple 1: Doudou le hamster Exemple 2: The Gardener Problem (Problème du jardinier : [voir H. Taha]",
        "Every year, during the Marcth-tmroughsepember growing season, a gardener uses a chemical test to check soil condition. Depending on the outcome of the test, productivity for the new sea- son can",
        "be one of three states : (1) good, (2) fair, and (3) poor. Over the years, the gardener has observed that last year's soil condition impacts current year's productivity and that the situation can be described by the following Markov chain : State of the system next year",
        "1 2 3 State of 1 .2 .5 3 P = the system 2 0 5 5 this year 0 0 1 FIGURE 4.5.1 The transition probabilities show that the soil condition can either deteriorate or stay the same but never improve.",
        "For example, if this year's soil condition is good (state 1), there is a 20% chance it will not change next year, a 50% chance it will be fair (state 2), and a 30% chance it will deteriorate to a poor",
        "condition (state 3). The gardener alters the transition probabilities by using organic fertilizer. In this case, the transition matrix becomes : 1 2 3 1/ .30 .60 .10 PI = 2 .10 .60 .30 3 .05 .40 .55 FIGURE 4.5.2",
        "The use of fertilizer can lead to improvement in soil condition. II y'a 10% de chance que l'état du sol change de moyen vers bon c-à-d de l'état 2 vers l'état 1; 5% de faible vers bon et 40% faible vers moyen.",
        "Exercice 1 : An engineering professor acquires a new computer once every two years. The professor can choose from three models : M1, M2, and M3. If the present model is M1, the",
        "next computer can be M2 with probability .25 or M3 with probability .1. If the present model is M2, the probabilities of switching to M1 and M3 are .5 and .15, respectively. And, if the present",
        "model is M3, then the probabilities of purchasing M1 and M2 are .7 and .2, respectively. Represent the situation as a Markov chain. Exercice 2: A police car is on patrol in a neighborhood known for its gang activities. During a",
        "patrol, there is a 60% chance of responding in time to the location where help is needed; else regular patrol will continue. Upon receiving a call, there is a 10chance for cancellation (in which 30 Chapitre 4. Modèles stochastiques",
        "case normal patrol is resumed) and a 30% chance that the car is already responding to a previous call. When the police car arrives at the scene, there is a 10% chance that the instigators will have",
        "fled (in which case the car returns back to patrol) and a 40% chance that apprehension is made immediately. Else, the officers will search the area. If apprehension occurs, there is a 60% chance",
        "of transporting the suspects to the police station; else they are released and the car returns to patrol. Express the probabilistic activities of the police patrol in the form of transition matrix. Retour au problème du jardinier avec les engrais : Dans la suite la matrice Pi est notée P.",
        "1 2 3 1/.30 .60 .10 P = 2] .10 .60 .30 3 .05 .40 55/ FIGURE 4.5.3 The initial condition of the soil is good that is (0) - ( 1 0 0 ). Determine the absolute",
        "probabilities of the three states of the system after 1, 8 and 16 gardening seasons. .30 .60 .10/ 30 .60 .10) .1550 5800 .2650 p2 = .10 .60 .30 .10 .60 .30 = .1050 .5400 3550 .05 .40 .55 - - .05 .40 .55/ .0825 .4900 .4275",
        ".1550 .5800 .2650 .1550 .5800 .2650 P* = .1050 5400 .3550 .1050 5400 .3550 .0825 .4900 .4275/ .0825 .4900 .4275 .10679 .53295 .36026) = .10226 .52645 .37129 .09950 .52193 .37857) .10679 53295 36026) .10679 .53295 36026) ps = .10226 .52645 .37129 .10226 .52645 37129 .09950 .52193 .37857/ .09950 .52193 .37857/",
        ".101753 525514 .372733 .101702 .525435 .372863 .101669 525384 .372863 ArtiveWindawe FIGURE 4.5.4 4.5 Chaines de Markov 31 .101753 525514 372733 .101753 .525514 372733 p16 = .101702 .525435 .372863 .101702 525435 372863 .101669 .525384 372863/ .101669 525384 372863/ .101659 .52454 .372881 .101659 52454 372881 .101659 52454 .372881 FIGURE 4.5.5",
        "Ainsi x(1) vaut .30 .60 .10) (1 0 0) .10 .60 .30 = (.30 .60 .1) .05 .40 .55 FIGURE 4.5.6 .101753 525514 .372733) (1 0 0) .101702 .525435 .372863 = (.101753 525514 372733) .101669 .525384 .372863/ x(8) vaut FIGURE 4.5.7 .101659 .52454 372881",
        "(1 0 0) .101659 .52454 372881 = (.101659 52454 372881) .101659 .52454 .372881 x(16) vaut FIGURE 4.5.8 The rows of p8 and the vector of absolute probabilities a x(8) are almost identical. The result is",
        "more evident for p16 . It demonstrates that, as the number of transitions increases, the absolute probabilities become independent of the initial x(0). The resulting probabilities are known as the steady-state probabilities. 4.5.4 Classification des états d'une une chaine de Markov On s'intéresse à la \"dynamique\" des chaines de Markov.",
        "les états d'une chaine de Markov peuvent être classés sur la base de la probabilité de transition Pij de P. Etats accessibles à partir d'un état donné : On considère une chaine de Markov homogène. Soit (,j)EE2:",
        "Définition 4.5.8 On dit qu'un état j est accessible à partir d'un état i (ou qu'un état i conduit à un état j) si il existe n N 0 tel que P(Xn = jXo - i) > 0, c-à-d si In 20 tel que pi9 > 0. On",
        "note j tij. En particulier, un état j est toujours accessible depuis lui même, puisque Pjj (0) > 0. Classes de communication : 32 Chapitre 4. Modèles stochastiques Définition 4.5.9 On dit que iet j communiquent si j est accessible à partir de i et i accessible",
        "à partir de j, C- à-d s'il existe (n,m) € N2 tels que Pi >Oet Pm > 0. On note tj 47 ij. Proposition 4.5.6 La relation communiquer 47 ij\" est une relation d'équivalence. Ceci signifie que l'on peut regrouper. les éléments de E en paquets, chaque paquet regroupant tous",
        "les éléments qui communiquent entre eux. Ainsi on a une partition de E en classes d'équivalence appelées classes de communication. Définition 4.5.10 Une chaine X est irréductible s'il existe une seule classe d'équivalence, c-à-d si tous les états communiquent.",
        "La notion d'iréductibilité est liée à la matrice de transition de la chaine et non à la loi initiale. Pour montrer qu'une chaine de Markov est irréductible, il peut être utile de tracer un graphe orienté dont",
        "les sommets sont les états de la chaine et où une arête représente une transition possible (l'arête (i,j) existe uniquement si Pij > 0). La chaine est alors irréductible si et seulement s'il existe un chemin fermé passant au moins une fois par tous les états de la chaine.",
        "La relation être accessible, notée $ s'étend aux classes d'équivalence : pour deux classes Cet C' on a C+-C $ J(i,j) ECx C. it. $ fV(i,j) ECxC,it-j) Une classe C est dite finale ou fermée si elle ne conduit à aucune autre c-à-d si la classe est",
        "minimale pour la relation t; ceci s'exprime par : pour tout WeEIeCal-y-yec Sinon, elle est dite transitoire ou transiente. Si C= fioj est fermée, io est dit état absorbant. Définition 4.5.11 On dit que",
        "e un état j est absorbant s'il retourne à lui-même avec certitude dans une transition c-à-d si Pij = 1. e un état j est transient s'il peut atteindre un autre état mais ne peut pas lui-même être atteint à",
        "partir d'un autre état, c-à-d si lim PIR = 0 pour tout i. n-0o e un état j est récurrent si la probabilité d'être revisité à partir d' autres états est 1 (c-à-d partant",
        "de j, la chaine repasse en j presque sûrement). Cela peut se produire ssi l'état j n'est pas transient. e un état j est périodique de période t > 1 si un retour n'est possible qu'en t, 2t, 3t, étapes,",
        "dans ce cas la période est d(j) = t = pgedm/p) > 0j >1 1. Ceci signifie que p9 = 0 chaque fois que n n'est pas divisible par t. Si d(j) = 1,l'état j est dit apériodique. Proposition 4.5.7 Les états d'une même classe ont même période.",
        "on peut donc parler de la période d'une classe d'états. Définition 4.5.12 Une chaine de Markov est e irréductible récurrente si elle est irréductible et si tous les états sont récurrents; irréductible transiente si elle est irrééuctible et si tous les états sont transitoires.",
        "Proposition 4.5.8 Une chaine de Markov irréductible sur un espace E fini est irréductible récur- rente. Par exp., considérons la chaine de Markov définie par la matrice de transition 4.5 Chaines de Markov 33 0 1 0 0 0 0 1 0 P - 0 0 3 .7",
        "0 0 .4 .6 les états 1 et 2 sont transients parce qu'ils ne peuvent pas être atteints à nouveau une fois que le système se trouve dans les états 3 et 4.",
        "En plus, les états 3 et 4 peuvent être à la fois des états absorbants si P33 = P44 = 1. Dans un tel cas, chaque état forme un ensemble fermé. Example 17.3-1 (Absorbing and Transient States) Consider the gardener Markov chain with no fertilizer. 5 3 P -",
        "5 5 0 1 States 1 and 2 are transient because they reach state 3 but can never be reached back. State 3 is absorbing because P33 = 1. These classifications can also be seen when lim Pi) = Ois computed. For example, -00 p(100) = 1",
        "which shows that in the long run, the probability of ever reentering transient state 1 or 2 is zero, whereas the probability of being \"trapped\" in absorbing state 3 is certain. FIGURE 4.5.9 Example 17.3-2 (Periodic States)",
        "We can test the periodicity of a state by computing P\" and observing the values of pl) for n = 2,3,4, These values will be positive only at the corresponding period of the state. For FIGURE 4.5.10 34 Chapitre 4. Modèles stochastiques example, in the chain P - 1 0",
        "4 we have .24 .76 0 .904 .0960) .0576 .9424 0 p2= 0 1 0 P- 0 1 0 P* = 0 1 0 0 .76 24 .144 .856 0 0 .9424 .0576) .97696 .02304 = U 1 0 .03456 .96544 0",
        "Continuing with n = 6,7, P\" shows that Pil and P33 are positive for even values of n and zero otherwise. This means that the period for states 1 and 3 is 2. FIGURE 4.5.11 Exemple 1: Considérons la chaine de Markov définie par le graphe : 0,5 0,9 2",
        "0,25 0,4 0,25 0,1 0,5 0,5 1 0,6 0,5 0,5 FIGURE 4.5.12 cette chaine de Markov est réductible. On a 3 classes : £1,3), f2), (4,5j. Létat 2 est transitoire. Les classes £1,3), et (4,5) sont finales. Quitte à renuméroter les états, on peut écrire : 1 3 4",
        "5 2 1 0,5 0,5 0 0 0 3 0,4 0,6 0 0 0 P = 4 0 0 0,1 0,9 0 5 0 0 0,5 0,5 0 2 0,25 0,25 0 0 0,5/ 4.5 Chaines de Markov 35 FIGURE 4.5.13 Exemple 2: Soit le graphe 0,4 0,3 1 2",
        "0,3 0.8 0,2 FIGURE 4.5.14 Les classes sont f1,2) et £3). Une seule classe finale : (3). L'état 3 est absorbant. Exemple 3 : Considérons cette fois-ci la chaine de Markov définie par le graphe : 0.4 0,2 1 2 3 0,8 0,6 0,2 0,25 0,8 0,9 0,25 0,1 6",
        "5 4 0,75 0,75 FIGURE 4.5.15 Ici, la chaine est irréductible. Chaque état est périodique de période 2. 1 3 5 2 4 6 1 0 0 0 0,8 0 0,2 3 0 0 0 0,2 0,8 0 P= 5 0 0 0 0 0,75 0,25 2 0,4 0,6 0",
        "0 0 0 4 0 0,9 0,1 0 0 0 6 (0,25 0 0,75 0 0 0 FIGURE 4.5.16 36 Chapitre 4. Modèles stochastiques Exercice : On considère la chaine de Markov sur 5 états 1, 2, 3, 4 et 5 de matrice de transition P:",
        "1/2 0 1/2 0 0 1/3 2/3 0 0 0 P= 0 1/4 1/4 1/4 1/4 0 0 0 3/4 1/4 0 0 0 1/5 4/5 1. Dessiner la chaine de Markov. 2. Classifier les états.",
        "3. Quelle est la probabilité en partant de 1 et en 4 étapes d'arriver en 5? Définition 4.5.13 Une chaine de Markov fermée (irréductible) est dite ergodique si tous les états sont récurrents et apériodiques (c-à-d si la matrice P admet une puissance Ph > 0).",
        "Dans ce cas les probabiltés absolues, après n étapes, T\" = mopn convergent vers une distribution limite, quand n 1 0o, qui est indépendante de la loi initiale T. 4.5.5 Lois ou probabilités stationnaires; Temps moyen de premier retour Lois ou probabilités stationnaires :",
        "L'une des problématiques les plus courantes concernant les chaines de Markov homogènes consiste à déterminer leurs distributions (ou probabilités) invariantes (ou stationnaires) et à étudier la convergence éventuelle de la chaine vers ces distributions. Définition 4.5.14 On appelle probabilté invariante ou loi stationnaire pour P (pour la chaine)",
        "toute mesure T = (T)IEE sur l'espace d'états E vérifiant : e T = TP, ViEE, Ti >0, - Z=1. IEE Définition 4.5.15 Une chaine de Markov, de matrice de transition P, est stationnaire si et",
        "seulement si sa loi initiale To est une probabilté stationnaire, c-à-d si elle vérifie TP-. Dans ce cas pour tout n, la loi de X, vérifie T\" = T. Proposition 4.5.9 1. Si l'espace d'états est fini, il existe au moins une mesure stationnaire.",
        "2. Si la chaine est irréductible, il existe au plus une mesure stationnaire. Proposition 4.5.10 Si la chaine est irréductible sur un espace d'états fini, il existe une unique probabilité stationnaire. Proposition 4.5.11 Soit (Xn)eN une chaine de Markov à états finis f1, Nj. On suppose que",
        "pour tout iE f1,. ,Nj, (P(Xn = i))NeN converge et on note Ti sa limite. Alors T = (T1, TN) est une distribution invariante pour la chaine. 4.5 Chaines de Markov 37 Temps moyen de premier retour d'une chaine ergodique : STEADY-STATE PROBABILITIES AND MEAN RETURN TIMES OF ERGODIC CHAINS",
        "In an ergodic Markov chain, the steady-state probabilities are defined as \"i = lim a\"), j=0,1,2,... n 00 These probabilities, which are independent of (ao)), can be determined from the equations la = mP Sm-1",
        "(One of the equations in a = mP is redundant.) What 1 = mP says is that the prob- abilities a remain unchanged after one transition, and for this reason they represent the steady-state distribution. A direct by-product of the steady-state probabilities is the determination of the",
        "expected number of transitions before the systems returns to a state j for the first time. This is known as the mean first return time or the mean recurrence time, and is com- puted in an n-state Markov chain as Pij .. FIGURE 4.5.17 38 Chapitre 4. Modèles stochastiques",
        "Example 17.4-1 To determine the steady-state probability distribution of the gardener problem with fertilizer (Example 17.1-3), we have .3 .6 1 (\"i \"2 W3) = (i \"2 \"3) 1 .6 3 .05 4 55 which yields the following set of equations: T1 = 3m1 + .172 + .05m3",
        "T2 = .6m + .672 + .4m3 T3 = .Imi + .3m2 + .55m3 7I + 12 73 = 1 Recalling that one (any one) of the first three equations is redundant, the solution is",
        "71 = 0.1017, 2 = 0.5254, and \"3 = 0.3729. What these probabilities say is that, in the long run, the soil condition approximately will be good 10% of the time, fair 52% of the time, and poor 37% of the time. FIGURE 4.5.18",
        "The mean first return times are computed as 1 1 1 Pi1 = - 9.83, #22 = - 1.9,33 = .3729 = 2.68 .1017 .5254 This means that, depending on the current state of the soil, it will take approximately 10 garden-",
        "ing seasons for the soil to return to a good state, 2 seasons to return to a fair state, and 3 seasons to return to a poor state. These results point to a more \"bleak\" than \"promising\" outlook for the",
        "soil condition under the proposed fertilizer program. A more aggressive program should im- prove the picture. For example, consider the following transition matrix in which the probabili- ties of moving to a good state are higher than in the previous matrix: 35 .6 .05 P .6 1 25 4 .35",
        "In this case, Ti = 0.31, 72 = 0.58, and T3 = 0.11, which yields P11 - 3.2,22 = 1.7, and H33 = 8.9,a reversal of the \"bleak\" outlook given previously. 4.6 Application : 39 FIGURE 4.5.19 Example 17.4-2 (Cost Model)",
        "Consider the gardener problem with fertilizer (Example 17.1-3). Suppose that the cost of the fer- tilizer is $50 per bag and the garden needs two bags if the soil is good. The amount of fertilizer is",
        "increased by 25% if the soil is fair and 60% if the soil is poor. The gardener estimates the annual yicld to be worth $250 if no fertilizer is used and $420 if fertilizer is applied. Is it worthwhile to use the fertilizer?",
        "Using the steady state probabilities in Example 17.4-1,we get Expected annual cost of fertilizer = 2 x $50 x Ti + (1.25 x 2) x $50 x T2 + (1.60 x 2) X $50 x T3 = 100 X .1017 + 125 x .5254 + 160 x .3729 = $135.51",
        "Increase in the annual value of the yield = $420 = $250 = $170 The results show that, on the average, the use of fertilizer nets 170 - 135.51 = $34.49. Hence the use of fertilizer is recommended. FIGURE 4.5.20 4.6 Application :",
        "On dispose de deux machines identiques fonctionnant indépendamment et pouvant tomber en panne au cours d'une journée avec la probabilité 9 - A On note Xn le nombre de machines en panne au début de la n-ième journée.",
        "1. On suppose que, si une machine est tombée en panne un jour, elle est réparée la nuit suivante et qu'on ne peut réparer qu'une machine dans la nuit. Montrer que l'on peut définir ainsi une chaine de Markov dont on déterminera le graphe, la matrice de transition et éventuellement",
        "les distributions stationnaires. 2. Même question en supposant qu'une machine en panne n'est réparée que le lendemain, le réparateur ne pouvant toujours réparer qu'une machine dans la journée. 3. Le réparateur, de plus en plus paresseux, met maintenant 2 jours pour réparer une seule",
        "machine. Montrer que (Xn) n'est plus une chaine de Markov, mais que l'on peut construire un espace de 5 états permettant de décrire le processus par une chaine de Markov dont on donnera le graphe des transitions. Calculer la probabilité que les 2 machines fonctionnent",
        "après n jours (n = 1,n =2etn = 3) si elles fonctionnent initialement. Corrigé : 1. L'ensemble des états est E - f0, 1 car si le soir il y a une ou aucune machine en panne, le",
        "lendemain matin, il y en aura 0; et si le soir, il y en a 2, le lendemain matin, il y en aura une seule. Le nombre de machines en panne le matin ne dépend que de celui de la veille au matin et de",
        "ce qu'il s'est passé dans la journée, ceci indépendamment de la période de l'année. On a 40 Chapitre 4. Modèles stochastiques donc bien une chaine de Markov dont il faut déterminer la matrice de transition. On a Poo - 2q(1 q)=1-gou (1-q) correspond à aucune panne dans la journée et",
        "2q(1 - q) à une seule panne qui peut provenir d'une machine ou de l'autre; e P01 = 9 (les 2 machines tombent en panne dans la journée et ces pannes sont indépendantes entre elles); P10 - I 9 (la machine en panne est réparée et l'autre fonctionne tjs);",
        "P11 - 9 (la machine en panne est réparée et l'autre est tombée en panne). 1-4 4 Ainsi, on a la matrice : P = 1 - 9 9 La distribution stationnaire est déterminée en résolvant (To Ti) = (To Ti) avec To + T1 - 1. 1 9 9",
        "On obtient TO = 1-q+g 1-9 et Ti 4+4 or 4 4 ainsi TO 13 et TI 13 2. Dans ce cas, l'ensembles des états est E' - 0, 1,2) car aucune machine n'est réparée la nuit. On a Poo = (1-4 q)2 (c-à-d aucune panne dans la journée);",
        "Por = 2q(1 q) (une des 2 machines est tombée en panne); Pio =1-q (la machine qui fonctionne ne tombe pas en panne); Phi = 9 (la machine qui fonctionne tombe en panne, l'autre est réeparée; Pi2 = 0 (la machine en panne est sûre de refonctionner le lendemain);",
        "P20 = P22 = 0; P21 = 1 (une seule des 2 machines en panne est réparée). D'ou la matrice (1-q)2 2q(1-4) 4? P' = 1-4 9 0 0 1 0 La distribution stationnaire est déterminée en résolvant (1-4)2 2q(1-4) 9 (o TI T2) =(To TI T2) 1-4 9 0",
        "avec TO + T1 T2 1. 0 1 0 On obtient TO - 1+q+ 1-4 Ti 1+q+ 24-4 et T2 - 1+q+ ; d'oû TO = 48 79 TI et T2",
        "3. Si on garde l'ensemble des états E', on n'a pas une chaine de markov car, lorque l'on a 2 machines en panne par exemple, on ne peut pas savoir si le lendemain, on en aura 1 ou 2 en",
        "panne : tout dépend si c'est le premier jour de réparation ou le second. On est donc amené à introduire 2 états supplémentaires : 1': 1 machine en panne dont c'est le deuxième jour de réparation, 2': 2 machines en panne et deuxième jour de réparation pour la première.",
        "D'oû l'espace de 5 états E\" = f0, 1, 2, 1', 2'. Dans ce cas, on a 00 2q(1 01 = p\"o2 = 0; 11 12 - 0; P'1'1 4, 2 21' - 1; les autres probabilités sont nulles.",
        "Laj probabilité que les 2 machines fonctionnent après n jours, ceci revient à calculer les puissances de la matrice de transition P\" : 4.6 Application : 41 en = 1 cette proba est égale P(X1) = OXo = 0) = P'00 (le premier terme ds la matrice de transition P\");",
        "an = 2 cette proba est égale P(X2) = OXo = 0) 00P (1-q)4 (le premier terme ds la matrice de transition P\"2); . n - 3 cette proba est égale P(X3) = OXo = 0) 'oop 00p 00 + PuplwPo-l-q\"t",
        "24(1-4)3 (le premier terme ds la matrice de transition P\"3). 2 % O o - G 9 / - 1 2 % S96 O) / 5. Processus de décision markoviens 5.1 Processus de décision markoviens Définition 5.1.1 Un processus de décision markovien (Markov decision process, ou MDP) est",
        "un processus stochastique contrôlé satisfaisant la propriété de Markov et défini par : un ensemble d'états S (incluant un étant initial so) dans lequel évolue le processus; un ensemble d'actions A qui contrôlent la dynamique de l'état; un espace des temps T, ou axe temporel;",
        "une fonction ou modèle de transition P(s, a,s') P(s's,a) (les probabilités de transition entre états), où a € A(s); cette fonction définit l'effet des actions de l'agent sur l'environnement; e une fonction de récompense r qui permet de déterminer le(les) but(s) à atteindre et les éventuelles zones dangereuses de l'environnement.",
        "Cette fonction r peut être définie de différentes manières, suivant le problème à résoudre : * r:SXAX, S X R 1 [0, 1] cas général. r(s,a,s',v) désigne la probabilité d'obtenir une récompense Vj pour être passé de l'état S às en ayant effectué l'action a;",
        "*r:SxAx: S * R, récompense déterministe; *r:SxA 1 R, récompense déterministe ratachée à l'action en ignorant son résultat; *r:S 1 R, récompense déterministe ratachée à un état donné. Pour une action a fixée, P(s's,a) représente la probabilité que le système passe dans",
        "l'état s après avoir exécuté l'action a dans l'état S. Evidemment, on a EP(Is,a) =1. 44 Chapitre 5. Processus de décision markoviens i*, - - % pslspa, a FIGURE 5.1.1 - Processus décisionnel de Markov",
        "Dans le MDP représenté par la figure ci-dessus, à chaque instant t de T l'action di est appliquée dans l'état courant St, influençant le processus dans sa transition vers l'état S1+1. La récompense Ti est émise au cours de cette transition.",
        "Un MDP est un modèle général pour un environnement stochastique dans lequel un agent peut prendre des décisions (et où les résultats de ses actions sont aléatoires) et reçoit des récompenses. On suppose A et S finis.",
        "Comme résultat d'avoir choisi l'action a dans l'état S à l'instant t, l'agent décideur re- Çoit une récompense, ou revenu Ti = r(s,a) € R. Les valeurs de Ti positives peuvent être considéerés comme des gains et les valeurs négatives comme des côuts.",
        ". La représentation vectorielle de la fonction de récompense r(s,a) consiste en AI (=card(A)) vecteurs Ta de dimension s. Remarques : 1. Les MDP sont utilisés pour étudier des probèmes d'optimisation à l'aide d'algorithmes de programmation dynamique ou d'apprentissage par renforcement qui consiste, pour un agent",
        "autonome, à apprendre les actions à prendre à partir d'expériences, de façon à optimiser une récompense quantitative au cours du temps. 2. On peut également considérer des récompenses aléatoires r(s,a) et dans ce cas on considère",
        "la valeur moyenne Tt = F(s,a). En particulier, T peut dépendre de l'état d'arrivée s' selon r(s,a,s'). On considère alors la valeur moyenne F(s,a) = Es Pls,a)r(s,4,s). 5.1 Processus de décision markoviens 45 Grille (occupancy grid) Rdorh 1 Rboms Actions: A E: Go east W: Go west S: Go south",
        "N: Go north Si positions sur la grille 1 Ro on 3 Degré de désirabilité 2 0 0.4 But But +1 o 0 1 2 3 Actions(s) R(s) (fonction de récompense) FIGURE 5.1.2 Actions aux effets incertains Go South (S) État courant Action P(s' s,a) (modèle de transition) États successeurs",
        "possibles (25%) (50 % (25 % FIGURE 5.1.3 46 Chapitre 5. Processus de décision markoviens Décision Une décision est un choix d'une action dans un état c'est une règle < if state then action > Exemples: (21,12) e W (21,13)E ou (20,13) W (20,12)5 (21,12) E 0.2 0.3 W W",
        "0.5 N (21,12) S (20,11) S 0.9 S 0.2 0.1 0.8 (20,10) (1,1) FIGURE 5.1.4 Plan (politique) Un plan est une stratégie: choix d'une action (décision) pour chaque état un plan est également appelé une politique (policy) Exemples: c'est un ensemble de règles if state then action Plan n1 (21,13)5",
        "£(21,12) * W, 120,135 s (20,12) (20,13) e S, 0.2 Wi0.3 W W (21,13) * S, 0.5 (21,12) S (20,11) N. j S 0.9 S S 0.2 0.1 0.8 (20,10) - (1,1) FIGURE 5.1.5 5.1 Processus de décision markoviens 47 Plan (politique)",
        "Un plan est une stratégie: choix d'une action (décision) pour chaque état un plan est également appelé une politique (policy) Exemples: c'est un ensemble de règles if state then action Plan n1 (21,13)5 ((21,12) e W, (20,13) W (20,12)5 (20,13) S, W 0.2 0.3 W (21,13) S, 0.5 (20,11) N",
        "(21,12) 1 S 0.9 40.2 Plan T2 0.1 0.8 ((21,12)5, (20,10) (20,11) * S, (1,1) (21,10) * E, . FIGURE 5.1.6 Exécution d'un plan (politique) Notons T(s) l'action désignée par le plan TL dans l'état S voici un algorithme d'exécution ou d'application d'un plan While (1) f",
        "1. S = état courant de l'environnement; 2. a=n(s); 3. execute a; ) L'étape 1 peut impliquer de la détection (sensing) et de la localisation L'état résultant de l'exécution de l'action à l'étape 3 est stochastique FIGURE 5.1.7 48 Chapitre 5. Processus de décision markoviens Interpretation/application d'un plan",
        "L'application d'un plan dans un MDP résulte en une chaîne de Markov le modèle de transition est donné par P(s']s, T(s)) Exemples: Plan T1 (21,13)5 ((21,12) e W, (20,13) (20,13)-S, 0.2 W 0.3 (20,12)5 W W (21,13) S, 0.5 (20,11) N (21,12) ) A 0.9 - $40.2 Plann2 0.1 0.8",
        "((21,12) S, (20,10) E(1,1) - Activer Windows (20,11) S, Arrédez arv naramàtrec (21,10) Erindid FIGURE 5.1.8 5.1 Processus de décision markoviens 49 Exemple 1 de MDP : Cet exemple représente un processus de Décision Markovien à trois états distincts (s0,51,52)",
        "représentés en vert. Depuis chacun des états, on peut effectuer une action de l'ensemble fao,ai). Les noeuds rouges représentent donc une décision possible (le choix d'une action dans un état donné). Les nombres indiqués sur les flèches sont les probabilités d'effectuer la transition à partir du noeud",
        "de décision. Enfin, les transitions peuvent générer des récompenses (dessinées ici en jaune). +5 0.10 So 0.70 S1 a1 ao (1.0 0.20 0.5 0.95 ao a1 0.5 0.4 0.05 ao 0.40 0.30 0.6 (a1) 0.30 S2 - Exemple de processus de Décision Markovien à trois états et à 6",
        "deux actions. FIGURE 5.1.9 50 Chapitre 5. Processus de décision markoviens La matrice de transition associée à l'action ao est la suivante : 0.50 0 0.50 0.70 0.10 0.20 0.40 0.60 La matrice de transition associée à l'action ai est la suivante : 0 0 1.0 0 0.95 0.05",
        "0.30 0.30 0.40 En ce qui concerne les récompenses, on perçoit une récompense de +5 lorsque l'on passe de l'état 81 à l'état S0 en accomplissant l'action ao on perçoit une récompense de -1 (aussi appelée pénalité) lorsque l'on passe de l'état 82 à l'état S0 en accomplissant l'action ai",
        "FIGURE 5.1.10 5.1 Processus de décision markoviens 51 Exemple 2: Schéma d'interaction entre agent (intelligence artificielle) et son environnement; cas du contrôle d'un drone : Dans le cadre du contrôle d'un drone, l'idée de l'apprentissage par renforcement est de concevoir",
        "un agent implanté au sein de notre drone et dictant à ce dernier les décisions rationnelles à prendre dans chacune des situations rencontrées. A chaque instant, ces décisions seront prises selon les informations disponibles sur l'environnement : le signal de perception sur l'environnement (signal d'observation) et le signal de récompense.",
        "En intégrant l'environnement dans notre schéma, il apparait que nous avons décrit une boucle d'interaction entre, d'une part, l'agent et, d'autre part, l'environnement dans lequel évolue l'agent. Obs., Récompense Action FIGURE 5.1.11 St,Ar,7i, S+1,A,+1,741, Si+2, A1+2, T+2, Processus de génération des données d'apprentissage",
        "Ainsi, à chaque instant, l'agent exécute une décision, laquelle influence les informations (signaux de perception et de récompense) transmises à l'agent par le biais de l'environnement. Ce proces- sus se: répète encore et encore. Cette boucle d'interactions définit une série temporelle, composée de :",
        "- décisions (ou actions), notées A; ; signaux d'observations, notées S; signaux de récompenses, notées T. C'est cette série qui engendre nos données d'apprentissage. e Description du processus stochastique à contrôler : Question : sur quels modèles formels et sur quelles représentations de l'environnement",
        "s'appuie un agent afin qu'un drone autonome puisse interpréter une sche complexe et y agir de faon rationnelle ? 52 Chapitre 5. Processus de décision markoviens Les MDP constituent un de ces modèles, capable de décrire formellement les interactions",
        "entre un environnement et un drone autonome. Un MDP peut représenter de nombreux problèmes de prise de décisions séquentielles traités en apprentissage par renforcement. . Etat d'un processus de Markov : On rappelle que la boucle d'interactions entre un agent et son environnement produit deux signaux :",
        "un signal de récompense et une observation. Si l'observation contient toutes les données sur le système (composé de l'environnement et l'agent) nécessaires et suffisantes pour décrire l'évolution future de celui-ci, alors l'observation décrit parfaitement l'état du système et aucune autre information n'est nécessaire pour son contrôle.",
        "Cette propriété est appelée observation complète de l'état. Un système doté d'une telle propriété est dit totalement observable. En fait, Un état est la totalité de l'information nécessaire et suffisante pour prédire l'évolution future d'un système. Question : Comment un drone passe t-il d'un état à un autre ?",
        "On note bien que rien dans la définition d'un processus de Markov ne permet de commander un drone. En effet, on peut tout au plus suivre l'évolution de celui-ci au travers des états dans les- quels il passera. Le contrôle d'un processus de Markov requiert l'introduction de la notion d'actions.",
        "e Actions d'un MDP : Jusqu'ici, on a défini une chine de Markov comme un modèle capable de décrire la dynamique d'un système non contrôlé. Pour permettre la commande d'un système, on doit ajouter à une chaine de Markov un ensemble d'actions.",
        "On remarque bien que la fonction de transition d'un MDP contient autant de matrices de transition qu'il y a d'actions. Cela signifie que l'état successeur S1+1 = s' dépend de l'état courant Si = S mais aussi de l'action courante A; = a à travers la probabilité conditionnelle P(s,a,s').",
        "Prenons un exemple trivial afin d'illustrer un MDP. Considérons un drone déployé dans une grille 3 X 3. Initialement, en bas à gauche de la grille, le drone souhaite se rendre en haut à",
        "droite de celle-ci. II dispose pour cela de 4 actions : gauche, droite, bas et haut. L'a action haut réussit avec une probabilité de 0,8 et échoue en allant soit à gauche soit à droite avec probabilité 0,1 1.",
        "Question : Quel est la séquence d'actions qui offre la probabilité d'être en haut à droite ? Si les actions sont déterministes, leurs effets sont prédictibles avec certitude, alors la séquence recherchée est celle dont le chemin, allant de la cellule en bas à gauche jusqu'à la cellule en",
        "haut à droite, est le plus court, par exemple haut, haut, gauche, gauche. Si au contraire les actions sont stochastiques, leurs effets sont tirés suivant une loi de proba- bilité; alors la séquence haut, haut, gauche, gauche est peut-être l'une de celles recherchées. 5.2 Problèmes décisionnels de Markov 53",
        "FIGURE 5.1.12 5.2 Problèmes décisionnels de Markov 5.3 Politiques d'actions Une politique ou stratégie notée T (pour contrôler un MDP) décrit la procédure suivie par l'agent pour choisir dans chaque état (à chaque instant) l'action à exécuter. II s' agit d'une fonction 1:S- A dans le cas déterministe;",
        "T:SxA 1 [0, 1] dans le cas stochastique. Une politique déterministe définit précisément l'action à effectuer; Une politique stochastique est une famille de distributions de probabilité selon laquelle une action a doit être sélectionnée pour chaque état (ou historique observé h). 54 Chapitre 5. Processus de décision markoviens",
        "On obtient ainsi quatre familles distinctes de stratégies, comme indiqué sur le tableau : politique Tt déterministe aléatoire markovienne St dt at, St [0, 1] histoire-dépendante he at he, St 0, 1 Tableau 1.1. Différentes familles de politiques pour les MDP. FIGURE 5.3.1",
        "Une politique déterministe est notée T = f(s), Vt, Wsi) ou T = f(hr), Vt, Vhi; T(h) définit l'action a choisie àl l'instant t si on a observé l'historique h. . Une politique stochastique est notée T = fm(a,st),t, VSt, Vaj ou T = f(a,h:), Vt, Vhy, Vaj;",
        "où m(a,s) = T(a/s:) = P(a: - as: = s) représente la probabilité de choisir l'action a à l'instant t sachant que l'état à l'instant t est S. Ces quatre familles de politique définissent les quatre ensembles suivants : = ITHS l'ensemble des politiques histoire-dépendantes stochastiques (aléatoires) cà-d l'en-",
        "semble le plus général des politiques; - ITHD l'ensemble des politiques histoire-dependantes déterministes; - ITMS l'ensemble des politiques markoviennes stochastiques (aléatoires); - IIMD l'ensemble des politiques markoviennes déterministes. histoire-dépendante aléatoire histoire-dépendante markovienne aléatoire déterministe markovienne déterministe Figure 1.2. Relations entre les différentes familles de politiques FIGURE 5.3.2",
        "Remarque : La définition des politiques peut ou non dépendre explicitement du temps. Définition 5.3.1 - Politique stationnaire. Une politique est stationnaire si elle ne dépend pas du temps c-à-d si Vt,t, = Ty. Parmi ces politiques stationnaires, les politiques markoviennes déterministes sont centrales dans",
        "l'étude des MDP. II s'agit du modèle le plus simple de stratégie décisionnelle, on nomme leur 5.4 Politique markovienne et chaine de Markov valuée 55 ensemble D. Définition 5.3.2 - Politiques markoviennes déterministes stationnaires. D est l'ensemble",
        "des fonctions T qui à tout état de S associent une action de A : T:SES T(s) EA Un autre ensemble important, noté D4 est constitué des politiques markoviennes aléatoires stochastiques) stationnaires.",
        "Les politiques de D et D4 sont très importantes car, comme on le verra, D et D4 contiennent les politiques optimales pour les principaux critères. 5.4 Politique markovienne et chaine de Markov valuée Un MDP et une politique markovienne T forment une chaine de Markov dont la matrice de transition",
        "est définie par Ws,s Pr(s,s') = P(S11 S'St s) - L (a,s)P(ss,a) dEA Dans le cas où T est déterministe, Pr(s,s') = P(ss,T(s)). La matrice Pr est construite simplement en retenant pour chaque état S la ligne correspondante dans la matrice Pr avec a = T(s).",
        "De même, on note Rt le vecteur de composantes R(s,(s)) pour T markovienne détermi- niste et L T(a,s)R(s,a) pour T markovienne stochastique. dEA Le triplet (S,Pa,R) est appelé un processus de Markov valué, ou chaine de Markov va-",
        "luée. II s'agit simplement d'une chaine de Markov avec des revenus associés aux transitions. Exemple : Imaginons un sujet qui, pendant l'hiver, souhaite décider chaque jour de prendre ou non un médicament pour lutter contre le rhume. On considère un horizon de temps infini (T = N).",
        "L'état du sujet, chaque matin, est soit malade (1) soit en bonne santé (0) : S = £0, 1. Et il peut choisir de prendre un traitement (1) ou non (0) :A = £0,1).",
        "On suppose que l'état du sujet au jour j+ 1 dépend uniquement de l'état du sujet le jour j et du fait qu'il ait pris ou non un traitement le jour j. L'évolution de la maladie est aléatoire et le traitement n'est pas systématiquement efficace. On",
        "suppose que la fonction de transition est connue et donnée par la matrice : a 0 1 S P(S1+1 = 1st = a) P(1, a 0 0.3 0.1 1 0.9 0.2 FIGURE 5.4.1 Chaque jour, le sujet obtient une récompense réelle modélisant son niveau de satisfaction de son",
        "état de santé, et prenant en compte le coût du traitement : 56 Chapitre 5. Processus de décision markoviens 1 S R(s,a) = 0 1 0.9 1 0 -0.1 FIGURE 5.4.2 une politique markovienne stationnaire déterministe possible est de choisir de prendre le traitement",
        "uniquement si on est malade : Vt € T,(s) =Osis, = 0, T(s) = 1 si St = 1. L'état de santé suit alors une chaine de Markov de matrice de transition : s 0 1 s Pr(s']s) = 0 0.7 0.8 1 0.3 0.2 FIGURE 5.4.3",
        "5.5 Critères de performance Se poser un problème décisionnel de Markov, c'est rechercher parmi une famille de politiques celles qui optimisent un critère de performance donné pour le processus décisionnel markovien considéré. Ce critère a pour ambition de caractériser les politiques qui permettront de générer des",
        "séquences de récompenses les plus importantes possibles. En termes formels, cela revient toujours à évaluer une politique sur la base d'une mesure du cumul espéré des récompenses instantanées le long d' une trajectoire, comme on peut le voir sur",
        "les critères les plus étudiés au sein de la théorie des MDP, qui sont respectivement : e le crière fini : E(Ro + Ri + R2 + + RN-1So) e le critère y-pondéré : E(Ro + yRi + yR2 +yR,so) e le critère total : E(Ro + R + R2",
        "Ri 0 e le critère moyen : lim + R2 + + Kn So). n-oo Les deux caractéristiques communes à ces 4 critères sont en effet d'une part leur formule additive en R, qui est une manière simple de résumer l'ensemble des récompenses reçues le long d'une",
        "trajectoire et, d'autre part, l'espérance E(.) qui est retenue pour résumer la distribution des récompenses pouvant être reçues le long des trajectoires, pour une même politique et un même état de départ. Ce choix d'un cumul espéré est bien sêr important, car il permet d'établir le principe",
        "d'optimalité de Bellman (\" les sous-politiques de la politique optimale sont des sous-politiques optimales \"), à la base des nombreux algorithmes de programmation dynamique permettant de résoudre efficacement les MDP. On va maintenant caractériser successivement les politiques optimales et présenter les",
        "algorithmes permettant d'obtenir ces politiques optimales pour chacun des critères précédents. 5.6 Fonctions de valeur 57 5.6 Fonctions de valeur Les 4 critères qu'on viens de voir permettent de définir une fonction de valeur qui, pour une",
        "politique T fixée, associe à tout état initial S € S la valeur du critère considéré en suivant T à partir de s: pour T fixée, V.S-R. On note W l'espace des fonctions de S dans R, identifiable à l'espace vectoriel RISI. L'en-",
        "semble est muni d'un ordre partiel naturel : VU,VEY USV $ VSES U(s) S V(s) L'objectif d'un problème décisionnel de Markov est alors de caractériser et de rechercher = si elles existent - les politiques optimales m* € ITHS telles que VI € ITHS VSES V\"(s) < V'(s) c-à-d",
        "a* € argmaxerws V On note V* = maxrerHs V = V. Dans le cadre des MDP, on recherche donc des politiques optimales meilleures que toute autre politique, quel que soit l'état de départ. Remarquons que l'existence d'une telle politique optimale n'est pas en soi évidente.",
        "La spécificité des problèmes décisionnels de Markov est alors de pouvoir être traduits en terme d'équations d'optimalité portant sur les fonctions de valeur, dont la résolution est de complexité moindre que le parcours exhaustif de l'espace global des politiques de ITHS (la taille du",
        "simple ensemble D est déjà de AISl). 5.6.1 1- cas du crière fini On suppose ici que l'agent doit contrôler le système en N étapes, avec N fini. Le critère fini conduit naturellement à définir la fonction de valeur (à horizon fini) qui associe à tout état S l'espérance de",
        "la somme des N prochaines récompenses obtenues en suivant la politique T à partir de s: Définition 5.6.1 - Fonction de valeur pour le critère fini. Si T = f0, 1, N y, on pose VSES VN( (s) - E\" X R,(St,a:)so =s) -0",
        "Dans cette définition, E\"() dénote l'espérance mathématique sur l'ensemble des réalisations du MDP en suivant la politique T. ET est associée à la distribution de probabilité Pr sur l'ensemble de ces réalisations. Notons qu'il est parfois utile d'ajouter au critère une récompense terminale TN fonction",
        "du seul état final SN. II suffit pour cela de considérer une étape artificielle supplémentaire où Vsi, ai, Rw(St,a;) = Rw(SN) (récompense terminele). C' est le cas par exemple lorsqu'il s'agit de piloter un système vers un état but en N étapes et à moindre coût.",
        "5.6.2 2- cas du crière moyen Lorsque la fréquence des décisions est importante, avec un facteur d'actualisation proche de 1, ou lorsqu'il n'est pas possible de donner une valeur économique aux récompenses, on préfère 58 Chapitre 5. Processus de décision markoviens",
        "considérer un critère qui représente la moyenne des récompenses le long d'une trajectoire et non plus leur somme pondérée. On associe ainsi à une politique l'espérance du gain moyen par étape. On définit alors le gain moyen p\"(s) associé à une politique particulière T et à un état S :",
        "Définition 5.6.2 Le gain moyen est VSES P\"(s) = lim E\" X R;(St,a)lso = S n-oo n Pour le critère moyen, une politique T* est dite gain-optimale si p\" (S) > p\" (s) pour toute politique T et tout état S.",
        "Ce critère est particulièrement utilisé dans des applications de type gestion de file d'attente, de réseau de communication, de stock etc. 5.7 Politiques markoviennes 5.7.1 Equivalence des politiques hisioire-dépendantes et markoviennes En voici une propriété fondamentale des MDP pour ces différents critères, qui est d'accepter",
        "comme politiques optimales des politiques simplement markoviennes, sans qu'il soit nécessaire de considérer l'espace total ITHA des politiques histoire-dépendantes. Proposition 5.7.1 Soit T € ITHS une politique aléatoire histoire-dépendante. Pour chaque état initial x € S, il existe alors une politique stochastique markovienne m' € IIMS telle que",
        "1. V() = V(x), 2. = V V7() 7 (x), 3. v X = V\"(r), 4. P\"(s)=p\"(x). Ce résultat permet d'affirmer que lorsque l'on connaît l'état initial (ou une distribution dej probabilité sur l'état initial), toute politique histoire-dépendante aléatoire peut être remplacée par une politique",
        "markovienne aléatoire ayant la même fonction de valeur. 5.8 Caractérisation des politiques optimales 5.8.1 Cas du critère fini Equations d'optimalité : Supposons que l'agent se trouve dans l'état S lors de la dernière étape de décision, confronté au",
        "choix de la meilleure action à exécuter. II est clair que la meilleure décision à prendre est celle qui maximise la récompense instantanée à venir, qui viendra s'ajouter à celles qu'il a déjà percues. On a ainsi : TEN-1(s) € argmaxaeA RN-1(s,a), et Vi(s) - max. Rw-1(s,a), dEA",
        "où TN-1 est la politique optimale à suivre à l'étape N - 1 et Vi la fonction de valeur optimale pour un horizon de longueur 1, obtenue en suivant cette politique optimale. 5.8 Caractérisation des politiques optimales 59",
        "Supposons maintenant l'agent dans l'état S à l'étape N - 2. Le choix d'une action a va lui rapporter de façon sûre la récompense RN-2(s,a) et l'aménera de manière aléatoire vers",
        "un nouvel état s' à l'étape N - 1. Là, il sait qu'en suivant la politique optimale TN-1 il pourra récupérer une récompense moyenne Vi(s). Le choix d'une action a à létape N - 2 conduit donc au mieux en moyenne à la somme de récompenses Rw-2(s,a) + Es PN-2(s,4)V7(6).",
        "Ainsi, le problème de l'agent à l'étape N = 2 se ramène simplement à rechercher l'action qui maximise cette somme, soit : TN-2(s) € argmaxdeA K-16ALmVBAA et V(s) - max RN-2(s,a) + X PN-2('s,4)V7(6). dEA Ce raisonnement peut s'étendre jusqu'à la première étape de décision, où l'on a donc :",
        "m(s) € argmaxaeA RZmia-0 et V(s) = max REmikaw.-0) dEA L'évaluation d'une politique à horizon fini se fait donc en partant de la fin, en résolvant des problèmes à un pas de temps, ce qui est à la base de la programmation dynamique. D'oû le",
        "Théorème 5.8.1 - - Equations d'optimalité pour le critère fini. Soit N < 00. Les fonctions de valeurs optimales V* = (VN Vi) sont les solutions uniques du système d'équations Vs ES V41(s) = max RW-1-ls,4)+2 PN-1-6,0,6), dEA",
        "avec n = J, N - 1 et Vo = 0. Les politiques optimales pour le critère fini T* = (o,,.. 1 sont alors déterminées par : Vs ES (s) € argmaxaeA W6ZmwM--0) pour t = 0, ,N-1.",
        "On voit donc ici dans le cadre du critère fini que les politiques optimales sont de type markovien déterministe, mais non stationnaire (le choix de la meilleure décision à prendre dépend de l'instantt). Evaluation d'une politique markovienne déterministe :",
        "Soit une politique T markovienne déterministe. La même démarche permet alors de caractériser sa fonction de valeu VA. 60 Chapitre 5. Processus de décision markoviens Théorème 5.8.2 = Caractérisation de VA. Soient N < 00 et T = (To, T1, ,TN-1) une po-",
        "litique markovienne. Alors VA = V avec V = (Vw,Vw-1--.,V) sont solutions du système d'équations linéaires VSES Vn+1(s) = RN-1-n(s, -1-0)+ZP--1-0)G, pour n = 0, N I et Vo = 0. 5.8.2 Cas du critère moyen On se limite ici au cadre des MDP récurrents (pour toute politique markovienne déterministe,",
        "la chaine de Markov correspondante est constituée d'une unique classe récurrente), unichaines (chaque chaine de Markov est constituée d'une unique classe récurrente plus éventuellement quelques états transitoires) ou multichaines (il existe au moins une politique dont la chaine de",
        "Markov correspondante soit constituée de deux classes récurrentes irréductibles ou plus). On suppose de plus ici que pour toute politique, la chaîne de Markov correspondante est apériodique. Evaluation d'une politique markovienne stationnaire : Soit T € DA une politique stationnaire et (S,P,R) le processus de Markov associé. Rap-",
        "pelons que le gain moyen ou critère moyen est défini par : N-1 Vs ES P\"(s) - lim E\" > Rx(S:)so=s N-4+oo M t-0 Sous forme matricielle, on a N-1 p lim Rr- N-+oo N t-0",
        "Soit P = limw-too N EN Pt la matrice limite de Pr. On montre que P existe et est une matrice stochastique pour tout S fini. De plus, Pr vérifie PPx = P. Le coefficient Prss peut être interprété comme la fraction de temps que le système passera dans",
        "l'état So en étant parti de l'état S. Pour des MDP apériodiques, on a de plus P = limw-too PN et Prss peut être interprété comme la probabilité à l'équilibre d'être dans l'état SO en étant parti de S.",
        "Enfin, pour un MDP unichaine, P est alors la matrice dont toutes les lignes sont identiques et égales à la mesure invariante Hz de la chaine contrôlée par la politique T. Ainsi Pas,s = Ha(s').",
        "De la définition précédente de p\" on déduit que p\" = PR. Pour un MDP unichaine on établit ainsi que p(s) = P est constant pour tout s, avec p =LH(S)Rx(s). SES Dans le cas général d'un MDP multichaine, p(s) est constant sur chaque classe de récurrence.",
        "Cette première caractérisation de p\" fait intervenir P qu'il n'est pas facile de calculer. II est toutefois possible d'obtenir autrement p\", en introduisant une nouvelle fonction de valeur pour le critère moyen, dite fonction de valeur relative : 5.9 Algorithmes de résolution des MDP 61",
        "Définition 5.8.1 = Fonction de valeur relative pour le critère moyen. VSES UG)=E\"(ER, - P\")so=s t-0 Théorème 5.8.3 Soit (S,P,R) un processus de Markov valué assicié à une politique T € DA.",
        "Alors Si p\" et UT sont le gain moyen et la fonction de valeur relative de T, on a 1. (I-Pr)p\"-0; 2. p\"+( I-Pr)U = Rr. On retiendra que la fonction de valeur relative U\" est l'unique solution de (I Pr)U= I P)R telle que PU = 0.",
        "Dans le cas simplifié d'un processus unichaine, la première équation se simplifie en p\"(s) p\"et la seconde peut s'écrire VSES U(s)+p = Rr(s) + EPssU(s) (1) S'ES Théorème 5.8.4 - Equations d'optimalité uniichaine. II existe une solution p,U au système d'équations définies pour tout S E S:",
        "P(s) = max E p('s,a)p(s), dEA SES U(s)+p=1 max R60+Epkaue) (2) dEA S'ES On a alors p = p*. 5.9 Algorithmes de résolution des MDP 1. Cas du critère fini : Le cas de l'horizon fini est assez simple. Les équations d'optimalité permettent en effet de calculer",
        "récursivement à partir de la dernière étape les fonctions de valeur optimales Vi, VN selon l'algorithme du premier théorème. 2. Cas du critère moyen : On présente ici deux principaux algorithmes de programmation dynamique pour calculer des politiques gain-optimales, dans le cas simplifié de MDP unichaines (toutes les politiques sont",
        "unichaines) pour lesquels le gain moyen p est constant. Le test d'arrêt est ici basé sur l'emploi de la semi-norme span sur Y: : WV € Y, span(V) - max V( (s) - min V (s). SES SES",
        "Contrairement à IIVII qui mesure l'écart de V à 0, la semi-norme span(V) mesure l'écart de V à un vecteur constant. Algorithme d'itération sur les valeurs relatives : C'est un algorithme d'itération sur les valeurs relatives U(s) = V(s)-p. 62 Chapitre 5. Processus de décision markoviens",
        "Algorithme d'itération sur les valeurs relatives - Critère moyen initialiser Uo € N choisir s* € S n +-0 répéter Pn+1 - maxacA R(s*, a +L-p6,QU.6)) pour S E S faire Un+1(s) = maxaeA R(s,a)+) Epl's,a)U.6)) - Pn+1 nt-n+1 jusqu'à span(Un+1 - Un) <E pour S € S faire",
        "T(s) € argmaxdeA A6LPVAL) retourner Pns Uns T. Sous différentes hypothèses techniques, on peut montrer sa convergence pour E 1 0 vers une solution (p*,V*) des équations d'optimalité (E) et donc vers une politique optimale T*. Algorithme modifié d'itération sur les politiques :",
        "L'algorithme ci-dessous est un algorithme modifié d'itération sur les politiques, qui ne né- cessite pas la résolution de l'équation (1) pour évaluer la fonction de valeur relative. Algorithme modifé d'itération sur les politiques - Critère moyen initialiser Vo € Y flag +-0 n +-0 répéter pour S € S faire",
        "T+1(s) € argmaxdeA £R(s,a) + LyPl1s,4)V.(6)) (T+1(s) = (s) si possible) VP(s) = 4DAHEACRaM m +-0 si span(V, Vn) < E alors flag +-1 sinon répéter pour S € S faire Vm+I(s) = (R(s,T+1(s)): + Erp0G) m t-m+1 jusqu'à span(m+l Vm) <8 Vn-1 + V n t-n+1",
        "jusqu'à flag = 1 retourner Vns Tn+1- Pour 8 élevé, l'algorithme est équivalent à l'itération sur les valeurs (non relative car on ne gère pas ici explicitement le revenu moyen Pn). Pour 8 proche de 0, on retrouve une itération sur",
        "les politiques classiques. Sous les mêmes conditions techniques précédentes, on montre que cet algorithme converge pour tout 8 vers une politique optimale pour E 1 0. Plus précisément, lorsque l'algorithme s' arrête, on a min(V,(s) Vn(s)) / 1n+1 / p\" / max Vn(s)), SES SES",
        "5.10 Etude de cas et Application Industrielle; cas d'un MDP 63 qui assure p-P'se. 5.10 Etude de cas et Application Industrielle; cas d'un MDP 5.10.1 Exemple de système de production Voir fichier joint \"Application indust. MDP\". Analyse du système de décision;",
        "Objectif du problème : déterminer une politique optimale relativement au côut moyen par unité de temps; Identification d'une politique déterministe optimale par résolution exhaustive; On rappelle que e Les états E de la chaine peuvent être partitionnés en classes d'équivalence appelées classes",
        "irréductibles. Si E est réduit à une seule classe, la chaîne de Markov est dite irréductible. Une classe d'équivalence C est dite fermée si, pour tout x, tels que x € Cet fx 1 y,",
        "y € C. Autrement dit, Vx € C, Vn € N, LyecP(s,y) = 1, c-à-d encore C est une classe dont on ne peut pas sortir. Une classe fermée réduite à un point C x est appelée un état absorbant. Un état X est absorbant ssi p(x,x) = 1.",
        "Si deux états communiquent alors ils ont même période. La période d'une classe est la période de chacun de ses éléments. Une classe est dite apériodique si sa période est 1. Exercices corrigés.",
        "Exercice 1. Considérons une chaine de Markov définie sur un espace E = f0, 9 formé de 10 états, dont la matrice de transition est : 64 Chapitre 5. Processus de décision markoviens 0.7 0 0 0.3 1 1 0 0 0 FIGURE 5.10.1 1. Tracer son graphe.",
        "2. Déterminer les classes de la chaine et leurs périodes. Solution : 0.3 0.7 1 1. FIGURE 5.10.2 2. On constate qu'il y a deux cycles donc les états de chacun des deux cycles communiquent. De plus,",
        "ces deux cycles contiennent le même état 0, ce qui implique qu'ils communiquent. II n'y a donc qu'une seule classe; la chaine est irréductible. Afin de déterminer la période de cette chaine de",
        "Markov, il suffit de déterminer celle d'un de ses états; prenons x = 0. Partant de 0 nous sommes de nouveau en 0 au bout de 4 transitions en passant par le petit cycle et au bout de 7 transitions en passant par le grand cycle, on a:",
        "pl4)(0,0) = 0.7 et pl4)(0,0) = 0.3. On en déduit que le PGCD est égal à 1;1 la chaine est donc apériodique. En résumé, cette chaine est formée d'une seule classe apériodique. Exercice 2. On reprend la chaine précédente avec un état de moins dans le second cycle.",
        "C'est-à-dire, considérons la chaine de Markov définie sur un espace E = f0, ,8 formé de 9 5.10 Etude de cas et Application Industrielle; cas d'un MDP 65 états, dont la matrice de transition est : 0.7 0 0 0.3 = FIGURE 5.10.3 1. Tracer son graphe.",
        "2. Déterminer les classes de la chaine et leurs périodes. Solution : 0.3 6 1 0.7 1 1. FIGURE 5.10.4 2. Comme précédemment cette chaine de Markov ne contient qu'une seule classe et est donc irréduc-",
        "tible. étudions la période en étudiant celle du sommet 0. Partant de 0, nous sommes de nouveau en 0 au bout de 4 transitions en passant par le petit cycle et au bout de 6 transitions en passant par le",
        "grand cycle. De plus, pour revenir en 0 on est obligé de passer par le petit ou par le grand cycle (\"ou\" non exclusif). Ainsi, nous avons p(0,0) > 0 ssi n = 4p + 6q pour (P,9) * (0,0). Donc",
        "pgcdin N 1p(0,0) > 0j = pgcd4p + 6ql(p,4) * (0,0)) = pgcd/4, 6j = 2. Ainsi la chaine est périodique de période 2. Exercice 3. Soit la chaine de Markov à 10 états E = f0, 9 de matrice de transition : 66 Chapitre 5. Processus de décision markoviens",
        "0 0 - . FIGURE 5.10.5 1. Tracer son graphe. 2. Déterminer les classes de la chaine et leurs périodes. Solution : 0 1 4 $ & 1 1 1 L 7 I 3 2 3 6 9 1 1. FIGURE 5.10.6",
        "5.10 Etude de cas et Application Industrielle; cas d'un MDP 67 2. Nous constatons l'existence de 4 classes : la classe f0) qui est fermée et réduite à un point, et forme donc un état absorbant 0;",
        "la classe £1,5) qui est fermée et apériodique en raison des boucles en 1 et 5; la classe [2, 3,6,7,9) qui est fermée et de période 3, les 3 sous-classes étant dans l'ordre(2y, £3,6) et (7,9;",
        "enfin la classe [4, 8), que nous pouvons quitter vers les classes f0) et £1,5) et qui n'est donc pas fermée. Nous pouvons voir que nous quittons la classe non-fermée [4, 8) vers les classes fermées f0) et",
        "£1,5). Or, nous voyons que nous quittons la classe [4, 8) de 4 ou de 8, et que nous avons toujours deux fois plus de chance de nous retrouver en 0 que dans la classe (1,5). Par suite la probabilité 2 2",
        "de se retrouver en 0 est de et en £1, 55 de 3 3"
    ],
    "summaries": [
        {
            "chunk_id": 1,
            "summary": "Une matire des matires de Mekns, Moulay Ismail"
        },
        {
            "chunk_id": 2,
            "summary": "Lois aléatoires discrtes 11 2.1.6 Lois discrtes usuelles"
        },
        {
            "chunk_id": 3,
            "summary": "Simulation de Variables aléatoires 19 3.1 Simulation de Variables aléatoires discr"
        },
        {
            "chunk_id": 4,
            "summary": "3.2.3 Simulation d'une loi normale : 20 3.3 Méthode de Monte Carlo pour le"
        },
        {
            "chunk_id": 5,
            "summary": "Processus stochastiques 23 4.1 Processus stochastiques 23 4.2 Processus de Wiener"
        },
        {
            "chunk_id": 6,
            "summary": "The following table summarises the main findings of the study:"
        },
        {
            "chunk_id": 7,
            "summary": "The following table shows the number of people who have died as a result of the conflict in eastern Ukraine."
        },
        {
            "chunk_id": 8,
            "summary": "Key words: Equivalence, Caractérisation, Cas du critre, Algorithm, MDP,"
        },
        {
            "chunk_id": 9,
            "summary": "Dans le cas d'un problme d'optimisation déterministe, les val"
        },
        {
            "chunk_id": 10,
            "summary": "L'incertitude de l'espace fondamental peut"
        },
        {
            "chunk_id": 11,
            "summary": "L'optimisation stochastique est un cadre qui permet de"
        },
        {
            "chunk_id": 12,
            "summary": "Dans un problme d'optimisation stochastique, on cherche souven"
        },
        {
            "chunk_id": 13,
            "summary": "Dans ce chapitre introductif nous sentons les notions basiques de "
        },
        {
            "chunk_id": 14,
            "summary": "Le nom d'ensemble d'un certain phéno- m"
        },
        {
            "chunk_id": 15,
            "summary": "L'ensemble 9P(2) s'agitre  l'exp"
        },
        {
            "chunk_id": 16,
            "summary": "Une tribu ou O-algbre bg sur 2 est un sous-"
        },
        {
            "chunk_id": 17,
            "summary": "Si (An)n est une suite de by alors UnAn  by (stabilité par"
        },
        {
            "chunk_id": 18,
            "summary": "RemarquONS qu'une tribu contient forcément 0."
        },
        {
            "chunk_id": 19,
            "summary": "Nous avons tre d'un éléments de probabilté lor"
        },
        {
            "chunk_id": 20,
            "summary": "Une sous-tribu s'il avait t"
        },
        {
            "chunk_id": 21,
            "summary": "Une partie quelconque de 9P(2) nous avons tre "
        },
        {
            "chunk_id": 22,
            "summary": "On appelle tribu des boréliens de R, on note 9B( (R), la tri"
        },
        {
            "chunk_id": 23,
            "summary": "Mesurabilité Définition 2.1.4 Soit (2,9F"
        },
        {
            "chunk_id": 24,
            "summary": "La mesurabilité d'une application est conservée chaque fois que l"
        },
        {
            "chunk_id": 25,
            "summary": "Une fonction de 9B(R) 9B(R)"
        },
        {
            "chunk_id": 26,
            "summary": "Une tte  l'occasion d'un deuxime fois"
        },
        {
            "chunk_id": 27,
            "summary": "Une variable aléatoire réelle (v.a.r.) X"
        },
        {
            "chunk_id": 28,
            "summary": "Une application mesurable de (,9) dans (R4,B(R4)"
        },
        {
            "chunk_id": 29,
            "summary": "La fonction indicatrice d'un ensemble A  by : 1 SIOEA 1A"
        },
        {
            "chunk_id": 30,
            "summary": "Une fonction constante de 2 1 R est mesurable pour toute tribu sur"
        },
        {
            "chunk_id": 31,
            "summary": "Une v.a.r. peu tre approchées en limite croissante simple"
        },
        {
            "chunk_id": 32,
            "summary": "La tribu engendée par une variable aléatoire"
        },
        {
            "chunk_id": 33,
            "summary": "Une mesure de probabilité ou simplement une probabilité sur (2, 9) est une"
        },
        {
            "chunk_id": 34,
            "summary": "P(2) = 1 e Si (An)meN est une suite d'éléments de by deux "
        },
        {
            "chunk_id": 35,
            "summary": "Voters in the UK go to the polls on 8 June to decide on whether or not to change the law"
        },
        {
            "chunk_id": 36,
            "summary": "Dans ce cas on a P(X = 1) = P(0) = 0; P(X)"
        },
        {
            "chunk_id": 37,
            "summary": "Une mesure de probabilité 2 sur ([0, [0, 1) telle que 2("
        },
        {
            "chunk_id": 38,
            "summary": "Une appelée mesure de Lebesgue sur [0,1,a "
        },
        {
            "chunk_id": 39,
            "summary": "Les ensembles  l'Institut National de la Recherche Scientifique (INR) s'"
        },
        {
            "chunk_id": 40,
            "summary": "The results of the first round of voting in the French presidential election, which was won by Emmanuel Macron"
        },
        {
            "chunk_id": 41,
            "summary": "El Formule de Bayes Avec les notations cédentes, si P(A) > 0 on"
        },
        {
            "chunk_id": 42,
            "summary": "Dans le cas d'une variable (ou d'un vecteur)"
        },
        {
            "chunk_id": 43,
            "summary": "La loi de X (sous P) est la probabilité Px sur(R"
        },
        {
            "chunk_id": 44,
            "summary": "La loi d'une v.a discrte X est la donne de toutes"
        },
        {
            "chunk_id": 45,
            "summary": "Dans ce cas VB  B(R), P(X"
        },
        {
            "chunk_id": 46,
            "summary": "Ont mme loi si X(21) = Y(22) et si pour tout"
        },
        {
            "chunk_id": 47,
            "summary": "Les lois de X et de Y sont marginales de X et de Y."
        },
        {
            "chunk_id": 48,
            "summary": "On appelle loi conditionnelle de Y sachant (X - x)"
        },
        {
            "chunk_id": 49,
            "summary": "Voters in France go to the polls on 8 May to decide on a constitutional amendment to"
        },
        {
            "chunk_id": 50,
            "summary": "L'espérance de X peut-on tre tre "
        },
        {
            "chunk_id": 51,
            "summary": "E(X) = L xP(X = xk)."
        },
        {
            "chunk_id": 52,
            "summary": "Var(X) = E[(x - E(X))2]= L (x"
        },
        {
            "chunk_id": 53,
            "summary": "Cov(X,Y)= E(X - E(X)Y-L E(Y"
        },
        {
            "chunk_id": 54,
            "summary": "Dans ce cas, la v.a."
        },
        {
            "chunk_id": 55,
            "summary": "C'est tre  l'arrivée et tre  l"
        },
        {
            "chunk_id": 56,
            "summary": "La loi binomiale de paramtres ont t  l'occasion d'"
        },
        {
            "chunk_id": 57,
            "summary": "La loi binomiale donne le nombre de succs qu'on peut ob"
        },
        {
            "chunk_id": 58,
            "summary": "A et Var(X) = 2."
        },
        {
            "chunk_id": 59,
            "summary": "Le nombre d'essais et le nombre d'événement"
        },
        {
            "chunk_id": 60,
            "summary": "Le nombre d'événements dans un intervalle de temps"
        },
        {
            "chunk_id": 61,
            "summary": "Une loi de Poisson de paramtre 2 = 60 X 5 ="
        },
        {
            "chunk_id": 62,
            "summary": "Le nombre de personnes réservant un billet d'avion pour la"
        },
        {
            "chunk_id": 63,
            "summary": "Une série de paramtre np, selon le Mecque, a"
        },
        {
            "chunk_id": 64,
            "summary": "Les clients arrivent une banque ou un limite de manimar, ce n'a pas "
        },
        {
            "chunk_id": 65,
            "summary": "Le nombre d'événements c--d d'arrivées c'"
        },
        {
            "chunk_id": 66,
            "summary": "La loi de spécifique par exp."
        },
        {
            "chunk_id": 67,
            "summary": "Le taux de réparation de moteurs de facon totalement alé"
        },
        {
            "chunk_id": 68,
            "summary": "Do not forget to ask your questions in the comment section below."
        },
        {
            "chunk_id": 69,
            "summary": "Le taux d'arrivées par heure, c--d Aheure, a"
        },
        {
            "chunk_id": 70,
            "summary": "Loi géométrique - (Apam)e Paucune arrivée par he"
        },
        {
            "chunk_id": 71,
            "summary": "La loi géométrique resente le nombre d'essais nécessaires"
        },
        {
            "chunk_id": 72,
            "summary": "On a e Une loi de Bernoulli de paramtre P est une loi"
        },
        {
            "chunk_id": 73,
            "summary": "La somme de deux v.a."
        },
        {
            "chunk_id": 74,
            "summary": "Caractérisation de paramtre p."
        },
        {
            "chunk_id": 75,
            "summary": "L'unique loi de probabilité discrte  perte de mém"
        },
        {
            "chunk_id": 76,
            "summary": "Cette propri est le plus souvent exprimée en termes de  temps d'"
        },
        {
            "chunk_id": 77,
            "summary": "Le temps d'attente du serveur de 9H  l'arrivé"
        },
        {
            "chunk_id": 78,
            "summary": "L'écoulement de ce délai arbitraire s'est arrivé d'"
        },
        {
            "chunk_id": 79,
            "summary": "L'arrivée d'un premier client a déj attendu  l"
        },
        {
            "chunk_id": 80,
            "summary": "C'est--l--t--t-"
        },
        {
            "chunk_id": 81,
            "summary": "Voters in France will go to the polls on 8 May to decide on a proposal to change the law to allow"
        },
        {
            "chunk_id": 82,
            "summary": "Une fonction positive d'intégrale 1 s"
        },
        {
            "chunk_id": 83,
            "summary": "Dans cependant, l'Institut National de la Recherche Scientifique (INRC)"
        },
        {
            "chunk_id": 84,
            "summary": "The coefficients of the coefficients of the coefficients of the coefficients of the coefficients of the coefficients of the coefficients"
        },
        {
            "chunk_id": 85,
            "summary": "La loi exponentielle est l'analogue dans le cas de la lo"
        },
        {
            "chunk_id": 86,
            "summary": "Si Xi et X2 sont deux v.a."
        },
        {
            "chunk_id": 87,
            "summary": "Fonction de répartition alors X1 + X2 - N (mi,o)?"
        },
        {
            "chunk_id": 88,
            "summary": "Proposition 2.1.8 On a Va, b  R, P(a"
        },
        {
            "chunk_id": 89,
            "summary": "La loi d'une v.a.r."
        },
        {
            "chunk_id": 90,
            "summary": "Proposition 2.1.9 Si (X,Y) est un couple de"
        },
        {
            "chunk_id": 91,
            "summary": "Proposition 2.110 SiX et Y sont deux V.a.r."
        },
        {
            "chunk_id": 92,
            "summary": "Soit X une variable aléatoire admettant pour densité f."
        },
        {
            "chunk_id": 93,
            "summary": "Proposition 2.1.11 Soient X et Y deux v.a"
        },
        {
            "chunk_id": 94,
            "summary": "The European Court of Human Rights (ECHR) has upheld a ruling by the European Court of Justice (EC"
        },
        {
            "chunk_id": 95,
            "summary": "Proposition 2.1.13 Soit X une v.a.r."
        },
        {
            "chunk_id": 96,
            "summary": "Pour tout n, on note X1 + + Xn X, n Alors pour tout E"
        },
        {
            "chunk_id": 97,
            "summary": "La suite (Xn)n2i avait  l'arrivée"
        },
        {
            "chunk_id": 98,
            "summary": "2.2.2 Théorme central limite : On rappelle ici l'importance"
        },
        {
            "chunk_id": 99,
            "summary": "L'Institut National de la Recherche Scientifique (INRC) a"
        },
        {
            "chunk_id": 100,
            "summary": "C'est--dire de la distribution de X, - m?"
        },
        {
            "chunk_id": 101,
            "summary": "Soit (Xn)neN- une suite de v.a.r."
        },
        {
            "chunk_id": 102,
            "summary": "Researchers at the University of California, Los Angeles (UCLA), have developed a new method"
        },
        {
            "chunk_id": 103,
            "summary": "Une suite de v.a. indépendantes de loi uniforme"
        },
        {
            "chunk_id": 104,
            "summary": "A partir de suite de v.a. indépendantes, on doit "
        },
        {
            "chunk_id": 105,
            "summary": "Simulation d'une v.a. binomiale de paramtres n > let E"
        },
        {
            "chunk_id": 106,
            "summary": "Cette simulation d'une V.a. discr"
        },
        {
            "chunk_id": 107,
            "summary": "Pk = P(X = xk), k  N, il suffit"
        },
        {
            "chunk_id": 108,
            "summary": "3.2 Simulation de Variables aléatoires  densité 3."
        },
        {
            "chunk_id": 109,
            "summary": "Une aléatoire de calculer F-1 du résultat obtenu."
        },
        {
            "chunk_id": 110,
            "summary": "3.2.2 Simulation d'une v.a."
        },
        {
            "chunk_id": 111,
            "summary": "Une mme mme loi  [0, 1 et [0,"
        },
        {
            "chunk_id": 112,
            "summary": "A partir de ce résultat, on construit la procé"
        },
        {
            "chunk_id": 113,
            "summary": "U et V  f(u) la réalisation de la simulation"
        },
        {
            "chunk_id": 114,
            "summary": "Une fonction de v.a."
        },
        {
            "chunk_id": 115,
            "summary": "La loi normale est un exemple de ce principe : Proposition 3."
        },
        {
            "chunk_id": 116,
            "summary": "Alors X et Y sont des V.a."
        },
        {
            "chunk_id": 117,
            "summary": "La méthode de Monte Carlo pour faire des calcul des intégrales"
        },
        {
            "chunk_id": 118,
            "summary": "Le principe de la méthode est alors de simuler ces v."
        },
        {
            "chunk_id": 119,
            "summary": "A l'on cherche  calculerériquement I pour une fonction"
        },
        {
            "chunk_id": 120,
            "summary": "Une formule de transfert s'écrit E(g)lpoa(u)du"
        },
        {
            "chunk_id": 121,
            "summary": "La loi des grand nombres affirme que 1 E(U) g(u"
        },
        {
            "chunk_id": 122,
            "summary": "Une approximation numérique (estimateur) de l'intégrale I est alor"
        },
        {
            "chunk_id": 123,
            "summary": "L'autre part, Var E8(U) Var(g"
        },
        {
            "chunk_id": 124,
            "summary": "Cette méthode de nombres aléatoires a"
        },
        {
            "chunk_id": 125,
            "summary": "En particulier, en grandes dimensions @ 3), elle est plus rapide et plus"
        },
        {
            "chunk_id": 126,
            "summary": "Une fonction  valeurs réelles 9 telle que P(X) est in"
        },
        {
            "chunk_id": 127,
            "summary": "Une tte  tte  Monte Carlo s'est "
        },
        {
            "chunk_id": 128,
            "summary": "Simulation de Variables aléatoires aléatoires 3.4 Méthode"
        },
        {
            "chunk_id": 129,
            "summary": "Une phénomnes complexe aléatoire s"
        },
        {
            "chunk_id": 130,
            "summary": "Une section cédente s'aprs d'aprs"
        },
        {
            "chunk_id": 131,
            "summary": "Une mme tte de mme mme t"
        },
        {
            "chunk_id": 132,
            "summary": "C'est calculer une distribution approchée de phé-"
        },
        {
            "chunk_id": 133,
            "summary": "Dans le cas d'une variable aléatoire X  densité, par"
        },
        {
            "chunk_id": 134,
            "summary": "Exercice (calcul de T par méthode de Monte Carlo) On cherche "
        },
        {
            "chunk_id": 135,
            "summary": "La méthode de Monte Carlo peut--tre tre tre"
        },
        {
            "chunk_id": 136,
            "summary": "Modles stochastiques 1 4.1 Processus stochastiques 4.1.1 Introduction"
        },
        {
            "chunk_id": 137,
            "summary": "Dans la réalité, partant d'un état initial, il y"
        },
        {
            "chunk_id": 138,
            "summary": "Un systme stochastique de manire probabiliste s"
        },
        {
            "chunk_id": 139,
            "summary": "Un modle stochastique d'un systme sto"
        },
        {
            "chunk_id": 140,
            "summary": "Processus stochastiques Définition 4.1.1 Soit (,9,P) un"
        },
        {
            "chunk_id": 141,
            "summary": "C'est tre un peu tre tre  l"
        },
        {
            "chunk_id": 142,
            "summary": "Si T n'est pas dénombrable, on dit que le processus est discret;"
        },
        {
            "chunk_id": 143,
            "summary": "Une périodique de marche aléatoire s"
        },
        {
            "chunk_id": 144,
            "summary": "A l'aprs-mme d'un arri"
        },
        {
            "chunk_id": 145,
            "summary": "C'est le seul paramtre libre du problme"
        },
        {
            "chunk_id": 146,
            "summary": "Le cas le plus simple (mouvement brownien), consiste  faire consid"
        },
        {
            "chunk_id": 147,
            "summary": "Une noton s'il s'il s'il s'il s"
        },
        {
            "chunk_id": 148,
            "summary": "Une chaque pas en avant (pile) et en retranchant (face) on a"
        },
        {
            "chunk_id": 149,
            "summary": "Dans ce cas, Xn = k si 2N, n - k, nécessaire"
        },
        {
            "chunk_id": 150,
            "summary": "Processus  accroissements stationnaires (X:)IER+,"
        },
        {
            "chunk_id": 151,
            "summary": "Une mouvement Wiener ou mouvement Wiener ou mouvement mme de"
        },
        {
            "chunk_id": 152,
            "summary": "Un processus (W:)IER+ réel est un processus de Wiener"
        },
        {
            "chunk_id": 153,
            "summary": "Le mouvement brownien est le nom donné aux trajectoires irrég"
        },
        {
            "chunk_id": 154,
            "summary": "Le nombre de \"tops\" et le nombre de \"tops\" avait tre"
        },
        {
            "chunk_id": 155,
            "summary": "Pour tout OKab, N(a) resente le nombre de se produ"
        },
        {
            "chunk_id": 156,
            "summary": "Si (TA)meN est une suite de v.a.r"
        },
        {
            "chunk_id": 157,
            "summary": "Un processus de Poisson de densité A  0"
        },
        {
            "chunk_id": 158,
            "summary": "Le temps de longueur t > 0 suit une loi de Poisson de"
        },
        {
            "chunk_id": 159,
            "summary": "Un processus de Poisson utilisés pour modéliser des files d'attente,"
        },
        {
            "chunk_id": 160,
            "summary": "Le passé de Markov, c--d Vti 12 In / 1, VA "
        },
        {
            "chunk_id": 161,
            "summary": "Un processus de Markov en temps discret est une suite (Xn)n de variables aléa"
        },
        {
            "chunk_id": 162,
            "summary": "Une chaine de Markov est un processus de Markov  temps discret ou "
        },
        {
            "chunk_id": 163,
            "summary": "Dans ce cas, la loi conditionnelle de Xn+1 sachant le passé s"
        },
        {
            "chunk_id": 164,
            "summary": "Une chaine de Markov peut tre vue un syst"
        },
        {
            "chunk_id": 165,
            "summary": "C'est un peu tre tre  l'"
        },
        {
            "chunk_id": 166,
            "summary": "Une chaine de Markov homogne peut--tre tre"
        },
        {
            "chunk_id": 167,
            "summary": "Une chaine de Markov est homogne si ses probabilités de transition"
        },
        {
            "chunk_id": 168,
            "summary": "Une chaine de Markov est homogne si Vn > 1, V(i,"
        },
        {
            "chunk_id": 169,
            "summary": "Alors X est une chaine de Markov homogne."
        },
        {
            "chunk_id": 170,
            "summary": "Dans la suite, toutes les chaines de Markov sont supposées homo"
        },
        {
            "chunk_id": 171,
            "summary": "On s'agit de modéliser une chaine de Markov  l'a"
        },
        {
            "chunk_id": 172,
            "summary": "Proposition 4.5.2 La matrice de transition P = (Pij)G.)EE2 est stocha"
        },
        {
            "chunk_id": 173,
            "summary": "La matrice d'une chaine de Markov est forcément stochastique et inverse"
        },
        {
            "chunk_id": 174,
            "summary": "C--d P(Xntk = jXn i P(X"
        },
        {
            "chunk_id": 175,
            "summary": "Si la grenouille arrive tout en haut, elle saute"
        },
        {
            "chunk_id": 176,
            "summary": "Si alors  l'instant n + 1 elle sera : au barreau i"
        },
        {
            "chunk_id": 177,
            "summary": "Une barreau s'exprime par P(Xn+1 =i+ 1X"
        },
        {
            "chunk_id": 178,
            "summary": "C'est le cas, la matrice de transition se traduit par :"
        },
        {
            "chunk_id": 179,
            "summary": "La dernire de la matrice est donc ( 1/2 0 0 1/2 0), l"
        },
        {
            "chunk_id": 180,
            "summary": "La premire de la matrice est alors que passer  l'état 1."
        },
        {
            "chunk_id": 181,
            "summary": "L'aide de la matrice de transition s'agitre un chaine de Markov"
        },
        {
            "chunk_id": 182,
            "summary": "La loi d'une chaine de Markov - La loi d'une chaine"
        },
        {
            "chunk_id": 183,
            "summary": "C'est selon Pioi Piiz Pin-in, rapporteur de la"
        },
        {
            "chunk_id": 184,
            "summary": "C'est un peu t--tre  l'"
        },
        {
            "chunk_id": 185,
            "summary": "Xi Jinping, president of the People's Republic of China, a déj"
        },
        {
            "chunk_id": 186,
            "summary": "Votes cast in the UK general election are now being counted."
        },
        {
            "chunk_id": 187,
            "summary": "La loi de Xn+k est don par par par Proposition 4.5.5."
        },
        {
            "chunk_id": 188,
            "summary": "P-D-LPX -Dp IEE En particulier, on a P-D"
        },
        {
            "chunk_id": 189,
            "summary": "Les formules de Chapman-kalmogorow, Exercice : Une chaine de Markov avec"
        },
        {
            "chunk_id": 190,
            "summary": "Exemple 1: Doudou le hamster Exemple 2: The Gardener Problem (Problme"
        },
        {
            "chunk_id": 191,
            "summary": "In the village of Marcth-tmroughsepember"
        },
        {
            "chunk_id": 192,
            "summary": "The state of the soil is one of the most important indicators of plant productivity."
        },
        {
            "chunk_id": 193,
            "summary": "The graph shows the state of the soil in a given year."
        },
        {
            "chunk_id": 194,
            "summary": "This table shows the likelihood of each year's soil condition improving or deteriorating over the next"
        },
        {
            "chunk_id": 195,
            "summary": "In this case, the gardener alters the transition probabilities by using organic fertilizer."
        },
        {
            "chunk_id": 196,
            "summary": "I y'a 5% de faible vers bon et 40% faible vers l'"
        },
        {
            "chunk_id": 197,
            "summary": "Exercice 1 : An engineering professor acquires a new computer once"
        },
        {
            "chunk_id": 198,
            "summary": "Next computer can be M2 with probability .25 or M3 with probability"
        },
        {
            "chunk_id": 199,
            "summary": "Exercice 1: The probability of buying M1 and M2 is .7 and"
        },
        {
            "chunk_id": 200,
            "summary": "All calls to the police are taken on a first-come, first-served basis"
        },
        {
            "chunk_id": 201,
            "summary": "The chances of a police car being called to the scene of an incident are:"
        },
        {
            "chunk_id": 202,
            "summary": "The chances that a police officer will make an arrest in the event of a"
        },
        {
            "chunk_id": 203,
            "summary": "The police patrol is in the middle of a crime scene."
        },
        {
            "chunk_id": 204,
            "summary": "Figure 4.5.3 The initial condition of the soil is good that is (0) - ("
        },
        {
            "chunk_id": 205,
            "summary": "probabilities of the three states of the system after 1, 8 and 16 gardening seasons."
        },
        {
            "chunk_id": 206,
            "summary": "The latest exchange rates between the British pound and the US dollar, as reported by the Bank of England."
        },
        {
            "chunk_id": 207,
            "summary": "ArtWindawe FIGURE 4.54 4.5 Markoves de Chain 31 ."
        },
        {
            "chunk_id": 208,
            "summary": "All figures are approximations and subject to change."
        },
        {
            "chunk_id": 209,
            "summary": "The coefficients of the coefficients of the coefficients of the coefficients of the coefficients of"
        },
        {
            "chunk_id": 210,
            "summary": "This paper presents a new classification of the Markov chain of Monte Carlo."
        },
        {
            "chunk_id": 211,
            "summary": "Les états d'une chaine de Markov peuvent "
        },
        {
            "chunk_id": 212,
            "summary": "Un état  un conduit i  un état j (ou qu'un "
        },
        {
            "chunk_id": 213,
            "summary": "On dit que iet j communiquent si j est accessible  partir de i et i accessible"
        },
        {
            "chunk_id": 214,
            "summary": "C'est--deux-Unis et C'est---deux-"
        },
        {
            "chunk_id": 215,
            "summary": "Une chaine X est irréductible s'il existe"
        },
        {
            "chunk_id": 216,
            "summary": "C'est tre tre un graphe orienté"
        },
        {
            "chunk_id": 217,
            "summary": "Les sommets de la chaine et o une arte resente resente"
        },
        {
            "chunk_id": 218,
            "summary": "Une classe C s'étend aux classes d'équivalence :"
        },
        {
            "chunk_id": 219,
            "summary": "Si C= fioj est fermée, io est dit "
        },
        {
            "chunk_id": 220,
            "summary": "E un état j est absorbant s'il retourne "
        },
        {
            "chunk_id": 221,
            "summary": "Partir d'un autre état, c-"
        },
        {
            "chunk_id": 222,
            "summary": "Cela peut se produire ssi l'état j n'"
        },
        {
            "chunk_id": 223,
            "summary": "Une période ont mme classe ont mme période"
        },
        {
            "chunk_id": 224,
            "summary": "Une chaine de Markov est irréductible et si tous les état"
        },
        {
            "chunk_id": 225,
            "summary": "Proposition 4.5.8 Une chaine de Markov irréductible sur un espace E fini"
        },
        {
            "chunk_id": 226,
            "summary": "Une fois ont tre atteints"
        },
        {
            "chunk_id": 227,
            "summary": "Dans un tel cas, chaque état forme un ensemble fermé."
        },
        {
            "chunk_id": 228,
            "summary": "States 1 and 2 are transient and states 3 and 4 are absorbing."
        },
        {
            "chunk_id": 229,
            "summary": "The probability of evering transient state 1 or 2 is zero, whereas the probability"
        },
        {
            "chunk_id": 230,
            "summary": "The periodicity of a state can be tested by computing P and observing the values of pl) for n =2,3,4"
        },
        {
            "chunk_id": 231,
            "summary": "BBC Sport takes a look at some of the key statistics behind England's World Cup win over"
        },
        {
            "chunk_id": 232,
            "summary": "The following graph shows the Markov chain of states:"
        },
        {
            "chunk_id": 233,
            "summary": "Chaine de Markov est réductible."
        },
        {
            "chunk_id": 234,
            "summary": "Soit le graphe 0,4 0,3 1 2 P = 4 0 0 0,1 0,"
        },
        {
            "chunk_id": 235,
            "summary": "Une classe finale : (3)."
        },
        {
            "chunk_id": 236,
            "summary": "Une périodique de période est irréductible."
        },
        {
            "chunk_id": 237,
            "summary": "The following table shows the results of a series of Markov chain Monte Carlo simulations."
        },
        {
            "chunk_id": 238,
            "summary": "Match reports from the French Open, which takes place on Sunday."
        },
        {
            "chunk_id": 239,
            "summary": "Une chaine de Markov fermée (irréductible) est dite ergodique"
        },
        {
            "chunk_id": 240,
            "summary": "Dans ce cas les probabiltés absolues, aprs n étapes"
        },
        {
            "chunk_id": 241,
            "summary": "Les problématiques de chaines de Markov homognes ont t t"
        },
        {
            "chunk_id": 242,
            "summary": "Une chaine de Markov, de matrice de transition P, est station"
        },
        {
            "chunk_id": 243,
            "summary": "Dans ce cas pour tout n, la loi de X, vérifie T"
        },
        {
            "chunk_id": 244,
            "summary": "Proposition 4.5.11 Soit (Xn)eN une chaine de Markov  éta"
        },
        {
            "chunk_id": 245,
            "summary": "Une chaine ergodique s'est tre un peu tre "
        },
        {
            "chunk_id": 246,
            "summary": "The steady-state probabilities are defined as \"i = lim a\""
        },
        {
            "chunk_id": 247,
            "summary": "The steady-state probabilities of a transition from one state to another are expressed as 1 = mP."
        },
        {
            "chunk_id": 248,
            "summary": "The mean first return time in a Markov chain is the number of transitions before the systems returns to a state j"
        },
        {
            "chunk_id": 249,
            "summary": "In this chapter, I will be looking at the steady-state probability distribution of the gardener problem with fertilizer."
        },
        {
            "chunk_id": 250,
            "summary": "The solution to this problem is given below."
        },
        {
            "chunk_id": 251,
            "summary": "The following table shows the probabilities of good, fair, and poor soil conditions over the course of a"
        },
        {
            "chunk_id": 252,
            "summary": "The amount of time it will take for a single garden to reach its full potential if it is"
        },
        {
            "chunk_id": 253,
            "summary": "The following table shows the results of a five-year study of soil moisture conditions in the"
        },
        {
            "chunk_id": 254,
            "summary": "The proposed fertilizer program should take into account the following:"
        },
        {
            "chunk_id": 255,
            "summary": "In this case, Ti =0.31, 72 = 0.58, and T3 = 0.11"
        },
        {
            "chunk_id": 256,
            "summary": "How much fertilizer do you need in your garden?"
        },
        {
            "chunk_id": 257,
            "summary": "What is the value of fertilizer used in a vegetable garden?"
        },
        {
            "chunk_id": 258,
            "summary": "The cost of fertilizer can be estimated using the following equation."
        },
        {
            "chunk_id": 259,
            "summary": "The effect of the use of fertilizer on the yield of a row crop has been investigated using a"
        },
        {
            "chunk_id": 260,
            "summary": "Le nombre de machines en panne au début de la n-i-me"
        },
        {
            "chunk_id": 261,
            "summary": "La nuit suivante et qu'on ne peut réparer qu'une machine dans la nu"
        },
        {
            "chunk_id": 262,
            "summary": "Une réparateur d'une machine en panne n'est réparée "
        },
        {
            "chunk_id": 263,
            "summary": "Une machine fonctionnent de 5 états par une chaine de Markov"
        },
        {
            "chunk_id": 264,
            "summary": "L'ensemble des états est E - f0, 1 car"
        },
        {
            "chunk_id": 265,
            "summary": "Le nombre de machines en panne le matin ne dépend que de celui de"
        },
        {
            "chunk_id": 266,
            "summary": "C'est s'il s'est passé dans la jour, ceci indépendam"
        },
        {
            "chunk_id": 267,
            "summary": "Une seule panne qui peut provenir d'une machine ou de l'autre"
        },
        {
            "chunk_id": 268,
            "summary": "The winning numbers in Saturday's drawing of the French lottery are:"
        },
        {
            "chunk_id": 269,
            "summary": "Dans ce cas, l'ensembles des états est E' - 0, 1,2"
        },
        {
            "chunk_id": 270,
            "summary": "The answer to the question: \"Which machine is the most powerful?\""
        },
        {
            "chunk_id": 271,
            "summary": "The winning numbers in Saturday's drawing of the French lottery are:"
        },
        {
            "chunk_id": 272,
            "summary": "Match reports from the World Twenty20 in India, where hosts India"
        },
        {
            "chunk_id": 273,
            "summary": "Si on garde l'ensemble des états E', on n"
        },
        {
            "chunk_id": 274,
            "summary": "La premire fois  panne : tout dépend si c'est le"
        },
        {
            "chunk_id": 275,
            "summary": "Dans ce cas, on a 00 2q(1 01 = p\"o2"
        },
        {
            "chunk_id": 276,
            "summary": "Les machines fonctionnent  calculer les puissances de la matrice de transition"
        },
        {
            "chunk_id": 277,
            "summary": "Le premier terme de la matrice de transition P\"2 s'est un"
        },
        {
            "chunk_id": 278,
            "summary": "Un processus de décision markovien (Markov decision process, ou MDP) est un process"
        },
        {
            "chunk_id": 279,
            "summary": "Un processus stochastique contrlé satisfaisant la propri de Markov"
        },
        {
            "chunk_id": 280,
            "summary": "Une fonction d'un agent sur l'environnement peut-on tre"
        },
        {
            "chunk_id": 281,
            "summary": "Cette fonction r peut tre définie de différentes"
        },
        {
            "chunk_id": 282,
            "summary": "Pour une action a fixée, P(s's,a) resente la"
        },
        {
            "chunk_id": 283,
            "summary": "Processus de décision markoviens i*, - % ps"
        },
        {
            "chunk_id": 284,
            "summary": "Dans leP resenté par la ci-dessus,  chaque"
        },
        {
            "chunk_id": 285,
            "summary": "Une général de l'Agriculture et Rural Development (MDP)"
        },
        {
            "chunk_id": 286,
            "summary": "Les valeurs de Ti peut-tre considées des gains et les"
        },
        {
            "chunk_id": 287,
            "summary": "La fonction de récompense r(s,a) vecteurs Ta de dimension"
        },
        {
            "chunk_id": 288,
            "summary": "C'est tre  l'arrivée d'"
        },
        {
            "chunk_id": 289,
            "summary": "On considre alors la valeurne F(s,a) = Es Pls,"
        },
        {
            "chunk_id": 290,
            "summary": "Figure 5.1.2 Actions aux effets incertains Go South (S) tat courant Action P"
        },
        {
            "chunk_id": 291,
            "summary": "Processus de décision markoviens Décision Une décision est un choix d'une action dans"
        },
        {
            "chunk_id": 292,
            "summary": "The French government's plan for the next five years:"
        },
        {
            "chunk_id": 293,
            "summary": "The latest official figures for the number of people living in poverty in France, as compiled by the"
        },
        {
            "chunk_id": 294,
            "summary": "C'est un ensemble de rgles if state then MzE MzE MzE MzE MzE MzE MzE MzE MzE MzE MzE"
        },
        {
            "chunk_id": 295,
            "summary": "Figure 5.1.6 Exécution d'un plan (politique) Notons T(s)"
        },
        {
            "chunk_id": 296,
            "summary": "L'étape 1 peut impliquer de la détection (sensing) et de la local"
        },
        {
            "chunk_id": 297,
            "summary": "Plan T1 (21,13)5 ((21,12) e W, (20,13) (20,13)-S"
        },
        {
            "chunk_id": 298,
            "summary": "The following table shows the number of users of Microsoft's Windows 10 operating system."
        },
        {
            "chunk_id": 299,
            "summary": "Les noeuds rouges resentent  partir de la transition  partir"
        },
        {
            "chunk_id": 300,
            "summary": "Une processus de processus de Décision Markovien  trois états et "
        },
        {
            "chunk_id": 301,
            "summary": "Processus de décision markoviens La matrice de transition associée  l'action"
        },
        {
            "chunk_id": 302,
            "summary": "On peroit une récompense de +5 lorsque l'on passe de l'"
        },
        {
            "chunk_id": 303,
            "summary": "The BBC News website has a series of flowcharts showing how drones are used to spy on"
        },
        {
            "chunk_id": 304,
            "summary": "Un agent implanté aunement de notre drone et dictant  ce dernier les décisions"
        },
        {
            "chunk_id": 305,
            "summary": "Les dons d'apprentissage des dons d'interaction s'"
        },
        {
            "chunk_id": 306,
            "summary": "Cette boucle d'interactions de série temporelle, composée de"
        },
        {
            "chunk_id": 307,
            "summary": "Le processus stochatique  contrler : Question : quels mod"
        },
        {
            "chunk_id": 308,
            "summary": "A l'arrivée de ces modles,  l'"
        },
        {
            "chunk_id": 309,
            "summary": "Un agent de l'étranger de l'Institut National de la Recherche Scientifique ("
        },
        {
            "chunk_id": 310,
            "summary": "Si l'observation contient toutes les dons sur lenement sur le syst"
        },
        {
            "chunk_id": 311,
            "summary": "Un systme doté d'une telle propri peut-on tre"
        },
        {
            "chunk_id": 312,
            "summary": "C'est un peu t--rm,    "
        },
        {
            "chunk_id": 313,
            "summary": "Un modle capable de décrire la dynamique d'un syst"
        },
        {
            "chunk_id": 314,
            "summary": "L'un des Etats-Unis de France, Jean-Yves Le Drian,"
        },
        {
            "chunk_id": 315,
            "summary": "C'est un tre d'un tre"
        },
        {
            "chunk_id": 316,
            "summary": "Un droite de celle-ci."
        },
        {
            "chunk_id": 317,
            "summary": "All photographs  AFP, EPA, Getty Images and Reuters"
        },
        {
            "chunk_id": 318,
            "summary": "Une gauche  gauche, haut, gauche alors laquence haut"
        },
        {
            "chunk_id": 319,
            "summary": "Une politique d'actions Une politique ou stratégie notée T ("
        },
        {
            "chunk_id": 320,
            "summary": "Une politique stochastique est une famille de distributions de probabilité selon"
        },
        {
            "chunk_id": 321,
            "summary": "Différentes familles de politiques pour les MDP."
        },
        {
            "chunk_id": 322,
            "summary": "Une politique stochastique est notée T = fm(a,st),t"
        },
        {
            "chunk_id": 323,
            "summary": "Ces quatre familles de politique définissent les quatre ensembles "
        },
        {
            "chunk_id": 324,
            "summary": "All figures are subject to change without notice."
        },
        {
            "chunk_id": 325,
            "summary": "Une politique est stationnaire si elle ne dépend pas du temps c-"
        },
        {
            "chunk_id": 326,
            "summary": "Politique markovienne et chaine de Markov valuée 55 ensemble D"
        },
        {
            "chunk_id": 327,
            "summary": "Une autre ensemble important, noté D4 est cons"
        },
        {
            "chunk_id": 328,
            "summary": "Politique markovienne et chaine de Markov valuée Un MDP et une politique markov"
        },
        {
            "chunk_id": 329,
            "summary": "Dans le cas o T - L (a,s)P(ss,a)"
        },
        {
            "chunk_id": 330,
            "summary": "Le triplet (S,Pa,R) appelé un processus de"
        },
        {
            "chunk_id": 331,
            "summary": "Une médicament s'agit d'une chaine de Markov avec des re"
        },
        {
            "chunk_id": 332,
            "summary": "Une traitement peut tre tre "
        },
        {
            "chunk_id": 333,
            "summary": "L'éléatoire de la maladie et le traitement n'est pas systéma"
        },
        {
            "chunk_id": 334,
            "summary": "Chaque jour, le sujet obtient une récompense réelle modélisant son ni"
        },
        {
            "chunk_id": 335,
            "summary": "Une politique markovienne avait tre un traitement "
        },
        {
            "chunk_id": 336,
            "summary": "L'état de santé suit alors une chaine de Markov de matrice de transition"
        },
        {
            "chunk_id": 337,
            "summary": "Une recherchercher de politiques celles qui optimisent un critre de"
        },
        {
            "chunk_id": 338,
            "summary": "Une politique peut-on tre tre"
        },
        {
            "chunk_id": 339,
            "summary": "Les crires de la théorie des MDP, qui sont tudi"
        },
        {
            "chunk_id": 340,
            "summary": "Les deux communes  ces 4 critres sont en effet d'une part"
        },
        {
            "chunk_id": 341,
            "summary": "C'est un choix d'un cumul espéré."
        },
        {
            "chunk_id": 342,
            "summary": "Une politique optimale sont des sous-politiques optimales"
        },
        {
            "chunk_id": 343,
            "summary": "Fonctions de valeur 57 5.6 Fonctions de valeur Les 4"
        },
        {
            "chunk_id": 344,
            "summary": "Une fonction de S dans R, identifiable  l'espace vectoriel"
        },
        {
            "chunk_id": 345,
            "summary": "Une politique optimale  l'objectif d'un problme décision"
        },
        {
            "chunk_id": 346,
            "summary": "Dans le cadre des MDP, on recherche donc des politiques optimales"
        },
        {
            "chunk_id": 347,
            "summary": "La spécificité des problmes décisionnels de Markov est a"
        },
        {
            "chunk_id": 348,
            "summary": "La fonction de valeur ( horizon fini) qui associe  tout éta"
        },
        {
            "chunk_id": 349,
            "summary": "La somme des N prochaines récompenses obtenues en suivant"
        },
        {
            "chunk_id": 350,
            "summary": "Notons-il est parfois utile d'ajouter une récompense terminale"
        },
        {
            "chunk_id": 351,
            "summary": "C'est le cas par exemple lorsqu'il s'agit de pilot"
        },
        {
            "chunk_id": 352,
            "summary": "Lorsque crire Lorsque la fréquence des décision"
        },
        {
            "chunk_id": 353,
            "summary": "Une politique particulire T et  un état S s'est"
        },
        {
            "chunk_id": 354,
            "summary": "L'Institut National de la Recherche Scientifique (INRP) avait"
        },
        {
            "chunk_id": 355,
            "summary": "Les politiques hisioire-dépendantes et markoviennes (MDP"
        },
        {
            "chunk_id": 356,
            "summary": "Une politique aléatoire histoire-dépendante (ITHS) s'"
        },
        {
            "chunk_id": 357,
            "summary": "Une politique histoire-dépendante aléatoire aléatoire "
        },
        {
            "chunk_id": 358,
            "summary": "Les politiques optimales peut-tre tre"
        },
        {
            "chunk_id": 359,
            "summary": "Une meilleure action  exécuter  argmaxae RN-1(s"
        },
        {
            "chunk_id": 360,
            "summary": "Une fonction de valeur optimale pour un horizon de longueur"
        },
        {
            "chunk_id": 361,
            "summary": "Lors de l'étape S  l'é"
        },
        {
            "chunk_id": 362,
            "summary": "L'étape N - 2 donc au mieux enne  la somme de récom"
        },
        {
            "chunk_id": 363,
            "summary": "La premire étape de décision  l'agent  l'étape N = 2"
        },
        {
            "chunk_id": 364,
            "summary": "L'évaluation d'une politique  horizon fini se fait donc en"
        },
        {
            "chunk_id": 365,
            "summary": "Les fonctions de valeurs optimales V* = (VN Vi)"
        },
        {
            "chunk_id": 366,
            "summary": "Les politiques optimales pour le critre fini T* ="
        },
        {
            "chunk_id": 367,
            "summary": "Une politique markovienne déterministe peut--"
        },
        {
            "chunk_id": 368,
            "summary": "La mme démarche permet alors de caractériser sa "
        },
        {
            "chunk_id": 369,
            "summary": "5.8.2 Cas du critre On se limite ici au cadre des MDP récurrents ("
        },
        {
            "chunk_id": 370,
            "summary": "La chaine de Markov est constituée d'une unique classe récurrent"
        },
        {
            "chunk_id": 371,
            "summary": "La chane de Markov correspondante s'est  l'auteur de l'"
        },
        {
            "chunk_id": 372,
            "summary": "Nos tte--ttes  l'"
        },
        {
            "chunk_id": 373,
            "summary": "Le coefficient Prss peut tre interpr dans le fraction de temps que le syst"
        },
        {
            "chunk_id": 374,
            "summary": "L'équilibre d'tre dans l'état SO"
        },
        {
            "chunk_id": 375,
            "summary": "C'est tre un tre d'un"
        },
        {
            "chunk_id": 376,
            "summary": "Dans le cas général d'un MDP multichaine, p(s) est"
        },
        {
            "chunk_id": 377,
            "summary": "Cette premire caractérisation de p\" intervenir P qu'il n'"
        },
        {
            "chunk_id": 378,
            "summary": "Un processus de Markov valué assicié  une"
        },
        {
            "chunk_id": 379,
            "summary": "La fonction de valeur relative U\" s'est un nouveau solution de (I Pr)"
        },
        {
            "chunk_id": 380,
            "summary": "Dans le cas simplifié d'un processus unichaine, la premire é"
        },
        {
            "chunk_id": 381,
            "summary": "Algorithm 5.9 MDes de résolution desP 1."
        },
        {
            "chunk_id": 382,
            "summary": "Une principaux algorithme de programmation dynamique pour calc des politiques gain"
        },
        {
            "chunk_id": 383,
            "summary": "C'est un nouveau test d'arrt  l'occasion de"
        },
        {
            "chunk_id": 384,
            "summary": "Algorithme d'itération sur les valeurs relatives : C'est un algorithme d'"
        },
        {
            "chunk_id": 385,
            "summary": "C'est un trs trs trs trs trs trs"
        },
        {
            "chunk_id": 386,
            "summary": "Une politique optimale T* (T(s)  argmax"
        },
        {
            "chunk_id": 387,
            "summary": "L'algorithme ci-dessous est un algorithme modifié d'itération sur"
        },
        {
            "chunk_id": 388,
            "summary": "T+1(s)  argmaxdeA £R(s,a) + Ly"
        },
        {
            "chunk_id": 389,
            "summary": "Pourquoi l'algorithme s'enfant  l'"
        },
        {
            "chunk_id": 390,
            "summary": "L'Institut National de la Recherche Scientifique (INRC) avait "
        },
        {
            "chunk_id": 391,
            "summary": "5.10 Etude de cas et Application Industrielle; cas d'un MDP 63"
        },
        {
            "chunk_id": 392,
            "summary": "C'est tre tre tre tre"
        },
        {
            "chunk_id": 393,
            "summary": "Une classe d'équivalence C est dite fer"
        },
        {
            "chunk_id": 394,
            "summary": "C  C  C  C  C  C  C  C"
        },
        {
            "chunk_id": 395,
            "summary": "La période de chacun de ses éléments a"
        },
        {
            "chunk_id": 396,
            "summary": "Images courtesy of AFP, EPA, Getty Images and Reuters"
        },
        {
            "chunk_id": 397,
            "summary": "The solution to this problem is given below:"
        },
        {
            "chunk_id": 398,
            "summary": "Une période de chaine de sre"
        },
        {
            "chunk_id": 399,
            "summary": "Markov, il suffit de déterminer celle d'un de ses états"
        },
        {
            "chunk_id": 400,
            "summary": "Une chaine périodique (PGCD) s'agit  l'"
        },
        {
            "chunk_id": 401,
            "summary": "C'est--dire, considéron la chaine de Markov définie"
        },
        {
            "chunk_id": 402,
            "summary": "The solution to this problem is presented in Figure 5.10.4."
        },
        {
            "chunk_id": 403,
            "summary": "Partant de 0, nous sommes de nouveau en 0 au bout de 4 transitions"
        },
        {
            "chunk_id": 404,
            "summary": "Pourquoi nous avons avons avons tre "
        },
        {
            "chunk_id": 405,
            "summary": "The following table shows the results of a study on the Markov chain:"
        },
        {
            "chunk_id": 406,
            "summary": "Solution : 0 1 4 $ & 1 1 1 L 7 I 3 2 3 6 9"
        },
        {
            "chunk_id": 407,
            "summary": "Etude de cas et Application Industrielle: cas d'un MDP 67"
        },
        {
            "chunk_id": 408,
            "summary": "The winners of this year's BBC Sports Personality of the Year awards have been"
        },
        {
            "chunk_id": 409,
            "summary": "Nous pouvons voir que nous quittons la classe non-"
        },
        {
            "chunk_id": 410,
            "summary": "Pourquoi nous avons toujours deux fois, nous avons touj"
        },
        {
            "chunk_id": 411,
            "summary": "BBC Sport takes a look back at some of the"
        }
    ]
}