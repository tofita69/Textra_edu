{
    "chunks": [
        "HAL open science Thermodynamics-based Artificial Neural Networks for constitutive modeling Filippo Masi, Ioannis Stefanou, Paolo Vannucci, Victor Maffi-Berthier To cite this version: Filippo Masi, Ioannis Stefanou, Paolo Vannucci, Victor Maffi-Berthier. Themmodynamic-based Ar- tificial Neural Networks for constitutive modeling. Journal of the Mechanics and Physics of Solids,",
        "2021, 147, pp.104277. 10,06/ApaDTT. hal-03079127 HAL Id: hal-03079127 https//halscience/la-03079127V1 Submitted on 17 Dec 2020 HAL is a multi-disciplinary open access L'archive ouverte pluridisciplinaire HAL, est archive for the deposit and dissemination of sci- destinée au dépôt et à la diffusion de documents",
        "entific research documents, whether they are pub- scientifiques de niveau recherche, publiés ou non, lished or not. The documents may come from émanant des établissements d'enseignement et de teaching and research institutions in France or recherche français ou étrangers, des laboratoires",
        "abroad, or from public or private research centers. publics ou privés. Thermodynamics-based Artificial Neural Networks for constitutive modeling Filippo Masia,b, Ioannis Stefanou**, Paolo Vannuccir, Victor Maffi-Berthierb aInstitut de Recherche en Génie Civil et Mécanique, UMR 6183, CNRS, Ecole Centrale de Nantes, Université de Nantes,",
        "1 rue de la Noe, F-44300, Nantes, France. bIngérop Conseil et Ingénierie, 18 rue des Deux Gares, F-92500, Rueil-Malmaison, France. PLMV, UMR 8100, Université de Versailles et Saint-Quentin, 55 avenue de Paris, F-78035, Versailles, France. Abstract Machine Learning methods and, in particular, Artificial Neural Networks (ANNs) have demonstrated",
        "promising capabilities in material constitutive modeling. One of the main drawbacks of such approaches is the lack of a rigorous frame based on the laws of physics. This may render physically inconsistent the predictions of a trained network, which can be even dangerous for real applications.",
        "Here we propose a new class of data-driven, physics-based, neural networks for constitutive modeling of strain rate independent processes at the material point level, which we define as Thermodynamic-based Artificial Neural Networks (TANNs). The two basic principles of thermo-",
        "dynamics are encoded in the network's architecture by taking advantage of automatic differentiation to compute the numerical derivatives of a network with respect to its inputs. In this way, derivatives of the free-energy, the dissipation rate and their relation with the stress and internal state variables",
        "are hardwired in the architecture of TANNS. Consequently, our approach does not have to identify the underlying pattern of thermodynamic laws during training, reducing the need of large data- sets. Moreover the training is more efficient and robust, and the predictions more accurate. Finally",
        "and more important, the predictions remain thermodynamicaly consistent, even for unseen data. Based on these features, TANNS are a starting point for data-driven, physics-based constitutive modeling with neural networks. We demonstrate the wide applicability of TANNS for modeling elasto-plastic materials, using",
        "both hyper- and hypo-plasticity models. Strain hardening and softening are also considered for the hyper-plastic scenario. Detailed comparisons show that the predictions of TANNS outperform those of standard ANNS. Finally, we demonstrate that the implementation of the laws of thermodynamics confers to TANNS",
        "high degrees of robustness to the presence of noise in the training data, compared to standard approaches. TANNS ' architecture is general, enabling applications to materials with different or more complex behavior, without any modification. Keywords: Data-driven modeling; Machine learning; Artificial neural network; Thermodynamics; Constitutive model.",
        "Masi, et al. (2020) preprint, Journal of the Mechanics and Physics of Solids. doi: 101016/mpa202A.0E7 1. Introduction A large spectrum of constitutive models have been proposed in the literature, based on observations and experimental testing. Existing constitutive laws can account for phenomena",
        "taking place at various length scales. This is achieved either through heuristic approaches and assumptions or through asymptotic approximations and averaging (e.g. Lloberas Valls et al., 2019; Nitka et al., 2011; Feyel, 2003; Bakhvalov and Panasenko, 1989). The history and",
        "the state of a material is commonly taken into account through ad hoc enrichment of simpler constitutive laws and extensive calibration. For this purpose, the laws of thermodynamics offer a useful framework for deriving more sophisticated laws, by intrinsically respecting the",
        "energy balance and the entropy production requirements (see e.g. Houlsby and Puzrin, 2000; Einav et al., 2007; Houlsby and Puzrin, 2007; Einav, 2012, among others). An important limitation in constitutive modeling is the availability of data at different",
        "time- and length-scales. However, with the increase of computational power, it is nowadays possible to foresee micromechanical simulations that can account for realistic physics and explore stress paths and non-linear phenomena, which are experimentally inaccessible with the current methods. Of course, some constitutive assumptions will be always necessary,",
        "but these might be at a smaller scale, where the material properties are measurable and easier to identify. This scale is for instance the scale of the microstructure of a material (e.g. the scale of sand grains, crystals, alloys' grains, composites' fibers, masonry bricks' etc. including their topological configuration).",
        "However, it is likely that the existing constitutive models might not be sufficient for describing complex material behaviors emerging from the microstructure. Therefore, calibration (parameter fitting) of known constitutive descriptors might be insufficient for representing the full space of material response, provided by sophisticated micromechanical simulations.",
        "Moreover, micromechanical simulations have currently a tremendous calculation cost, which is impossible to afford in large-scale, non-linear, incremental simulations (e.g. Finite Elements) that are usually needed in applications (cf. Masi et al., 2020, 2018; Rattez et al., 2018a,b;",
        "Collins-Craft et al., 2020; Lloberas Valls et al., 2019; Nitka et al., 2011; Eijnden et al., 2016; Feyel, 2003). A promising solution to this issue seems to be Machine Learning. According to Geron (2015), \"Machine Learning is the science (and art) of programming computers SO they can",
        "learn from data\" \" In the context of computer programming, learning is defined by Mitchell et al. (1997) as follows: \"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by",
        "P, improves with experience\". In the frame of constitutive modeling, a Machine Learning program can learn the stress-strain behavior of a material, given examples of stress-strain increments, which are either determined experimentally or through detailed micromechanical",
        "simulations. The data that the system uses to learn are called the training data-set and \"Corresponding author. Email addresses: lippo.masilec-mantes.fr (Filippo Masi), ioannis.stefanougec-nantes.fr (Ioannis Stefanou), paolovammuciduwsg.tr (Paolo Vannucci), victor.mafi-berthiertherdingerop.com (Victor Maffi-Berthier) 2 each training example is called a training instance (or sample). In our case, the task T, for",
        "instance, can be the prediction of the stress for a given strain increment and internal state of the material. The experience E is the training data-set and the performance measure P can be the prediction error. Machine Learning is a general term to describe a large spectrum",
        "of numerical methods. Some of them offer very rich interpolation spaces, which, in theory, could be used for approximating complicated functions belonging to uncommon spaces. Here we focus on the method of Artificial Neural Networks (ANNs), which is considered to be a",
        "sub-class of Machine Learning methods. According to Cybenko (1989) and Chen and Chen (1995), ANNs have proved to be universal approximators, due to their rich interpolation space. Therefore, they seem to be a useful and promising tool for approximating constitutive",
        "laws of many materials (e.g. sand, masonry, alloys, ceramics, composites etc.). Recognizing this potential, there is an increasing amount of new literature employing ANNS successfully in constitutive modeling of non-linear materials from model identification based on experiments and detailed numerical simulations. Starting form the seminal work",
        "of Ghaboussi et al. (1991) and without being exhaustive, we refer to Ghaboussi and Sidarta (1998); Lefik and Schrefler (2003); Jung and Ghaboussi (2006); Settgast et al. (2019); Liu and Wu (2019); Lu et al. (2019); Xu et al. (2020); Huang et al. (2020); Liu and Wu (2019);",
        "Gajek et al. (2020); Gorji et al. (2020) and references therein. The main idea in these works is to appropriately train ANNs, feeding them with material data, and predict the material response at the material point level. In this sense ANNs can be seen as rich interpolation",
        "spaces, able to represent complex material behavior. For instance, we record the works of Heider et al. (2020); Ghavamian and Simone (2019); Mozaffar et al. (2019); Frankel et al. (2019); Gonzalez et al. (2019); Gorji et al. (2020), who demonstrated that Recurrent Neural",
        "Networks (RNNs), an extension of neural networks, can be particularly useful for modeling path-dependent plasticity models. RNNs, differently from ANNS, process time sequences. As suggested by Gorji et al. (2020), the history-dependent variables of RNNs can potentially mimic the role of physical quantities.",
        "The Boundary Value Problem (BVP), set to determine the behavior of a solid under mechanical and/or multiphysics couplings, is then solved by replacing the standard constitutive equations or algorithms by the trained ANN. This replacement is straightforward and non-intrusive in",
        "Finite Element (FE) codes. We record, without being exhaustive, the successful embedding of ANNs as material description subroutines in FE codes by Lefik and Schrefler (2003); Jung and Ghaboussi (2006); Lefik et al. (2009); Settgast et al. (2019). Ghavamian and",
        "Simone (2019) further implemented ANNs in a FE2 scheme for accelerating multiscale FE simulations for materials displaying strain softening, with Perzyna viscoplaticity model. It is worth emphasizing that the aforementioned data-driven approaches are different from another promising data-driven method (i.e., data driven computing Kirchdoerfer and Ortiz,",
        "2016) in which the BVP is solved directly from experimental material data (measurements), bypassing the empirical material modeling step, involving the calibration of constitutive parameters (Kirchdoerfer and Ortiz, 2016; Ibanez et al., 2017; Kirchdoerfer and Ortiz, 2018,?; Ibanez et al., 2018; Eggersmann et al., 2019). While data-driven computing can",
        "be extremely powerful in many applications (Eggersmann et al., 2019), the first class of methods above-mentioned (based on the constitutive behavior at the material point level) 3 can be advantageous when modeling complex and abstract constitutive behaviors, which are",
        "not a priori known. Moreover, they can be used even if the BVP does not have a unique solution due to important non-linearities and bifurcation phenomena (e.g. loss of uniqueness, strain localization at the length of interest, multiphysics, runway instabilities etc.).",
        "Nevertheless, until now ANNs for constitutive modeling are mainly used as a 'black-box' mathematical operator, which once trained on available data-sets, does not embody the basic laws of thermodynamics. As a result, vast amount of high quality data (e.g. with reduced",
        "noise and free of outliers) are needed to enable ANNs to identify and learn the underlying thermodynamic laws. Moreover, nothing guarantees that the predictions of trained ANNs will be thermodynamically consistent, especially for unseen data. In this paper, we encode the two basic laws of thermodynamics in the architecture",
        "of neural networks. This assures thermodynamically consistent predictions, even for unseen data (which can exceed the range of training data-sets). Therefore, we assure thermodynamicaly consistent network's predictions, both for seen and unseen data (which can exceed the",
        "range of the training data-sets). Moreover, our network does not have to identify/learn the underlying pattern of thermodynamical laws. Consequently, smaller data-sets are needed in principle, the training is more efficient and the accuracy of the predictions higher. The",
        "price to pay, in comparison with existing approaches, is the need of two additional scalar functions (outputs) in the training data-set. These are the free-energy and the dissipation rate. However, these quantities are easily accessible in micromechanical simulations (e.g.",
        "Nitka et al., 2011; Eijnden et al., 2016; Feyel, 2003) and can also be obtained experimentally in some cases. Then, based on classical derivations in thermodynamics (e.g. Houlsby and Puzrin, 2007; Einav, 2012) specific interconnections are programmed inside our ANN architecture to impose the necessary thermodynamic restrictions. These thermodynamic",
        "restrictions concern the stresses and internal state variables and their relation with the free- energy and the dissipation rate. Our approach is inspired by the so-called Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019), in which reverse-mode autodiff (Baydin et al.,",
        "2017) is used, allowing the numerical calculation of the derivatives of an ANN with respect to its inputs. The calculation of these derivatives, imposes some numerical requirements regarding the mathematical class of the activation functions to be used. More specifically, the internal",
        "ANN restrictions, derived from the first law of thermodynamics, require activation functions whose second gradient does not vanish. Otherwise, the problem of second-order vanishing gradients, as it is called here (cf. classical vanishing gradients problem in ANNs, e.g. Geron,",
        "2015), can inhibit back-propagation and make training to fail. This new problem and its remedy is extensively explored and discussed herein. For the sake of simplicity and for distinguishing our approach from existing ones, we call the proposed ANN architecture Thermodynamics-based Artificial Neural Networks (TANNs).",
        "In our opinion TANNS should be the starting point for data-driven and physics-based constitutive modeling at the material point level. The paper is structured as follows. Section 2 presents a brief summary of the theoretical background of thermodynamics. In Section 3 an overview of the methodology proposed and",
        "architecture of TANNs is given. The main differences with classical, standard ANNs for 4 material constitutive modeling are also discussed. Particular attention is given to the choice of activation functions and the issue of second-order vanishing gradient is investigated in",
        "detail. Generation of material data-sets, with which the training of ANNs is performed, is presented in Section 4. In a first phase, we apply TANNs for the constitutive modeling of three-dimensional elasto-plastic material models, Section 5. In particular, we consider both hyper-plasticity models and smoother hypo-plasticity ones. Extensive comparisons with",
        "standard ANNs, which are not based on thermodynamics, are also presented. In a second phase, we investigate the performance and robustness of TANNS with the presence of noise in the training data, Section 6. This is achieved by generating a set of pseudo-experimental",
        "data, adding several levels of artificial noise. Supplementary figures and data are available in Supplementary data file. For the implementation of Artificial Neural Networks and Thermodynamic-based Artificial Neural Networks, we leverage Tensorflow v2.0. All code accompanying this manuscript is available upon request. 2. Thermodynamics principles: energy conservation and dissipation inequality",
        "2.1. Energy conservation A convenient way to express the (local) energy conservation is pè = a VSymy divq + ph, (1) with p being the material density; e the specific internal energy (per unit mass); a the",
        "Cauchy stress tensor; Vv the spatial velocity gradient tensor; 9 the rate of heat flux per unit area; h the specific energy source (supply) per unit mass, and \".\" denotes contraction of adjacent indices. 2.2. Second principle",
        "The second law of thermodynamics can be formulated in terms of the local Clausius- Duhem inequality ph 4 ps N div (2) 8 A with S being the specific (per unit mass) entropy; h/0 and -(q n)/0 the rate of entropy",
        "supply and flux, respectively. By removing the heat supply h between the energy equation (1) and the entropy inequality (2) leads to q VO p(es - é) + a VSymy N 0, (3) 8",
        "where the first two terms represent the rate of mechanical dissipation D = p(0s = è) + a . VSymy and the latter the thermal dissipation rate, i.e., Dth = 9-V8 The thermal dissipation is non-negative because heat only flows from regions of higher temperature to",
        "lower temperature-that is, the heat flux 9 is always in the direction of the negative thermal gradient. As it follows we argue that the mechanical dissipation rate must itself be non- negative (point-wise), i.e., D N 0. 5 2.3. Dissipation function",
        "The definition of the (mechanical) dissipation rate D leads to pé = pes + 0. VSymy - D. (4) Let us define the specific (per unit volume) internal energy E = pe and entropy S = ps and",
        "further assume constant material density, i.e., dp = 0-that is, É = pè and $ = ps. We shall assume a small strain regime, i.e., Vu A 1, with 8 : VSymu the small strain tensor, where",
        "u is the displacement vector field, and & : VSymy its rate of change. Equation (4) hence becomes É eS + a -D. (5) Let assume a strain-rate independent material such that the energy potential is E:=E(S,,Z), (6) and the dissipation rate, being a first-order homogeneous function of Z, is",
        "D:=D(S.6,Z.Z). (7) where Z = (Gis 5N) denotes a set of N (additional) internal state variables, Sis i = 1, N. We define here (thermodynamic) state variables those macroscopic quantities characterizing the state of a system, see e.g. Maugin and Muschik (1994). The physical",
        "representation of Si is not a priori prescribed. For instance, in the case of isotropic damage, 5 is a scalar; in anistotropic damage, a tensor; in the case of elasto-plasticity, a second order tensor, etc. The generalization to a finite-strain formulation can be achieved by",
        "considering the deformation gradient, F, and the first Piola-Kirchhoff tensor, P, as strain and stress measures, respectively (see e.g. Mariano and Galano, 2015; Anand et al., 2012). Nevertheless, as it would presented in Section 3, an incremental formulation of the material",
        "response is herein adopted. Therefore, the hypothesis of a small strain regime is usually realistic, at least for a large class of materials and an updated Lagrangian scheme. Time differentiation of the internal energy gives OE QE OE É = & + (8) as ds 1=1 05",
        "which is equal to (5) and, grouping terms, it leads to QE OE OE + + = (9) as dE as The arbitrariness of $, é, and 5 leads to the following relations OE 8 = (10a) as QE a (10b) ds N OE > 5i+D=0. (10c) i=1 a5; 6",
        "Further introducing the thermodynamic stress, conjugate to Si X = (X1, ,XN), with OE Xi : Vie[1,N], (11) asi we obtain the following, alternative definition of the dissipation D=2x5 (12) i=1 2.4. Isothermal processes In the case of isothermal process, the (specific) Helmholtz free-energy, F : E - Se =",
        "F(,,Z), which is the Legendre transform conjugate of e, is preferable. In this case, the dissipation rate is such that D : D(,,Z,2). The equations presented above (9-30) still hold (by replacing E with F) OF OF OF S = a X5 (13) 80 ds a5i",
        "3. Thermodynamic-based Artificial Neural Networks Within the framework of ANN or RNN material models, we can distinguish two main classes. The first consists of direct, so-called \"black-box\", approaches, where the information flow passes through the machine learning tool which operates as a mere regression operator,",
        "see e.g. Ghaboussi et al. (1991); Lefik and Schrefler (2003). The second class coincides with ANN and/or RNN models incorporating some knowledge in an informed, guided graph with intermediate history-dependent variables or detecting history-dependent features, see",
        "Heider et al. (2020); Mozaffar et al. (2019); Gorji et al. (2020), among others. Whilst the latter case has demonstrated to be extremely successful for path-dependent plasticity models, both classes are affected by the lack of physics, being the predictions not always",
        "compatible with thermodynamic principles (at least). Figure la depicts the direct approach AE Aa AE Aa T 0 Da I Qt AE (b) informed neural network (i- (c) informed neural network (i- (a) black-box (BB) network. NN1). NN2).",
        "Figure 1: Examples of direct, black-box (BB) (a) and informed (b, c) neural networks for material laws modeling. Inputs are highlighted in gray (0), outputs in black (0) 7 (BB), in which ANNs, usually Feed-Forward Neural Networks (FFNNs), are used to predict",
        "the stress increment, (output, 0) O = Ao = o+Ar a', from the input I = (E,AE), being g' the precedent strain and As its increment. In concise form, we write O = BB@ I. In this",
        "scheme, g' and As can be regarded as the state variables, namely the ANN state variables (not necessarily coinciding with those introduced in Sect. 2), on which the updated material stress depends on. Two examples of guided, informed ANNS, either FFNNS or RNNS, are",
        "illustrated in Figures 1b and lc. In both cases, the neural network intrinsically accounts for path-dependency, see e.g. Heider et al. (2020), making sequence of predictions of the main output. The network i-NN1 makes use of the last predicted output, i.e., a', to make",
        "predictions of the next output, O = Ao. The inputs are hence I = (E,AE, a'). We shall notice that, differently from BB, the stress at the precedent state, d', is also considered to be an ANN state variable. Other alternatives exist in the selection of the ANN variables of",
        "state. One may chose, as we shall see in Section 5, thermodynamic) state variables to be ANN state variables. In the case of temperature-dependent material response, the second case (1-NN2) allows to make predictions that depend on the precedent temperature state, 8, namely O = i-NN2@ I,",
        "with I = (E,AE, a',0) and O = (Ac, A8). The main aim of this work is to change the classical paradigm of data-driven ANN material modeling into physics-based ANN material modeling. We propose a new class of ANNs based on thermodynamics, which are Thermodynamic-based Artificial Neural",
        "Networks (TANNs). By exploiting the theoretical background presented in Section 2, we propose neural networks which, by definition, respect the thermodynamic principles, holding true for any class of material. In this framework, TANNS posses the special feature",
        "that the entire constitutive response of a material can be derived from definition of only two (pseudo-) potential functions: an energy potential and a dissipation pseudo-potential (Houlsby and Puzrin, 2007). TANNS are fed with thermodynamics \"information\" by relying on the automatic differentiation technique (Baydin et al., 2017) to differentiate neural",
        "networks outputs with respect to their inputs. This strategy allows to construct a general framework of neural networks material models which, in principle, can be exploited to predict the behavior of any material and assure that the predictions of TANNS will be",
        "thermodynamically consistent even for inputs that exceed the training range of data. In this paper, we only focus on strain-rate independent processes. Moreover, our approach can be extended, following the developments in Houlsby and Puzrin (2000), to materials showing viscosity and strain-rate dependency.",
        "The model relies on an incremental formulation and can be used in existing Finite Element formulations (among others), see e.g. Lefik and Schrefler (2003). Figure 2 illustrates the scheme of TANNS. The model inputs are the strain increment, the previous material state",
        "at time t, which is identified herein through the material stress, a', temperature, e, and the internal state variables, 5 as well as the time increment At, namely I = (,A6,0,0.6,A0). The primary outputs, O1, are internal variables increment, AKi, the temperature increment,",
        "AB, and the energy potential at time f+At, F'+Ar, i.e. Oi = (AKi, AB, FA). Secondary outputs, O2-that is, outputs computed by differentiation of the neural network with respect to the 8 inputs-are the stress increment, Ao, and the dissipation rate, D'Ar, 7 which we denote as",
        "O2 = VIOi = (Ao, D'+A). The class of neural networks we propose differs from the previous ones by the fact that the quantity of main interest, i.e., the stress increment, is obtained as a derived one, which",
        "intrinsically satisfies the first principle of thermodynamics (and, as we shall see, the second principle, as well). In the following, we briefly recall the basic concepts of artificial neural networks (paragraph 3.1), we then focus on the issue of the second-order vanishing gradients",
        "that may afflict the training and the performance of an ANN model (paragraph 3.2). In particular, it is shown that, in the framework of Thermodynamic-based Artificial Neural Networks, particular attention has to be paid to the selection of activation functions. Finally,",
        "we present in detail the architecture of our model (paragraph 3.3). At AE Aa 9 Figure 2: Schematic architecture of TANN. Inputs are highlighted in gray (C); outputs in black, (e) and and intermediate quantities in white (O). Dashed lines represent definitions, while arrows are used to",
        "denote neural network links. 3.1. Artificial neural networks overview We give herein a brief overview of the basic concepts of ANNs and in particular FFNNS. For more details, we refer to Hu and Hwang (2002) and Géron (2019). ANNs can be",
        "regarded as non-linear operators, composed of an assembly of mutually connected processing units-nodes-, which take an input signal I and return the output 0, namely O = ANN@I. (14) ANNs consist of at least three types of layers: input, output and hidden layers, with equal",
        "or different number of nodes. Figure 3 depicts a network composed of one hidden layer, with 3 nodes, an input layer with 2 inputs, and an output layer with 1 node. When an ANN has",
        "two or more hidden layers, it is called a deep neural network (Géron, 2019). Denoting the input array with I = (i), with t = 1,2...,n1 (nr is the number of inputs), and the outputs",
        "with O = 0;) with j = 1,2.. - ,no (no is the number of outputs), the signal flows from layer (1- 1) to layer (I) according to (I-1) PP A) Z) with k wip (-1) + (15) 9",
        "where PP are the outputs of node k, at layer (I); S is the activation function of layer (); ne\" is the number of neurons in layer (1 - 1); wp are the weights between the s-th node in",
        "layer (1-1 1) and the k-th node in layer (); and b) are the biases of layer (I). With reference to Figure 3, the output is given by 0= (0) z(o)) with z(o) = Zr w/2) (1) + b(2) p with Zr (1) Xt +",
        "where the activation function of the output layer, A(out), is a linear function, as in most of the cases for regression problems. The weights and biases of interconnections are adjusted, in an iterative procedure (gradient descent algorithm Géron, 2019), to minimize the error",
        "between the benchmark, 0, and prediction, O, that is measured by a loss function, L. In the following, the Mean (over a set of N samples) Absolute Error (MAE) is used as loss function, i.e., EAI 1Oi-O: L= (16) N",
        "where i = 1,2,...N. The errors related to each node of the output layer are hence back- propagated to the nodes in the hidden layers and used to calculate the gradients of the loss function, namely af a1-me aplm aL awkm aw/\" at-m) apl-m) I-m+I) (17) aL az-mD) ap-meD) aL",
        "apl-m) j=1 apl-m) atmD ap-mep which are then used to update weights and biases, and force the minimization of the loss function values, i.e. af wP-Rw = wp E (18) àw/p' where E is the so-called learning rate. The weights and biases updating, the so-called training",
        "process, is performed on a subset of the input-output data-set, defined as training set, known from experimental tests or numerical simulations of the phenomenon investigated. The ANN is trained. The training process is stopped as the loss function is below a specific tolerance.",
        "Then a test set, a subset of the input-output data-set different to the training set, is used to check the error of the network predictions. Once the ANN is trained, it is used in recall mode to obtain the output of the problem at hand.",
        "Due to their rich interpolation space, ANNs have proved to be universal approximators, see e.g. Cybenko (1989); Chen and Chen (1995), although the choice of hyper-parameters, such as the number of neurons, the network topology, the weights, etc. are problem-dependent.",
        "The same stands for the activation functions, which may be chosen to have some desirable properties of non-linearity, differentiation, monotonicity, etc. Most of these properties stem 10 P 1 output input layer layer hidden layer (1)",
        "Figure 3: Graph illustration of an ANN structure with two inputs, one output, and one hidden layer with three nodes. from issues related to the gradient descent algorithm and the so-called (first-order) vanishing gradient problem. As it follows, we briefly present this well-known issue and we further give",
        "insights in a variation of it: the second-order vanishing gradient. 3.2. First- and second-order vanishing gradients During the training process, if the gradient of the loss function with respect to a certain weight tends to zero-that is, see Eq. (18), when = ap? /az & 0 (with A' the first-",
        "derivative of the activation function with respect to its arguments)-the update operation can fail, and weights and biases are not updated. In this case, we have the so-called first-order vanishing gradient (Géron, 2019). Figure 4 displays some of the most common activation A() A() A() A() tanh(:) max(0,-) A(: 0)",
        "A(: 20) A'() A'() A'() A' (z) 5 cosk-() 05 max(0,1) A'(x<0)=e (20)=1 Figure 4: Some of the most common activation functions and their first-order gradient. From left to right: the logistic (sigmoid) function, the hyperbolic tangent, the Rectified Linear Unit (ReLU), and the Exponential Linear Unit (ELU).",
        "functions and their derivatives-that is, the logistic (sigmoid) function, the hyperbolic tangent, the Rectified Linear Unit (ReLU), and the Exponential Linear Unit (ELU). The sigmoid function is S-shaped, continuous, differentiable, its output values range from 0 to 1, and its",
        "first-order gradient (derivative) assumes values much smaller than 1. When inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close 11 to 0. Thus when backpropagation kicks in, it has virtually no gradient to propagate back",
        "through the network, which is problematic for training. The hyperbolic tangent activation function is very similar to the sigmoid, but it is centered at zero allowing to maintain the output values within a normalized range (between -1 and 1). Nevertheless, it suffers",
        "from saturated gradients (at Z = 0, for Z <K -1 and Z >> 1). ReLU is continuous but not differentiable at Z = 0. Nevertheless it is an unsaturated activation function for positive values of Z (its gradient has no maximum) and, therefore, it allows to avoid vanishing gradient",
        "issues for Z > 0. Nevertheless, it suffers from a problem known as the dying ReLUs: during training, some neurons are effectively deactivated, meaning they stop outputting anything other than 0 (for Z < 0). To this purpose many variants exist. The ELU activation, for",
        "instance, takes on negative values when Z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradient problem, as discussed earlier. Second, it has a nonzero gradient for Z < 0, which avoids the dying units issue. Finally, the",
        "function is smooth everywhere, including Z = 0, which helps speed up gradient descent. When dealing with TANNS, second-order vanishing gradients can appear. This is a new concept and, in order to illustrate it, we will use a simple example. Assume an ANN which",
        "takes as input some I = x and returns (a) O1 = x2 and (b) its derivative with respect to the input, i.e., O2 = VIOi = 2x (see Figure 5). Let us consider one hidden layer, with activation",
        "function A and N, nodes. The activation function of the single output layer, which returns 3?, is assumed to be linear. In this case, the output (a) is given by 01 =plo) = S(0) e)) 01 o) + b(o) (19) 01 + 50",
        "The derivatives of the outputs with respect to the inputs can be easily computed, in this simple example, by taking advantage of the automatic (numerical) differentiation (Baydin et al., 2017). Output (b) is hence computed by the ANN as a01 àp(o) az(o) ap azP 02 = V,Oi 5 aI",
        "àz(0) ap\" az\" aI (20) 801 aI Consider the following loss function L= WoLo + Wviolv,0, where Lo and Lv,o are the loss functions corresponding to output Oi and O2 = Vy01, respectively. Regularized weights, Wo and WVy can be used to obtain comparable order 12 2r A()",
        "Figure 5: ANN which takes as input x and returns (a) O1 = x2 and (b) its derivative with respect to the input, i.e., ViOi = 2x, with one hidden layer whose activation function is A. of magnitude of the two loss functions. During training, weights and biases are updated",
        "according to Eq. (18) where the computed gradients are aL AL + Ev,o (2la) awlo) aL = LVio (21b) aw aL L (21c) àb(o) aL - Ev,o (21d) ab\" It follows, from relations (21b) and (21d), that the gradient descent algorithm needs the",
        "computation of both first- and second-order gradients of the activation function A. This particular result is a direct consequence of the minimization of the error between the gradient of the outputs with respect to the inputs, i.e. O2 = V/01, and the corresponding benchmark",
        "values, 2x. This is what we call second-order vanishing gradient problem. It is tantamount to the first-order variant, but it involves the second derivatives (and not only the first) of the activation functions in an ANN. With reference to Figure 4, none of the depicted, classical",
        "activation functions is suitable for such class of problems. Consequently, care must be taken in selecting activation functions that do not have second-order vanishing gradients. To this purpose, Appendix A presents an example illustrating the issue of second-order vanishing gradients and proper solutions are given to this problem.",
        "3.3. Architecture of Thermodynamic-based Artificial Neural Networks Herein we detail the architecture and the internal steps/definitions TANNS are relying on. The architecture is detailed in Figure 6. The input vector is I = (,A6,0,0.4,A), the",
        "primary and secondary outputs are O = (AKi, AB, FHA) and VIo = (Ac, D'+A), respectively. TANN involves the following steps: 1. computation of the updated strain (definition): g'+Ar g' + As 2. prediction of the kinematic variables and temperature increments with two sub-ANNs: AE = SNN@ (W,AE,d,8.) and",
        "AB = SNNA@ (gAr A6,c,8,5) 13 3. computation of (a) the updated kinematic variables rates (backward finite difference approximation): +1 AL At (b) the updated kinematic variables (definition): $+1:=5+ AE (c) the updated temperature (definition): 0+1 e' + AB 4. prediction of the updated energy potential:",
        "FAr = SNNP@(g+Ar S'Ar 8'A) 5. computation of the updated dissipation rate (definition, Eq. (13)): D'+Ar : aFA agAr k+Ar 6. computation of (a) the updated stress (definition, Eq. (13)): o1+Ar aFAr ag'HAT (b) the stress increment (definition): Ao : o+Ar a At At t+At +A) AE tA 6+At At",
        "at 4 SNNF Aa AE SNN, Ot 1+At) SNNF Aa AB Q++A) SNNS SNN, $ (a) non-isothermal processes. (b) isothermal processes. Figure 6: Architecture of TANNS: general case (a) and for isothermal processes (b). Inputs are highlighted",
        "in gray (0); outputs in black, C for direct ANN predictions and for derived outputs; and intermediate quantities (definitions) are in white (0) and (0). Relationships obtained from definitions are represented with dashed lines, while arrows denote ANNS. TANNS are thus composed of three sub-ANNs; SNNy predicts the internal variables",
        "increment, SNNe predicts the temperature increment (note that in case of the isothermal conditions, this component can be removed from the architecture, see Fig. 6b), and SNNF predicts the Helmholtz free-energy. The main output, the increment in stress, is computed",
        "according to expression (13), which stems from thermodynamic requirements. By virtue of the fact that the entire constitutive response of a material can be derived from definition of only two pseudo-)potential functions, the model is able to predict the stress increment from",
        "the knowledge of the energy potential (and the internal variables 5i). It is worth noticing that, differently from common approaches (cf. Sect. 3), the sub-network SNNF is required to learn a scalar quantity-that is, the Helmholtz free-energy potential. This offer compelling",
        "advantages. When dealing with ANNS, the curse of dimensions (increasing effort in training 14 and large amount of training data required) is an important issue when the studied problem passes to higher dimensions, see e.g. Bessa et al. (2017). Passing from 1D to 3D, for instance,",
        "increases the number of variables the ANNs need to learn. For stresses, from one single scalar value, in 1D, we pass to a vector with six-components, in 3D. The computational effort is thus not trivial. Nevertheless, TANNS are, in principle, less affected by these issues as the",
        "two pseude-)potentials, on which the entire set of predictions relies on, are scalar functions. The computation of dissipation, from expression (13), plays a double role. First, it assures thermodynamic consistency of the predictions of TANNS (first law). Second, it brings the",
        "information to distinguish between reversible and irreversible processes, e.g. elasticity from plasticity/damage, etc., and it is trained to be positive or zero (second law). It is worth noticing that further improvements of the performance of TANNS may be",
        "obtained, as suggested in the work of Karpatne et al. (2017), by adding a physical inconsistency term to the loss functions (e.g., with respect to dissipation). 4. Generation of data We present the procedures used to generate material data TANNS are trained with",
        "in the following applications (see Sect. 5). We distinguish two different strategies. The first one, based on the numerical integration of an incremental form of the constitutive relations, is used to generate data for an hyper-plastic von Mises constitutive model with",
        "kinematic hardening (Houlsby and Puzrin, 2000, 2007). A different procedure is instead used to generate data for von Mises hypo-plasticity (Einav, 2012). In the case of hyper-plasticity models, we assume the Ziegler's orthogonality condition (see paragraph 4.1 and Ziegler, 2012; Houlsby and Puzrin, 2000, 2007), which, in general,",
        "it is not a strict requirement. Nevertheless, it is worth noticing that this restriction applies only on the generated data, and not on the ANN class here proposed. More precisely, TANN architecture still holds even for materials for which the Ziegler's normality condition does not",
        "apply. We shall recall that the aim is to demonstrate the advantages of thermodynamics- based neural networks with respect to classical approaches. Hence the restrictions, imposed by the orthogonality hypothesis for the generation of data, are expected not to affect the comparisons presented in Section 5.",
        "Hypo-plasticity is here used to show that the framework of thermodynamics encoded in TANNs is general and does not depend on restrictive assumptions such as the Ziegler's orthogonality condition afflicting hyper-plasticity. Furthermore, we consider the hypo-plastic material case to test TANNS against materials with a smooth response, which is more",
        "representative of realistic materials. 4.1. Incremental formulation 4.1.1. Hyper-plasticity Following the hyper-plasticity framework proposed in Einav et al. (2007), the thermo- mechanical, non-linear, incremental constitutive relation for strain-rate independent materials, 15 undergoing infinitesimal strains, is here derived in the framework of isothermal processes",
        "(0 = cost). By differentiating the energy expressions (13) and rearranging the terms, we obtain the following non-linear incremental relations d=0eF-ét ZaF-5 (22a) -Xi = OyeF.ê+ > Ou4F. (22b) where the following notation is adopted aF aF a?F OEsF = deF = OuAF = OEiyOEu dEiyask a4a5",
        "Further, introducing the thermodynamic dissipative stresses xt = (X1, .. XN) and assuming the Ziegler's orthogonality condition (Ziegler, 2012), the following non-linear, incremental constitutive relation can be found Mly-o . & ify=0 E = (23) Mlyso . & else with OesF 2k ELF.(9 y à axk dy OgeF ZKOLAF. e",
        "OEsF] aXk E = Mly-0 = CE and Mlyco = OdeF (24) 0 B Vg 0 B and . denotes the contraction of adjacent indices. In the above relations (23-24), whose derivation is presented in Appendix B, y = J(E, Z,X) is the yield function, 0 denotes a",
        "quantity (scalar or tensorial, depending on the dimensionality of the internal variable set) equal to zero, and dy dy CE OyeF, ds ax; i=1 y y dy B = OyeF i=1 a5i ax; i=1 aX; - aXk 4.1.2. Hypo-plasticity",
        "The theoretical framework used here to generate the hypo-plastic data can be found in Einav (2012). Einav (2012) proposed a new theoretical model, called hplastic, unifying hypo- and hyper-plasticity models. In particular, compared to standard hypo-plasticity, the incremental material formulation can be derived from (pseudo-) potentials. The h'plastic 16",
        "model allows ease integration of the incremental constitutive equations, i.e., Eq. (5.15a) in Einav (2012). Here we use the following incremental equations (Eq.s 7.14 and 7.15 in Einav, 2012) for the relaxation strain rate (z) and stress increment (d), according to von Mises model: a - z = (25a)",
        "2k2 k 2 5- - - à = Kép + 2G (25b) k 2 where k represents the elastic limit in simple shear; S is a material parameter (s > 0); K",
        "and G are, respectively, the bulk and shear moduli; Ep is the mean strain rate; à and z are, respectively, the deviatoric total and relaxation strain rate tensors; and a' is the deviatoric stress. 4.2. Data generation",
        "Data are generated in a Python environment, where SymPy and SciPy libraries are used for symbolic calculations and numerical integration. The accuracy of the generation process is 10-6 for strains and 10-4 MPa for stresses. For the case of hyer-plasticity, data are generated by identifying an initial state for the",
        "material at time t, state at time t : E' = and g', Si 0 and a given strain increment é, assuming constant and unitary time increment At = 1 (8 = Ag'). Numerical integration of the ordinary differential equations (23) is performed",
        "with an explicit solver (Bogacki and Shampine, 1989) to obtain the state at the new time t + At, i.e., o1+Ar -XHAr state at time t + At : E'+Ar = 4HAr 2+At For hypo-plasticity, data are generated similarly but only internal variables Si, deformation",
        "8, and stress a are used to represent the material state at time t and t+ At, through numerical resolution of Eq.s (25a) and (25b). The training data play a crucial role for both the accuracy of the predictions and",
        "the generalization with respect to the ANN state variables, e.g., strain increments. The generalization capability of a network is here defined as the ability to make predictions for loading paths different from those used in the training operation. Nevertheless, a significant 17",
        "dependency on the ANN state variables is usually observed. This may result in a poor network generalization. In Lefik and Schrefler (2003), an improvement of the generalization capability of ANNs is proposed. Artificial sub-sets of data, with zero strain increments,",
        "are added in the set of training data to force the network in learning that to zero input increments correspond zero output increments. In the available literature, strain-stress loading paths are commonly used in training. If recursive neural networks are used, feeding them with history variables (loading paths) is",
        "the only possible solution (see e.g. Mozaffar et al., 2019). Nevertheless, ANNs do not necessary need the data-sets to be (historical) paths. Herein, we generate data randomly. Conversely, this allows us to (1) improve the representativeness: of the material data and (2) improve the generalization of the network on",
        "the strain increments. For the hyper-plastic material model, the initial state, E and g, and the strain increment, AE, are randomly generated from standard distributions with mean value equal to zero and standard deviation equal to Emax, Emaxi and AEmax respectively. The",
        "Cauchy and thermodynamic stresses, a' and X, as well as the internal variables 5i are then calculated to satisfy the constraint y' S 0. This incremental procedure is repeated for Nsamples, resulting in a set of Nsamples ordered pairs (E,8,Ae; EA), from which the corresponding",
        "energy potential and dissipation rate at time t + At are evaluated. For the case of hypo- plasticity, data are generated by random loading paths as the procedure aforementioned for hyper-plasticity is not applicable to the theoretical framework in Einav (2012), as no",
        "definition of yield surface is needed for the derivation of the incremental material constitutive law. Figure 7 depicts the sampling for one of the investigated applications (see paragraph 5.1). 18 1500 - Perain 100001 = derain 1250 - Fval 8000 - dval 1000 D Perain D dtrain 6000 750 J",
        "Poa D 4vr J 500 4000 250 2000 0 0 -3000-2000-1000 0 1000 2000 3000 0 25 50 75 100 125 150 175 200 p.pAr (MPa) q.q\"ar (MPa) 800 - Strain Arain 600- - Sval Kval 600 D Str 400 D Arain J 400 p.val J Va 200 200 0",
        "-0.02 -0.01 0.00 0.01 0.02 0.00 0.01 0.02 0.03 0.04 6,.6,a () d,ear (-) 800 = Fptrain - rain 600- - fpval - Eval 600 J D FAte ain 400- - rain 400 E J fpval D LVay 200 200 0- -0.02 -0.01 0.00 0.01 0.02",
        "0.00 0.01 0.02 0.03 0.04 5-4ar () z',\"A(-) - Flrain 4000 - Dn 6000 - FVy 3000 DVy 4000 € 2000 2000 1000 20 25 35 0.0 0.5 1.0 1.5 2.0 2.5 prar (N-mm) DAr (N-mm/s)",
        "Figure 7: Sampling for material case H-1 (cf. Table 1). From top to bottom: mean and deviatoric stress (p and q); mean and deviatoric total deformation (Ep and e); mean and deviatoric plastic deformation (Sp and z); energy (F) and dissipation rate (D). Training and validation data-sets are also distinguished.",
        "19 5. Applications Herein we use TANNS to the modeling of multi-dimensional elasto-plastic materials and demonstrate their wide applicability and effectiveness. It is worth noticing that, even though the applications here investigated consist of elasto-plastic materials, the proposed class of",
        "neural networks can be successfully applied (without any modification) to materials with different or more complex behavior, accounting e.g. for damage and/or other non-linearities (in the framework of strain-rate independent processes). In paragraph 5.1, von Mises hyper- plasticity is accounted for, considering perfect-plasticity, hardening and softening behaviors.",
        "In paragraph 5.1, we further investigate hypo-plastic material models. In the examples presented herein, reference dependent variables, such as the total plastic strain, were considered. However, the internal state variables set Z, see Eq. (7), and, consequently, our approach are not limited to this kind of state variables.",
        "As it follows, the hyper-parameters (i.e., number of hidden layers, neurons, activation functions, etc.) of the networks are selected to give the best predictions, while requiring minimum number of hidden layers and nodes per layer. This is accomplished by comparing",
        "the learning error on the set of test patterns, per each trial choice of the hyper-parameters. In each training process, we use early-stopping. In other words, training is stopped as the error of a validation set starts to increase while the learning error still decreases (Géron,",
        "2019). The validation set is used to avoid over-fitting of the training data. Throughout this Section relatively simple deep feed-forward neural networks architectures are used (with, at maximum, two hidden layers) and no additional regularization techniques are employed (e.g., L1/L2 penalties, dropout, etc.). Each numerical example is accompanied",
        "with a detailed discussion about the network architecture. 5.1. von Mises hyper-plasticity In order to illustrate the performance of TANNs, we use the simple von Mises elasto- plastic model with kinematic hardening and softening. The model can be derived from the following expressions of the energy potential and dissipation rate",
        "9K F= p)-(Ep-5p)+ 2 H + G(e- ) (e + Z, 2 D=kV2 Vz-:, where k represents the elastic limit in simple shear; K and G are the bulk and shear moduli; H the hardening (softening) parameter; Ep and 5p are, respectively, the mean total and",
        "plastic deformation; and e and Z are, respectively, the total and plastic deviatoric strain tensors. The yield surface can be derived as shown in Appendix B (Houlsby and Puzrin, 2007) and is defined as y=D-X'-z= VX'.X - V2k S 0, (26) with Xij = 2G eij Zij) + Hzij- 20",
        "Table 1: Material parameters for 3D elasto-plastic von Mises material. case K G k H (GPa) (GPa) (MPa) (GPa) H-1 167 77 140 0 H-2 167 77 140 -10 H-3 167 77 140 10 5.1.1. Training",
        "Data are generated as detailed in Section 4. A total of 6000 data with random increments of deformation are generated. In order to improve the performance of the network in recall mode, additional sampling with random uni-axial and bi-axial loading paths are also used.",
        "The samples are split into training (50%), validation (25%), and test (25%) sets. The sampling in terms of the mean and deviatoric stresses, P and 4, and deformations, Ep and e, is presented in Figure 7 for material case H-1 (perfect plasticity). We distinguish between",
        "training and validation sets. For the sake of simplicity, stress and deformation are converted in the principal axes frame of reference. Table 2 shows the mean, standard deviation, and maximum values of the training data-sets. Adam optimizer with Nesterov's acceleration",
        "gradient (Dozat, 2016) is selected and a batch size of 10 samples is used. Data are normalized between -1 and 1. We use the Mean Absolute Error (MAE), and not the Mean Square Error (MSE), as loss",
        "function for each output in order to assure the same precision between data of low and high numerical values. Regularized weights are used to have consistent order of magnitude of different quantities involved in the loss functions.",
        "The network architecture is adapted to the size of the inputs and outputs, with respect to the mono-dimensional case. In particular, the sub-network SNN, consists of two hidden layers, with 48 neurons (leaky ReLU activation function), and three output layers, one per each",
        "(principal) component of (increment of) 5. The sub-network S-NNF has one hidden layer with 36 neurons (activation ELU2). The output layers for both sub-networks have linear activation functions and biases set to zero. The resulting number of hyper-parameters is",
        "R 3000 (cf. Ghaboussi and Sidarta, 1998; ?; Lefik et al., 2009; Mozaffar et al., 2019).. Figure 8 displays the loss functions of each output as the training is performed, for material case H-1 (perfect plasticity). The early stopping rule assures convergence, after approximately",
        "1000 epochs, with MAEs of the same order of magnitude for the 4 outputs, AK, F'+Ar, 1 Ac, and D'+A The adimensional MAE is approximately equal to 1 X 10-4 for all outputs at the end of the training. Similar behaviors are also recovered for cases H-2 (softening), H-3 (hardening).",
        "5.1.2. Predictions in recall mode Once the network has been trained, it is used, in recall mode, to make predictions. We briefly present the performance of TANNs in predicting the material response for a random loading path. Figure 9 depicts the comparison with the target material model for material",
        "case H-1. The network displays extremely good performance and the ability to predict 21 Table 2: Mean (y), standard deviation (st), and maximum values of the training data-sets. data H st max Ei () 8x 10-5 0.010 0.041 Asi () 2x 10-5 0.003 0.014 5i () 8x 10-5 0.010 0.041",
        "AG () 0 1x: 10-4 0.0011 ai (MPa) -1.4 143 544 Ac, (MPa) 123 7577 36744 F'Ar (N-mm) 1.82 2.72 37.9 D'+Ar (N-mm/s) 0.41 0.38 2.61 le-1 Afitrain Afival le-2 Ftrain Fval - le-3 Aditrain Adival le-4 Dtrain Dval 10 100 1000 epochs",
        "Figure 8: Errors in terms of the adimensional Mean Absolute Error (MAE) of the predictions of TANN (loss functions), as the training is being performed, evaluated with respect to the training (train) and validation (val) sets. Weights and biases update are computed only on the training set. random loading path.",
        "5.1.3. TANN VS standard ANN. Generalization of the network Herein we investigate the performance of TANNS with respect to the classical approach of ANNs (Ghaboussi et al., 1991; Lefik and Schrefler, 2003), as well as the sensitivity with",
        "respect to the input variables range. Figure 10 displays the architecture of the network, ANN, with inputs I = (E, AEi,C, 5) and output O = (ALi, Aci), with i = 1,2,3 denoting the principal components. The architecture is selected to give the best performance, preserving",
        "the same number of hyper-parameters between TANN and standard ANN. The network, ANN, consists of the two sub-networks, aNNy and aNNa, with two hidden layers, each one, leaky ReLU activation functions, and number of neurons per layer equal to 48. As for SNNy",
        "and SNNG, in aNNy and aNN three output layers (1 neuron each) are used, with linear activation functions and zero biases. In Figure 11 we present the comparison of the MAE of the network predictions with respect to the target values (training and validation data-sets).",
        "It is worth emphasizing that both ANNs and TANNs are dependent on the choice of the user, concerning, for instance, the hyper-parameters. Moreover, the actual configurations of both networks may benefit of ltemative/extensions, such as RNNs. Nevertheless, the 22 le-2 0.5 0.0 3 A a -0.5 -1.0 -1.5",
        "20 40 60 80 Increments () (a) random loading path. le3 5.0 le-3 model model TANN 2.5 TANN € 0 0.0 -1 a 6 G-5.0 -2 -7.5 -3 -1.5 -1.0 -0.5 0.0 0.5 -1.5 -10 -0.5 0.0 0.5 E1 () le-2 61 (-) le-2 (b) stress and internal variable. lel",
        "1.5 model TANN 8. model TANN 6 1.0 0.5. A .0 5p (-) le-3 Luky () le-3 (c) energy and dissipation rate. Figure 9: Predictions of TANN for a uni-axial random loading path, compared with the target constitutive",
        "model, case H-1, perfect plasticity: (a) loading path; (b) principal stress, 01, and internal variable, 51, predictions; (c) energy and dissipation rate predictions. following comparisons show the added value of our approach compared to standard ones that do not explicitly contain physics, as TANNS.",
        "We first compare the performance of both networks, TANNS and standard ANNS, in predicting the material response for cyclic isotropic loading paths (material case H-1, cf. Table 1). A linear elastic material response is expected and retrieved. Figure 12 displays the",
        "stress predictions of TANNS and ANNs, compared with the target values, for different strain increments. It is worth mentioning that the standard approach of ANNs does not succeed in accurately predicting the elastic deformation range. Moreover, contrary to TANNS, the",
        "stress predictions of standard ANNS, depend strongly on the cyclic loading. As the network is used recursively, in recall mode, the stress predictions rapidly become less and less precise, due to error accumulation. The performance of both networks is further compared for the following tri-axial loading 23 1+At Ae Ae",
        "Ac aNN, Aa a' ANN. +At $ - (a) ANN scheme. (b) ANN architecture. Figure 10: Schematic (a) and full architecture (b) of the network, not based on thermodynamics, standard ANNS. Inputs are highlighted in gray (C), outputs in black (e) le-1 le-1 TANN Ak,train TANN Abival ANN Aftrain le-2",
        "le-2 ANN Abival 14 le-3 TANN Acitrain le-34 TANN Adiyal ANN Acitrain le-4 ANN Adival 10 100 1000 10 100 1000 epochs epochs (a) mean absolute error of AE prediction (b) mean absolute error of Aci prediction",
        "Figure 11: Training of ANNs compared with TANNS evaluated with respect to the training (train) and validation (val) sets. path nT ni Agi = As sgn cos AE2 - AE3 = As sgn sin (27) 2N 2N",
        "with N = Emax/Ae, Emax = 2 X 10-3 + 1, and As = 1 X 10-5 + 1 X 10-1. Figures 13 and 14 display the material response in terms of the principal stresses, 01 and",
        "03, and inelastic strains, 51 and 53, respectively. We show in Figure 15 the energy and dissipation rate predicted by TANNS with those computed, with standard ANNs, directly using the corresponding definitions for the free-energy and dissipation rate, Eq. (5.1). The",
        "predictions of TANNS are in good agreement with the constitutive model, independently from the strain increment, which exceeds considerably the training range. Nevertheless, the performance of ANNs is found to be strongly affected by the values of As. For strain",
        "increments well inside the training range, i.e., As = 1 X 10-3, standard ANNs are well predict the material response. In particular, computing, through Eq. (5.1, the dissipation rate and energy from the ANNs' predictions reveals that ANNs can successfully predict",
        "output respecting the requirements of the thermodynamics. The first and second principles of thermodynamics are indirectly learned during training (on thermodynamic consistent data). However, standard ANNs perform poorly for strain increments smaller and larger",
        "than the ones at which it was trained (As = 1 X 103, cf. Table 2). And in these cases, 24 standard ANNs predict thermodynamically inconsistent outputs. The predictions of TANNS are, instead, always thermodynamically consistent. Moreover,",
        "the quantities of primary interest, such as the stress, the internal state variable, and the energy are in extremely good agreement with the reference model. The same stands also for the dissipation rate. We notice, once more, that its values are always positive, even when",
        "the network is used for predictions beyond the training range. Figure $3 displays the predictions for very small strain increments, i.e. As = 1x10-5. TANNS successfully still predict the response in this limiting case, while ANNs do not. Indeed, the",
        "training data were generated guaranteeing an accuracy of the order of 10-6 in terms of strains and such small strain increments are at the margin of the computing precision. In the Supplementary Material, we present the results of a uni-axial loading scenario,",
        "in Figures S1 and S2, for material case H-1 (perfect plasticity). Kinematic hardening and softening material cases and the predictions of TANNS and ANNs are shown in Figures $4-S9. It is worth noticing that in all the cases, even for very large strain increments-for which",
        "the predictions of the network in terms of dissipation rate differ from the target values-, TANNS successfully predict the Jacobian, i.e., aci dej (i,j = 1,2,3), in very good agreement with the reference model. This is of particular importance for numerical simulations with implicit",
        "algorithms. Therefore, TANNS can successfully replace complicated constitutive models or multiscale approaches, but considerably and safely decreasing the calculation cost, even when the requested increments are outside the training range. We emphasize that the performance of TANNs and standard ANNs can be improved",
        "by increasing the dimension of the training data-sets, the number of the hyper-parameters (e.g. numbers of hidden layers, etc.). Nevertheless, the fundamental gap between the two approaches in assuring thermodynamically consistent quantities still persist. 25 le3 e-3 1.0 model 3.0 model TANN TANN ANN 2.5 ANN 0.5 2.0 € 0.0",
        "1.5 5.0.5 1.0 d 0.5 model A -1.0 0.0 TANN ANN 51 () le-3 Ep 6p (-) le-3 Suku (-) le-5 (a) strain increment As = 1 X 10-3. le4 le3 el model TANN ANN 3 0- 4 -2 model A - model -4 TANN 5 TANN 0 ANN ANN",
        "-1.0 -0.5 0.0 0.5 1e1 1.0 -0.5 0.0 0.5 1.0 -6 E1 (-) Ep- 5p (-) le-1 Liku () le-3 (b) strain increment As = 1 X 10-2. le5 le5 le4 model 0.00 4 TANN ANN -0.25 E 2 0 -0.50 6 2-0.75 model - model -4 A-1.00 0 TANN",
        "TANN -6 ANN -1.25 ANN -1.0 -0.5 0.0 0.5 1.0 1.0 -0.5 0.0 0.5 1.0 () Ep 6p (-) Sysu (-) le-2 (c) strain increment As = 1 X 10-1. Figure 12: Comparison of the stress, energy, and dissipation predictions of TANNS and standard ANNs.",
        "Energy and dissipation for ANNS are computed according to Eq. (5.1), for the cyclic, isotropic loading path AEi = AE, = AE = As sgn (cos EN) -with N = Emax/AE, Emax = 2 X 10-3 (a), Emax = x10-1 (b), and Emax = 1",
        "(c), for material case H-1 (perfect plasticity). Each row represents the prediction at different As increments. 26 le3 le3 1.25 model 1.5 model TANN TANN 1.00 ANN ANN 1.0 * 0.5 E 50.25 0.00 0.0 E1 () le-3 63 (-) le-3 (a) strain increment As = 1 X 10-4. le3",
        "le3 1.25 model 1.5 model 1.00 TANN TANN ANN 1.0 ANN Ea 0.5 50.25 6 0.00 0.0 -1 0 le-3 le-3 E1 () 53 (-) (b) strain increment As = 1 X 10-3. le4 le4 model model 6 TANN 6. TANN ANN ANN a 4 41 6 2- 6 0",
        "-1.0 -0.5 0.0 0.5 1e1 0.0 0.5 1.0 1.5 E1 (-) (-) 2e1 E3 (c) strain increment As = 1 X 10-2. le5 le5 model model 6 TANN 6- TANN ANN ANN 4 a 6 2 6 -1.0 -0.5 0.0 0.5 1.0 0.0 0.5 1.0 1.5 2.0 E1 ()",
        "3 (-) (a) strain increment As = 1 X 10-1. Figure 13: Comparison of the stress predictions of TANNS and standard ANNs with respect to the target values, for the tri-axial cyclic loading path, Eq. (27), for material case H-1 (perfect plasticity). Each row",
        "represents the prediction at different As increments. 27 le-3 1.0 le-3 0.01 model TANN 0.8 -0.5 ANN 3 3 0.6 a1.01 G0.4 model TANN -1.54 0.2 ANN USUUEU 0.0 -2.0- le-3 3 E1 (-) 63() le-3 (a) strain increment As = 1 X 10-4. le-3 1.0 le-3 0.01 model TANN",
        "0.8 -0.5 ANN 0.6 3 C 1.01 0.4 model TANN -1.5 0.2 ANN 0.0 -2.0 -2 -1 0 2 3 51 () le-3 (-) le-3 (b) strain increment As = 1 X 10-3. le-1 le-2 model model 0.04 TANN TANN ANN ANN 3o.5 C a 2 -1.0 0 -1.5 2",
        "-1.0 -0.5 0.0 0.5 1e1 0.0 0.5 1.0 1.5 251 EI () 63 () (c) strain increment As = 1 X 10-2. le-1 model model 0.0 * TANN TANN ANN ANN 30.5 C 2 3 -1.0 0 -2 -1.5 -1.0 -0.5 0.0 0.5 1.0 0.0 0.5 1.0 1.5 2.0",
        "E1 (-) 53 (-) (a) strain increment As = 1 X 10-1. Figure 14: Comparison of the internal variable predictions of TANNS and standard ANNs with respect to the target values, for the tri-axial cyclic loading path, Eq. (27), for material case H-1 (perfect plasticity).",
        "Each row represents the prediction at different AE increments. 28 le-2 6 model model TANN 3 TANN & ANN ANN I 3 2 2 & A 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 Ep 5p () 3 Susu () le-4",
        "(a) strain increment As = 1 X 10-4. le-1 model model 5 TANN 3 TANN ANN a ANN 3 2 2 A A 0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 Ep-5p (-) le-3 byky (-) le-3 (b) strain increment As = 1 X 10-3.",
        "1.50 le4 lel model L model 1.25 TANN TANN 1.00 ANN 0 ANN I 0.75 0.50 A 0.25 A 0.00 0.00 0.25 0.50 0.75 1.00 0.0 0.5 1.0 1.5 Ep- -6p (-) 155150 Liky () le-2 (c) strain increment As = 1 X 10-2. le6 le3 1.50 model 1.25 TANN",
        "a 1.00 ANN 5 0.75 -2 50.50 & 0.25 A model TANN 0.00 -6 ANN 0.00 0.25 0.50 0.75 1.00 1.25 1.50 0.0 0.5 1.0 1.5 Ep 6p (-) hibuy (-) le-1 (a) strain increment As = 1 x 10-1.",
        "Figure 15: Comparison of the energy and dissipation rate predictions of TANNS and computation according to Eq. (5.1) for standard ANNS with respect to the target values, for the tri-axial cyclic loading path, Eq. (27), for material case H-1 (perfect plasticity). Each rOW represents the prediction at different As increments.",
        "29 5.2. von Mises hypo-plasticity We illustrate the performance of TANNS in predicting smooth material behaviors as well, modeled here using the hypo-plasticity model, presented in Einav (2012) and paragraph 4.1.2. The energy potential and dissipation rate are given by 9K F (Ep- Sp)+ 2 +Gle-2)-e-2, D=d.:",
        "with z being defined in Eq. (25a). We consider K = 167 GPa, G = 77 GPa, and S = 1, see Eq.s (25a) and (25b). Data are generated as detailed in Section 4. 8000 are data generated through random",
        "loading paths. As in the case of hyper-plasticity, additional sampling with random uni-axial and bi-axial random loading paths are also used. The samples are split into training (50%), validation (25%), and test (25%) sets. The sampling in terms of the mean and deviatoric",
        "stresses, p and 9, and deformations, Ep and e, is presented in Figure $10. The architecture and hyper-parameters of TANNS are maintained equal to the hyper- plastic case (see paragraph 5.1). The internal variables Si are selected to coincide with",
        "the inelastic strain. We emphasize that this particular choice does not affect the results of TANNS. As extensively discussed in Einav (2012), an alternative choice to the selection of the inelastic strain as internal variable can be the material porosity.",
        "The early stopping rule assures convergence, after approximately 1000 epochs, with MAEs of the same order of magnitude for the 4 outputs, AK, F'+Ar, Ac, and D'+Ar The (adimensional) MAE is approximately equal to 1 X 10-4 for all outputs at the end of the training.",
        "5.2.1. TANN VS standard ANN. Generalization of the network As for the hyper-plastic cases, we investigate the performance of TANNS with respect to standard ANNs through illustrative examples. The architecture and hyper-parameters of ANNs are maintained equal to the hyper-plastic case (see paragraph 5.1.3).",
        "Figure 16 shows the predictions of both networks for the following bi-axial loading path ni AEi = -AE2 = As sgn COs AE3 = 0, 2N with N = Emax/AE, Emax = 2 X 10-3 + 1, and As = 2 X 10-4,2x 10-3. TANNs' predictions",
        "are in excellent agreement with the target model. The smoother material response, with respect to the hyper-plastic scenario, is well captured by the networks. Standard ANNs clearly underperform. 30 le2 3 le-3 model model TANN 2 TANN ANN ANN 0 € 6 -1 -1 d & -2 -2 2",
        "-3 2 E1 (-) le-3 (-) le-3 (a) strain increment As = 2 X 10-4. le2 le-2 model 2 ANN TANN model € € 01 TANN 0 6 ANN 4 1 2 oota 2 61 (-) le-2 E1 (-) le-2 (b) strain increment As = 2 X 10-3.",
        "Figure 16: Comparison of the stress and internal variable predictions of TANNS and standard ANNs with respect to the target values, for a bi-axial cyclic loading path, AE2 = -AE1, with AEi as in Eq. (5.2.1), for a perfect hypo-plastic material. Each row represents the predictions at different As increments.",
        "Additional demonstration of the performance of TANNS is given in Figure 17, for a bi- axial loading path with strain-controlled ratcheting. Ratcheting is a well known phenomenon shown by many materials during cyclic loading, which has been modeled here with the",
        "h?plasticity framework (Einav, 2012). In particular, we show that TANNS, contrary to ANNs, successfully predict principal stresses, inelastic strains, energy potential, and dissipation rate. 31 le2 le2 model TANN ANN / 0 01 - - € € -1 6 -1 model 6 TANN -2 -2 ANN -3 -6 51 (-)",
        "le-3 2 (-) le-3 (a) principal stresses. le-3 le-3 model model 61 TANN 2 TANN ANN ANN 0 E € -2 a U S 4 -2 -6 8 6 -6 4 E1 (-) le-3 2 () le-3 (b) principal internal variables. le-1 le-2 2.0 - model 6 TANN 1.5 ANN",
        "a 1.0 & 0.5 model A 2 0.0 TANN a ANN 3 -2 Ep-6p () le-4 Syku (-) le-4 (c) energy and dissipation rate. Figure 17: Comparison of the predictions of TANNS and standard ANNs with respect to the target values,",
        "for the bi-axial loading path with strain-controlled ratcheting, for a perfect hypo-plastic material. 32 6. Noise in training data and robustness of predictions After having demonstrated the performance of TANNS and their superiority to standard approaches in modeling path-dependent material behaviors, we investigate the effect of noise",
        "in the measurements of the data used to train artificial neural networks. This is achieved by training TANNS (and ANNs) using the previously generated data and adding, in the training and validation sets, artificial noise. For sake of clarity, we consider a perfectly",
        "plastic material (case H-1, cf. Tab. 1). The additive noise, ns, is based on a normal distribution with standard deviation (sd) equal to 10% of the mean value of the clean data. In particular, we consider the following scenarios, independently:",
        "(1) noise in 4Ar, 7 i.e., nsg, with sd = 10% of the mean value of 5A, (2) noise in CAr, 7 i.e., nso, with sd = 10% of the mean value of CAr,",
        "(3) noise in F+Ar 1 i.e., nsF, with sd = 10% of the mean value of Fl+Ar, and (4) noise in D'+Ar, 1 i.e., nsF, with sd = 10% of the mean value of D'+A We emphasize that the aforementioned noise levels were chosen to demonstrate the performance",
        "of TANNS and generally lower levels of noise are expected in practical applications. However, we examine each scenario independently in order to explore better the effect of noise on training and on the accuracy of the predictions. In cases (1) and (2), once the noised",
        "quantities are computed (denoted with aAr and Z+A), the increments, i.e., Adi and ASi are re-evaluated as Adi = a#Ar ai and ASi = gear - 5, respectively. The architecture and hyper-parameters of the neural networks, both TANNs and ANNs,",
        "designated in this study are the same as those used in Section 5. It should be noticed that, for each case (1-4), the data used to train the networks are not respecting the thermodynamics requirements due to the added noise, i.e., Eq. (13).",
        "The addition of noise can have an impact on the training of the networks and their predictions. We first focus on the former. Figure 18 displays the loss functions of each output as the training of TANNs is performed, for noise added in stresses, case (2). The MAE is",
        "evaluated between the TANNs' predictions and the (noised) training and validation data- sets. Table 3 shows the MAEs of the predictions of TANNS with respect to the validation data-sets, for each level of noise, at the end of the training. Although the earlystopping",
        "technique is used, training is accomplished, in all cases of noise, after approximately 1000 epochs. By comparing the training using the original, un-noised data (Fig. 8) and that using the noised ones (Fig. 18), we can observe that TANNS are unable to learn the noised signal, i.e.,",
        "Ad,. This is a direct consequence of the fact that the network evaluates the stress increments from the knowledge of the stress state at time t and the energy potential predictions. When noise is added, the first law of thermodynamics is violated and the training operation with",
        "noised data is unsuccessful, with respect to the noised training and validation data-sets. However this is not a drawback of our approach. On the contrary, it is an indication of the 33 le-1 Afitrain Alival le-2 Ftrain Fyal 3 le-3 Aditrain Adival le-4 Dtrain Dval 10 100 1000 epochs",
        "Figure 18: Errors of the predictions of TANN, as the training is being performed, evaluated with respect to the training (train) and validation (val) sets. Noise is added in stress to both training and validation data-sets, case (2).",
        "Table 3: MAEs of the predictions of TANNS with respect to the validation data-sets, for the original, un- noised data and each level of noise, at the end of the training, Early-stopping is used and the training is always completed at approximately 1000 epochs. Mean Absolute Error (le-4)",
        "ALi Aci Fl+Ar ADI+A: un-noised3.2 4.1 0.8 7.5 nsy 324 2.5 0.9 5.9 nsa 3.3 39 1.0 5.4 nsF 3.3 3.1 19 7.3 nsp 3.4 4.9 0.9 57 quality of the data, which in this case they don't respect the laws of thermodynamics due to",
        "measurement noise. Notice that the values of the training error is consistent with Eq. (13), the expression given in Section 3.4 and the magnitude of the noise. The implementation of the laws of thermodynamics in the network's architecture shields the learning process and prohibits learning of inconsistent data.",
        "For instance, with reference to Table 3, we can see that for case (2), nsa, the MAEs in the predictions of the inelastic strains, energy and dissipation rate approximately coincide with those obtained with the un-noised data.",
        "The aforementioned behavior is not observed in standard ANNS. As an example, we show in Table 4 the MAEs of the predictions of standard ANNs with respect to the validation data-sets, for noised stresses. In this case, we can see that the network, unaware of the",
        "requirements of the thermodynamics, learns successfully the noised outputs. This means that, once standard ANNs are asked to make predictions, in recall mode, the outputs will be affected by the noisy training in an unpredicted way. For the levels of noise cases (1), (3), and (4), similar results are obtained.",
        "34 Table 4: MAES of the predictions of standard ANNs with respect to the validation data-sets, for the original, un-noised data and noise on stresses, at the end of the training (approximately 1000 epochs). Mean Absolute Error (le-4) ALi Aci un-noised5.8 4.2 nsa 5.9 4.2",
        "In Figure 19 compared the predictions in recall mode of both networks based on noise data. The predictions of the training with clean (un-noised) data is also presented for helping the comparison. We notice that TANNS, whilst trained on data with relatively large",
        "levels of noise, successfully predict the material response and perform more or less as when trained on data free of noise. On the contrary, standard ANNs are strongly affected by the large levels of noise of the data used to train the network. Similar results are found in",
        "presence of noise in the training and validation data of the internal variable, Si, see Figure S11, in the Supplementary Material. It should be noticed that, in this case, ANNs do not manage to successfully minimize the loss function of ALi, with the selected number of hyper-",
        "parameters. This is the consequence of the ANN architecture which have been chosen to achieve the best performance with thermodynamic consistent (clean of noise) data. However, we emphasize that, if the number of hyper-parameters of the ANN model were increased",
        "to achieve convergence with respect to the noised data, then ANNs would learn the noised material response, resulting to be highly affected by noise measurements. Consequently, we can state that TANNs show high degree of robustness to noise, when compared to ANNs. 7. Concluding remarks",
        "A new class of artificial neural networks models to replace constitutive laws and predict the material response at the material point level was proposed. The two basic laws of thermodynamics were directly encoded in the architecture of the model, which we refer to",
        "as Thermodynamic-lased Neural Network (TANN). Our approach was inspired by the SO- called Physics-Informed Neural Networks (PINNS) (Raissi et al., 2019), where the automatic differentiation was used to perform the numerical calculation of the derivative of a neural",
        "network with respect to its inputs. Feed-Forward Neural Networks were used herein, but the approach is general and can be applied to Recurrent Neural Networks (RNNs) or other types of ANNs as well. The numerical requirements regarding the mathematical class of appropriate activation",
        "functions to be used together with automatic differentiation were investigated. More specifically, the internal restrictions, derived from the first law of thermodynamics, require activation functions whose second gradient does not vanish. This new problem and its remedy was extensively explored and discussed in the manuscript. 35 le-2 le3 le-3 model",
        "model 1.5 model 0.0 ANN ANN 0 ANN noised -0.5 1 noised ANN € 1.0 31.0 ANN 2 5 0.5 51.5 C0xxxc0c0xxx006 A -2.0 0.0 noised ANN -2.5 0.0 0.5 1.0 1.5 () le-3 () le-3 usuy () 324 le3 le-3 le-2 1.25 model 0.0 model - model 1.00 TANN",
        "0 TANN 3 TANN noised noised 3 0.75 -0.5 TANN TANN 3 0.50 C 2 -1.0. 5 0.25 0.00 noised TANN -1.5 51 () le-3 () le-3 0.0 0.5 1.0 1.5 (a) strain increment As = 1 X (b) strain increment As = 1 X Ayki () le-4 10-4. 10-3.",
        "(c) strain increment As = 1 X 10-2. Figure 19: Influence of noise in the stress, Oi, for the predictions of the stress, internal variable, and dissipation rate of TANNS and of standard ANNS with respect to the target values, for the tri-axial cyclic",
        "loading path for material case H-1 (perfect plasticity). Noise strongly affect the predictions of standard ANNS, see Fig.s 13-15. TANN, relying on an incremental formulation and on the theoretical developments in Houlsby and Puzrin (2007), posses the special feature that the entire constitutive response",
        "of a material can be derived from definition of only two scalar functions: the free-energy and the dissipation rate. This assures thermodynamically consistent predictions both for seen and unseen data. Differently from the standard ANN approaches, TANN does not have",
        "to identify, through learning, the underlying thermodynamic laws. Indeed, predictions of standard ANNs may be thermodynamicaly inconsistent, even though the training of the network has been performed on consistent material data. Being aware of physics, TANNs",
        "are found to be a robust approach with the presence of noise measurements in the training data, contrary to the standard ANN approach. For the cases here investigated, we showed that TANNS are characterized by high accuracy of the predictions, higher than those of standard approaches. The integration",
        "of thermodynamic principles inside the network renders TANN's ability of generalization (i.e., make predictions for loading paths different from those used in the training operation) remarkably good. Consequently, TANN is an excellent candidate for replacing constitutive calculations at Finite Element incremental formulations. Moreover, thanks to the implementation",
        "of the free-energy in the network predictions and its thermodynamical relation with the stresses, the Jacobian AE at the material point level is better predicted even for increments far beyond the training data-set range. As a result quadratic convergence in implicit formulations can be preserved, reducing the calculation cost.",
        "Finally, we investigated the presence of noise in data and the effect on the training process 36 and predictions in recall mode. The thermodynamic framework of TANNS shields the training operation and prohibits learning of inconsistent data. As a result, TANNS posses",
        "high degrees of robustness to noise, compared to standard ANNS. Further extensions of TANN in a wide range of applications, for complex materials, are straightforwards, as the thermodynamics principles hold true for any known class of material, at any length (micro- and macro-scale). Acknowledgments",
        "The authors would like to acknowledge the anonymous reviewers whose feed-backs helped improve this work. The author I.S. would like to acknowledge the support of the European Research Council (ERC) under the European Union Horizon 2020 research and innovation program (Grant agreement ID 757848 CoQuake). 37",
        "7.1. Appendix A. Understanding second-order vanishing gradient In the following, we investigate the performance and influence of different activation functions on the computational time to train an ANN with input I, primary output O1,and",
        "secondary output O2 = VIO1. Consider the above discussed example with I = x, O1 = x?, and O2 = 2x. The ANN has one hidden layer, with N, = 6 nodes, and activation functions",
        "as reported in Table 5. The output layer has linear activation and null bias. The absolute error is selected as loss function for both Oi and O2. Training is performed on 1000 samples, normalized between -1 and 1. A very small value for the learning rate is selected, i.e.,",
        "E = 10-5 in order to facilitate the gradient descent algorithm in reaching small values of the loss function. We use early-stopping. In other words, training is stopped as the error of a validation set (500 samples) starts to increase while the learning error still decreases (Géron,",
        "2019). The validation set is used to avoid over-fitting of the training data. Table 5: Set of activation functions considered to investigate the performance of the network with outputs 0= x and VIo = 2x, with I = x, in the framework of first- and second-order vanishing gradients.",
        "Function Z range S(z) '(z) '\"(z) z<0 0 0 0 ReLUz z20 Z 1 0 z<0 0 0 0 ReLUoS24z z20 0.5:2+z Z+ 1 1 z<0 0 0 0 ReLU z20 22 2z 2 ELUe Vz ez-1 ez ez z<0 e-1 ez ez ELU, z20 z 1 0 z<0 e2-1",
        "ez ez ELUOSz z20 0.52?+z Z+ 1 1 z<0 e-1 ez ez ELU, z20 z2 2z 2 z<0 e-1 ez ez ELU z20 2* 4z3 12z2 z<0 ez-1 ez ez ELU40522 z20 24 + 0.522+z 423+z+1 12:2+1 For each tested activation function, Table 6 shows the adimensional Mean Absolute Error",
        "(MAE) calculated using a set of new, unseen data (500 samples) of input-output predictions for x2 and 2x. The advancement of training is quantified herein as the number of epochs, i.e., the number with which the training algorithm works with the training data-set Géron",
        "(2019). Activation functions with quadratic terms, or of higher degree, perform very well, compared to their linear equivalents. RELU2, ELU2 outperform as their shape is very similar to the input-output regression they are trained to learn. Nevertheless, it is worth 38",
        "noticing that training fails when activation functions with vanishing second gradient are used (e.g. RELUZ and ELUz). Figure 20 compares the ANN predictions for a selection of activation functions with the analytical (exact) results. Whilst RELUZ is clearly inadequate,",
        "ELUZ predictions overall agree with the analytical values. This is due to the fact that the ANN takes advantage of the exponential term, for negative Z and thus successfully manage to satisfy both O and Vro. Additional hidden layers may improve the performance of the",
        "network. It can be further noticed that activation function of high degree, e.g. ELUe, ELU, and ELUA405242 even if successful, require a large number of epochs. Table 6: Activation functions and performance with unseen data. Activation function SA L Lo Lv,o no. epochs (10-4) (10-4) (10-4) () ReLUz",
        "1521.2 205.98 1315.18 920 ReLU0.524z 762.4 93.58 668.85 8054 ReLU2 0.061 0.0241 0.0371 148 ELUe 127.2 26.83 100.38 19477 ELU, 108.56 12.12 96.44 17280 ELUOSB4z 65.5 10.91 54.63 12178 ELU,2 0.13 0.067 0.067 88 ELUA 65.36 33.75 31.61 20051 ELU40S 12.94 1.81 11.13 9683 39 1.2 2z 1.0 ReLU, Z",
        "ReLU, 0.8 ELU, ELU, 0.61 o 0.4 0.2 01 Z -1.0 -0.5 0 0.5 1.0 -1.0 -0.5 0 0.5 1.0 z() 2 (-) 0.25 1.0 2 0.75 2z 0.20 ReLU, ReLU, ELU, 0.50 ELU, 20.15 0.25 0 0.14 C' 0 -0.25 0.05 0.50 01 -0.75 -0.4 -0.2 0 0.2 0.4",
        "-1.0 -0.4 -0.2 0 0.2 0.4 z() 2 (-) (a) x2 predictions, O1, using ReLU and ELUz- (b) 2x predictions, O2, using ReLU and ELUz- le-2 le-1 65 4 2z ELU, ELU, ELUAsAs 2 C E EV ELU. ELU, d 0 ELU, -2 -4 0 2 0 2 (-) le-1",
        "(-) le-1 le-2 le-2 1.0 2 3 2z 0.8 ELU, 2 ELU, ELUasAs ELUAsA 0.6 ELU. C ELV 0.4 E, d 0 ELU, 0.2 -1 -2 U -0.5 0.5 -3 -1.5 -1.0 -0.5 0 0.5 1.0 1.5 (-) le-1 z() le-1",
        "(c) 2 predictions, O1, using ELUZ, ELU0522+2 (a) 2x predictions, 02, using ELUz, ELUOS2 ELU,, ELUe, and ELU. ELU2, ELUe, and ELU. Figure 20: Comparison of different activation functions for the prediction of the primary output, x2 (a), and",
        "secondary output, 2x (b). From top to bottom the range of Z decreases from larger to smaller values, to observe the behavior at z & 0. 40 Appendix B. Derivation of the incremental material formulation By differentiating the energy expressions (13) and rearranging the terms, we obtain the",
        "following non-linear incremental relations à = OsF-g+ > OEAF-5k + OFê (28a) -Ki = OyF.E+ OLE F + agoFè (28b) -S = OF.E+ > dog F . 5k + OgFè, (28c) where the following notation is adopted 8F 8PF OgsF = deyF= dEioEu dEiO5 8F 8?F OEoF = O0F =",
        "dEij00 a02 We introduce the thermodynamic dissipative stresses X+ = (X1,. ...,XN) with aD X,= ViE[1,N). (29) ab For a rate-independent material, the dissipation is a homogeneous first-order function in the internal variable rates 5i (Houlsby and Puzrin, 2007). This homogeneity can be expressed by the Euler's relation OD",
        "D = x-5 (30) i=1 a5i which, together with (11), implies x-x)-4=0 (31) i-1 Ziegler's orthogonality condition (Ziegler, 2012) is further assumed, i.e., X; = Xi Vie [1,N]. Being D homogeneous first-order function in 5i the Legendre transform, conjugate to Xi, is",
        "degenerate, that is equal to zero, and represents the yield function y = 5(6,6,Z,X), i.e. ly = 2x-4-D=0, (32) where 2 is a non-negative multiplier. From the properties of Legendre transform, the following flow rules must hold dy 5 = A Vie[ [1,N]. (33) ax; 41",
        "Since 2 N 0 and ly = 0,<0. If y=0, the following consistency equation is met Oy dy dy y - + B=0. (34) ds i=1 asi i-1 ax; 00 By further using the flow rules (33) and Ziegler's normality condition, we obtain CE Co A = - e, (35)",
        "B B with dy dy C= OyeF, Os ax; i-1 dy dy Ce OgoF, 00 ax; i=1 and dy dy dy ày B = OueF = a5i ax; - aX; aXk Finally, we arrive to the following, incremental non-linear formulation, for y = 0, MgE Mgo o Mje Mxe &",
        "Mos Moo E = Ml-0 5, with É= -S 0 Ml-0 = CE.B Co dy (36) ax; B ax; A Bc. Ce B B and MgE = OgsF- Zk OF-(% ax, Mge = OseF - Zk deF-( axk Mxe = agsF- ZOuF-(. axk y Mxo = OyoF - ZOuF-(. ax;",
        "Mes = OBsF = Zk OF-( axe) J Moe = O00F - Zk Oes F axk In case of y < 0, relation (36) becomes OEsF OEoF] ageF OyoF E = Mlyco 5, with Mlyso = OBsF O0oF (37) 0 0 0 0 42 References",
        "O. Lloberas Valls, M. Raschi Schaw, A. E. Huespe, X. Oliver Olivella, Reduced finite element square techniques (rfe2): towards industrial multiscale fe software, in: COMPLAS 2019: XV International Conference on Computational Plasticity: Fundamentals and Applications, International Centre for Numerical Methods in Engineering (CIMNE), 2019, pp. 157-169.",
        "M. Nitka, G. Combe, C. Dascalu, J. Desrues, Two-scale modeling of granular materials: a DEM-FEM approach, Granular Matter 13 (2011) 277-281. doi:10.1007/s10035-011-0255-6. F. Feyel, A multilevel finite element method (FE2) to describe the response of highly non-linear structures",
        "using generalized continua, Computer Methods in Applied Mechanics and Engineering 192 (2003) 3233- 3244. doi10.1016/S0045-7825(03/00348-7. N. Bakhvalov, G. Panasenko, Homogenisation: Averaging Processes in Periodic Media: Mathematical Problems in the Mechanics of Composite Materials, 1989. G. Houlsby, A. Puzrin, A thermomechanical framework for constitutive models for rate-independent",
        "dissipative materials, International journal of Plasticity 16 (2000) 1017-1047. I. Einav, G. Houlsby, G. Nguyen, Coupled damage and plasticity models derived from energy and dissipation potentials, International Journal of Solids and Structures 44 (2007) 2487-2508.",
        "G. T. Houlsby, A. M. Puzrin, Principles of hyperplasticity: an approach to plasticity theory based on thermodynamic principles, Springer Science & Business Media, 2007. I. Einav, The unification of hypo-plastic and elasto-plastic theories, International Journal of Solids and Structures 49 (2012) 1305-1315.",
        "F. Masi, I. Stefanou, V. Maffi-Berthier, P. Vannucci, A discrete element method based-approach for arched masonry structures under blast loads, Engineering Structures 216 (2020) 110721. dothtps//dolor/1. lolpempinet2P.IUTAL. F. Masi, I. Stefanou, P. Vannucci, A study on the effects of an explosion in the Pantheon of Rome,",
        "Engineering Structures 164 (2018) 259-273. doi10.1016/jengstruct.2018.02.082- H. Rattez, I. Stefanou, J. Sulem, The importance of thermelydromechanical couplings and microstructure to strain localization in 3d continua with application to seismic faults. part i: Theory and linear stability",
        "analysis, Journal of the Mechanics and Physics of Solids 115 (2018a) 54 = 76. domhtps/dolorg/l. 1016/3mps2018.03.00. H. Rattez, I. Stefanou, J. Sulem, M. Veveakis, T. Poulet, The importance of hemmoelydromechanica. couplings and microstructure to strain localization in 3d continua with application to seismic faults. part",
        "ii: Numerical implementation and post-bifurcation analysis, Journal of the Mechanics and Physics of Solids 115 (2018b) 1 29. doihttps://doi.or/10.1016/mps:.2018.03.003. N. A. Collins-Craft, I. Stefanou, J. Sulem, I. Einav, A cosserat breakage mechanics model for brittle, granular media, Journal of the Mechanics and Physics of Solids (2020) 103975. dolhttps//dol.org/10.1016/mps. 2020.103975.",
        "A. P. V. D. Eijnden, P. Bésuelle, F. Collin, R. Chambon, J. Desrues, Modeling the strain localization around an underground gallery with a hydro-mechanical double scale model ; effect of anisotropy, Computers and Geotechnics (2016). do10.1016/.compge0.2016.08.06. A. Geron, Hands-on MachineLearning with Scikit-Learn & Tensorflow, volume 1, O'Reilly Media, 2015.",
        "dol10.1017/CB0978107415324.00.. MAarXIIOILIGRN4. T. M. Mitchell, et al., Machine learning. 1997, Burr Ridge, IL: McGraw Hill 45 (1997) 870-877. G. Cybenko, Approximation by superpositions of a sigmoidal function, Mathematics of control, signals and systems 2 (1989) 303-314. T. Chen, H. Chen, Universal approximation to nonlinear operators by neural networks with arbitrary",
        "activation functions and its application to dynamical systems, IEEE Transactions on Neural Networks 6 (1995) 911-917. J. Ghaboussi, J. H. Garrett, X. Wu, Koowlatga200hased modeling of material behavior with neural networks, Journal of Engineering Mechanics 117 (1991) 132-153. doi10.1061/(ASCE)733-9390991) 117:1(132). 43",
        "J. Ghaboussi, D. Sidarta, New nested adaptive neural networks (nann) for constitutive modeling, Computers and Geotechnics 22 (1998) 29-52. M. Lefik, B. A. Schrefler, Artificial neural network as an incremental non-linear constitutive model for a finite element code, Computer methods in applied mechanics and engineering 192 (2003) 3265-3283.",
        "S. Jung, J. Ghaboussi, Neural network constitutive model for rate-dependent materials, Computers & Structures 84 (2006) 955-963. C. Settgast, M. Abendroth, M. Kuna, Constitutive modeling of plastic deformation behavior of open-cell foam structures using neural networks, Mechanics of Materials 131 (2019) 1-10.",
        "Z. Liu, C. Wu, Exploring the 3d architectures of deep material network in data-driven multiscale mechanics, Journal of the Mechanics and Physics of Solids 127 (2019) 20-46. X. Lu, D. G. Giovanis, J. Yvonnet, V. Papadopoulos, F. Detrez, J. Bai, A data-driven computational",
        "homogenization method based on neural networks for the nonlinear anisotropic electrical response of graphene/Polymer nanocomposites, Computational Mechanics 64 (2019) 307-321. K. Xu, D. Z. Huang, E. Darve, Learning constitutive relations using symmetric positive definite neural networks, arXiv preprint arXiv:2004.00265 (2020).",
        "D. Z. Huang, K. Xu, C. Farhat, E. Darve, Learning constitutive relations from indirect observations using deep neural networks, Journal of Computational Physics (2020) 109491. S. Gajek, M. Schneider, T. Bohlke, On the micromechanics of deep material networks, Journal of the Mechanics and Physics of Solids (2020) 103984.",
        "M. B. Gorji, M. Mozaffar, J. N. Heidenreich, J. Cao, D. Mohr, On the potential of recurrent neural networks for modeling path dependent plasticity, Journal of the Mechanics and Physics of Solids (2020) 103972. Y. Heider, K. Wang, W. Sun, S0(3)-invariance of informed-graph-based deep neural network for anisotropic",
        "elastoplastic materials, Computer Methods in Applied Mechanics and Engineering 363 (2020) 112875. doihttps://doi.org/10.1016/.cma.cma.2020.112875. F. Ghavamian, A. Simone, Accelerating multiscale finite element simulations of history-dependent materials using a recurrent neural network, Computer Methods in Applied Mechanics and Engineering 357 (2019) 112594.",
        "M. Mozaffar, R. Bostanabad, W. Chen, K. Ehmann, J. Cao, M. Bessa, Deep learning predicts path- dependent plasticity, Proceedings of the National Academy of Sciences 116 (2019) 26414-26420. A. L. Frankel, R. E. Jones, C. Alleman, J. A. Templeton, Predicting the mechanical response of oligocrystals",
        "with deep learning, Computational Materials Science 169 (2019) 109099. D. Gonzalez, F. Chinesta, E. Cueto, Learning corrections for hyperelastic models from data, Frontiers in Materials 6 (2019) 14. M. Lefik, D. Boso, B. Schrefler, Artificial neural networks in numerical modelling of composites, Computer",
        "Methods in Applied Mechanics and Engineering 198 (2009) 1785-1804. T. Kirchdoerfer, M. Ortiz, Data-driven computational mechanics, Computer Methods in Applied Mechanics and Engineering 304 (2016) 81-101. R. Ibanez, D. Borzacchiello, J. V. Aguado, E. Abisset-Chavanne, E. Cueto, P. Ladevèze, F. Chinesta, Data-driven non-linear elasticity: constitutive manifold construction and problem discretization,",
        "Computational Mechanics 60 (2017) 813-826. T. Kirchdoerfer, M. Ortiz, Data-driven computing in dynamics, International Journal for Numerical Methods in Engineering 113 (2018) 1697-1710. R. Ibanez, E. Abisset-Chavanne, J. V. Aguado, D. Gonzalez, E. Cueto, F. Chinesta, A manifold learning approach to data-driven computational elasticity and inelasticity, Archives of Computational Methods",
        "in Engineering 25 (2018) 47-57. R. Eggersmann, T. Kirchdoerfer, S. Reese, L. Stainier, M. Ortiz, Model-free data-driven inelasticity, Computer Methods in Applied Mechanics and Engineering 350 (2019) 81-99. M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics-informed neural networks: A deep learning framework",
        "for solving forward and inverse problems involving nonlinear partial differential equations, Journal of Computational Physics 378 (2019) 686-707. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, J. M. Siskind, Automatic differentiation in machine learning: 44 a survey, The Journal of Machine Learning Research 18 (2017) 5595-5637.",
        "G. A. Maugin, W. Muschik, Thermodynamics with internal variables. Part I. General concepts, 1994. P. M. Mariano, L. Galano, Fundamentals of the Mechanics of Solids, Springer, 2015. L. Anand, O. Aslan, S. A. Chester, A large-deformation gradient theory for elastic-plastic materials: Strain",
        "softening and regularization of shear bands, International Journal of Plasticity 30-31 (2012) 116 - 143. doi.https//doi.or/10.016/jplas.2011.10.002. Y. H. Hu, J.-N. Hwang, Handbook of neural network signal processing, 2002. A. Géron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, O'Reilly Media, 2019.",
        "M. Bessa, R. Bostanabad, Z. Liu, A. Hu, D. W. Apley, C. Brinson, W. Chen, W. K. Liu, A framework for data-driven analysis of materials under uncertainty: Countering the curse of dimensionality, Computer Methods in Applied Mechanics and Engineering 320 (2017) 633-667.",
        "A. Karpatne, W. Watkins, J. Read, V. Kumar, Physics-guided neural networks (pgnn): An application in lake temperature modeling, arXiv preprint arXiv:1710.11431 (2017). H. Ziegler, An introduction to thermomechanics, Elsevier, 2012. P. Bogacki, L. F. Shampine, A 3 (2) pair of Runge-Kutta formulas, Applied Mathematics Letters 2 (1989) 321-325.",
        "T. Dozat, Incorporating Nesterov momentum into Adam (2016). 45"
    ],
    "summaries": [
        {
            "chunk_id": 1,
            "summary": "Researchers at the Massachusetts Institute of Technology (MIT) have developed an artificial neural network that can be used to"
        },
        {
            "chunk_id": 2,
            "summary": "All documents in the HAL archive are copyrighted."
        },
        {
            "chunk_id": 3,
            "summary": "The aim of this project is to collect and disseminate research documents, whether they are pub-"
        },
        {
            "chunk_id": 4,
            "summary": "The European Research Council (ERC) is a member of the European Union (EU)."
        },
        {
            "chunk_id": 5,
            "summary": "The paper is available on the following websites:"
        },
        {
            "chunk_id": 6,
            "summary": "The aim of this paper is to develop new approaches for training networks."
        },
        {
            "chunk_id": 7,
            "summary": "A new class of artificial neural networks has been proposed."
        },
        {
            "chunk_id": 8,
            "summary": "In this paper, we study the dynamics of a free-energy dissipation network."
        },
        {
            "chunk_id": 9,
            "summary": "In this paper we present a new approach for training the TANNS model, which is based on"
        },
        {
            "chunk_id": 10,
            "summary": "In this paper, we show that topological entropy networks (TANNS) can be used to"
        },
        {
            "chunk_id": 11,
            "summary": "In this paper, we develop a new model for the prediction of the properties of polymers, called"
        },
        {
            "chunk_id": 12,
            "summary": "This paper presents a novel approach to materials science and engineering, called topological artificial neural network (TAN), which"
        },
        {
            "chunk_id": 13,
            "summary": "A new set of laws has been proposed to account for a wide range of phenomena in solid materials"
        },
        {
            "chunk_id": 14,
            "summary": "The aim of this paper is to develop a method for the estimation of the size of the"
        },
        {
            "chunk_id": 15,
            "summary": "thermodynamics is the study of the laws of matter."
        },
        {
            "chunk_id": 16,
            "summary": "This paper presents a new approach to the study of energy balance and the entropy production requirements"
        },
        {
            "chunk_id": 17,
            "summary": "The aim of this project is to develop new methods for the study of micromechanical phenomena."
        },
        {
            "chunk_id": 18,
            "summary": "There are many different scales to the scale of material properties, e.g."
        },
        {
            "chunk_id": 19,
            "summary": "A number of well-known models have been developed to describe the behaviour of a wide range of materials,"
        },
        {
            "chunk_id": 20,
            "summary": "The current state-of-the-art in micromechanical simulations is limited by the"
        },
        {
            "chunk_id": 21,
            "summary": "Artificial intelligence (AI) is one of the most important areas of research in computer science, but its use"
        },
        {
            "chunk_id": 22,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid considers the meaning of learning."
        },
        {
            "chunk_id": 23,
            "summary": "Stress-strain learning is a branch of machine learning that aims to \"reduce"
        },
        {
            "chunk_id": 24,
            "summary": "The training example is a set of training examples that the system uses to train itself."
        },
        {
            "chunk_id": 25,
            "summary": "Machine learning is a branch of computer science that uses computers to learn from large amounts of data."
        },
        {
            "chunk_id": 26,
            "summary": "In this paper, we study some of the best-known methods for approximating complex functions"
        },
        {
            "chunk_id": 27,
            "summary": "In this paper, we show that artificial neural networks (ANNs) can be used to"
        },
        {
            "chunk_id": 28,
            "summary": "The Department of Materials Science and Engineering at the University of Manchester (UNM) has a long tradition of"
        },
        {
            "chunk_id": 29,
            "summary": "The following is a list of papers published between 1991 and 2020."
        },
        {
            "chunk_id": 30,
            "summary": "The works in this series are based on the theory of artificial neural networks (ANNs)."
        },
        {
            "chunk_id": 31,
            "summary": "We record the work of leading researchers in the field of machine learning."
        },
        {
            "chunk_id": 32,
            "summary": "Advances in artificial intelligence (AI) and machine learning (ML) have led to the development of"
        },
        {
            "chunk_id": 33,
            "summary": "An artificial intelligence (ANN)-based approach is used to train a new class of solvers"
        },
        {
            "chunk_id": 34,
            "summary": "All photographs courtesy of AFP, EPA, Getty Images and Reuters"
        },
        {
            "chunk_id": 35,
            "summary": "In this paper, Simone and Perzyna present a new data-driven approach to finite element (FE)"
        },
        {
            "chunk_id": 36,
            "summary": "In this paper, we present a new approach to the problem of Bose-Einstein condensate (BVP) physics"
        },
        {
            "chunk_id": 37,
            "summary": "In this paper, we present a new set of methods for the prediction of the behaviour"
        },
        {
            "chunk_id": 38,
            "summary": "In this paper, we present a new class of solvers, the Bifurcation VP solvers"
        },
        {
            "chunk_id": 39,
            "summary": "Albert Einstein's adjoint network (ANN) is one of the most powerful tools for"
        },
        {
            "chunk_id": 40,
            "summary": "In order to train artificial neural networks (ANNs) on thermodynamics, a number of conditions (e."
        },
        {
            "chunk_id": 41,
            "summary": "In this paper, we present a novel method for training neural networks on unseen data"
        },
        {
            "chunk_id": 42,
            "summary": "In this paper, we show how we can train a network to predict the behaviour of"
        },
        {
            "chunk_id": 43,
            "summary": "In this paper, we present a new approach for the calculation of the free-energy"
        },
        {
            "chunk_id": 44,
            "summary": "First, we derive a new type of entropy-based artificial intelligence (ANN) that can be used to constrain"
        },
        {
            "chunk_id": 45,
            "summary": "In this paper, we present a novel approach to the study of constraints on the behaviour of"
        },
        {
            "chunk_id": 46,
            "summary": "In this paper, the activation function of an ANN (anti-nepotism network)"
        },
        {
            "chunk_id": 47,
            "summary": "In this paper we deal with the classical vanishing gradients problem in ANNs, which"
        },
        {
            "chunk_id": 48,
            "summary": "A new problem in the area of artificial neural networks (ANNs) has been discovered, in which"
        },
        {
            "chunk_id": 49,
            "summary": "The aim of this paper is to provide an overview of the current state-of-the-art in the"
        },
        {
            "chunk_id": 50,
            "summary": "This paper presents a new type of ANN, the topological acoustic networks (TANNs"
        },
        {
            "chunk_id": 51,
            "summary": "In this paper, we consider the use of artificial neural networks (ANNs) for the modelling of elas"
        },
        {
            "chunk_id": 52,
            "summary": "In the first phase of our work, we investigate the thermodynamics of artificial neural networks (ANN"
        },
        {
            "chunk_id": 53,
            "summary": "For the implementation of Artificial Neural Networks and Thermodynamic-based Artificial Neural Networks, we leverageflow v1."
        },
        {
            "chunk_id": 54,
            "summary": "A glossary of terms for the English language."
        },
        {
            "chunk_id": 55,
            "summary": "The following table lists the tensors used in Newton's second law of motion"
        },
        {
            "chunk_id": 56,
            "summary": "The second law of thermodynamics can be formulated in terms of the local Clausius- Du"
        },
        {
            "chunk_id": 57,
            "summary": "The entropy inequality of the heat supply h and the entropy inequality of the flux"
        },
        {
            "chunk_id": 58,
            "summary": "The rate of thermal dissipation is a function of the rate of mechanical dissipation."
        },
        {
            "chunk_id": 59,
            "summary": "In this paper we show that the mechanical dissipation rate of a solid-state heat sink is"
        },
        {
            "chunk_id": 60,
            "summary": "Let us define the (mechanical) dissipation rate D."
        },
        {
            "chunk_id": 61,
            "summary": "We shall assume constant material density, i.e., dp = 0"
        },
        {
            "chunk_id": 62,
            "summary": "Let assume a strain-rate independent material such that the energy potential is E:=E(S,,Z"
        },
        {
            "chunk_id": 63,
            "summary": "The state of a system is defined by the following equation."
        },
        {
            "chunk_id": 64,
            "summary": "A finite-strain formulation is a representation of a tensor that is not a priori prescribed."
        },
        {
            "chunk_id": 65,
            "summary": "Section 2 of this paper presents an incremental formulation of the strain and stress tensors of a"
        },
        {
            "chunk_id": 66,
            "summary": "This paper presents the results of a Lagrangian scheme for a large class of materials and an updated La"
        },
        {
            "chunk_id": 67,
            "summary": "The table below shows the relationships between the following terms:"
        },
        {
            "chunk_id": 68,
            "summary": "In this paper we introduce the following definitions of isothermal processes:"
        },
        {
            "chunk_id": 69,
            "summary": "The coefficients of the coefficients of the Legendre transform conjugate of e and the coefficients of the coefficients of"
        },
        {
            "chunk_id": 70,
            "summary": "In this paper, we present a new class of artificial neural networks (ANNs), based on the"
        },
        {
            "chunk_id": 71,
            "summary": "The second class of graph theory is based on the idea that a graph can be"
        },
        {
            "chunk_id": 72,
            "summary": "In this series of letters from leading scientists, we look back at some of the most important contributions"
        },
        {
            "chunk_id": 73,
            "summary": "In this paper, we present a direct approach to the design of a black-box"
        },
        {
            "chunk_id": 74,
            "summary": "In this paper we show how artificial intelligence (AI) and neural networks (ANNs) can"
        },
        {
            "chunk_id": 75,
            "summary": "We write O = Ao = o+Ar a', from the input"
        },
        {
            "chunk_id": 76,
            "summary": "The guided informed ANNS (FFNNS and RNNS) and guided informed ANNS (RN"
        },
        {
            "chunk_id": 77,
            "summary": "In this paper, we show how two neural networks, i-NN1 and i-NN2,"
        },
        {
            "chunk_id": 78,
            "summary": "In this paper we will consider the selection of the ANN variables of the next output, O = Ao."
        },
        {
            "chunk_id": 79,
            "summary": "In the case of temperature-dependent material response, the second case (1-NN2) allows to make predictions"
        },
        {
            "chunk_id": 80,
            "summary": "We present a new class of artificial Neural Networks (ANNs) based on thermodynamics."
        },
        {
            "chunk_id": 81,
            "summary": "In this paper, we present a new framework for the study of networks."
        },
        {
            "chunk_id": 82,
            "summary": "TANNS are a new class of \"Stokes-Stokes neural networks\" (SNNs) that"
        },
        {
            "chunk_id": 83,
            "summary": "In this paper, we present a novel strategy for predicting the behaviour of materials using neural networks,"
        },
        {
            "chunk_id": 84,
            "summary": "We have developed a new approach to the study of chemical processes, based on the assumption that the"
        },
        {
            "chunk_id": 85,
            "summary": "TANNS is a novel strain-based model for the prediction of the final state of a"
        },
        {
            "chunk_id": 86,
            "summary": "This paper presents a solution to the problem of how to measure the change in a material's"
        },
        {
            "chunk_id": 87,
            "summary": "The primary outputs, O1, are outputs computed by differentiation of the neural network with respect to the 8 inputs"
        },
        {
            "chunk_id": 88,
            "summary": "We propose a new class of neural networks, in which the main interest of the network"
        },
        {
            "chunk_id": 89,
            "summary": "intrinsically satisfies the first principle of thermodynamics (and, as we shall see, the second principle"
        },
        {
            "chunk_id": 90,
            "summary": "This paper presents a new approach to the training and performance of artificial neural networks (ANNs"
        },
        {
            "chunk_id": 91,
            "summary": "At AE Aa 9 Figure 1: architecture of TANN."
        },
        {
            "chunk_id": 92,
            "summary": "The term artificial neural networks (ANNs) can be defined as \"a network of computers"
        },
        {
            "chunk_id": 93,
            "summary": "An ANN is a signal processing unit."
        },
        {
            "chunk_id": 94,
            "summary": "An ANN is a network composed of several layers, each with a hidden"
        },
        {
            "chunk_id": 95,
            "summary": "Deep learning is a branch of computer science that deals with the problem of learning"
        },
        {
            "chunk_id": 96,
            "summary": "- ,no (no is the number of outputs), the signal flows from layer"
        },
        {
            "chunk_id": 97,
            "summary": "The number of neurons in a layer is proportional to the number of neurons in the layer"
        },
        {
            "chunk_id": 98,
            "summary": "Figure 2 shows the results of the equation for the Hilbert space."
        },
        {
            "chunk_id": 99,
            "summary": "In this paper, a regression problem for the activation function of the output layer, A(out"
        },
        {
            "chunk_id": 100,
            "summary": "The following table shows the relationship between the benchmark, 0, and prediction, O, that"
        },
        {
            "chunk_id": 101,
            "summary": "A loss function for the output layer of a Monte Carlo Monte Carlo simulation is used to calculate the errors related to the"
        },
        {
            "chunk_id": 102,
            "summary": "A loss function is a set of values, i.e."
        },
        {
            "chunk_id": 103,
            "summary": "The artificial neural network (ANN) is a machine-learning technique used to train neural networks."
        },
        {
            "chunk_id": 104,
            "summary": "In this paper, an artificial neural network (ANN) is trained with a set of input-output"
        },
        {
            "chunk_id": 105,
            "summary": "An adjoint network (ANN) is a branch of adjoint theory that deals with the"
        },
        {
            "chunk_id": 106,
            "summary": "The 10 P 1 output layer layer hidden layer function is a function of the 10"
        },
        {
            "chunk_id": 107,
            "summary": "In our paper, we present a solution to a well-known problem in artificial neural networks (ANNs)."
        },
        {
            "chunk_id": 108,
            "summary": "The first-order vanishing gradient is a function of the loss function with respect to a certain weight."
        },
        {
            "chunk_id": 109,
            "summary": "The most common activation A A gradient (Géron, 2019)."
        },
        {
            "chunk_id": 110,
            "summary": "Some of the most common activation functions and their first-order gradients."
        },
        {
            "chunk_id": 111,
            "summary": "The functions and their derivatives-that is, the logistic (sigmoid) function,"
        },
        {
            "chunk_id": 112,
            "summary": "First-order gradient (backpropagation) assumes values much smaller than 1."
        },
        {
            "chunk_id": 113,
            "summary": "In this paper we present a new hyperbolic activation function for the sigmoid network."
        },
        {
            "chunk_id": 114,
            "summary": "In this paper we present a new function, ReLU, which can be used to control the vanishing gradient of a"
        },
        {
            "chunk_id": 115,
            "summary": "The brain's ability to control emotion is governed by a set of instructions called ELUs,"
        },
        {
            "chunk_id": 116,
            "summary": "The following example shows how to deal with the vanishing gradient problem of Z  0 in Mathematica."
        },
        {
            "chunk_id": 117,
            "summary": "In this paper, we will look at vanishing gradients in TANNS."
        },
        {
            "chunk_id": 118,
            "summary": "In this paper we are going to look at a hidden layer of the periodic table."
        },
        {
            "chunk_id": 119,
            "summary": "The activation function of the single output layer, which returns 3?, is assumed to be linear"
        },
        {
            "chunk_id": 120,
            "summary": "In this paper, we show how to use the ANN to compute the output (b) of a given"
        },
        {
            "chunk_id": 121,
            "summary": "The following is a description of the loss function L= WoLo + Wviolv,0, where Lo"
        },
        {
            "chunk_id": 122,
            "summary": "In this paper, we present the results of our work on a new class of loss function called ANN, which"
        },
        {
            "chunk_id": 123,
            "summary": "The gradient descent algorithm needs the following properties:"
        },
        {
            "chunk_id": 124,
            "summary": "This paper presents a direct computation of the error between the outputs of the activation function A and the inputs"
        },
        {
            "chunk_id": 125,
            "summary": "In this lecture, I will show you how to solve a classical vanishing problem in an ANN."
        },
        {
            "chunk_id": 126,
            "summary": "Second-order vanishing gradients are one of the most common class of problems encountered in programming languages."
        },
        {
            "chunk_id": 127,
            "summary": "In this paper we describe the architecture and the internal steps/definitions "
        },
        {
            "chunk_id": 128,
            "summary": "The tensor adversarial network (TANN) is a set of equations that can be used to predict the behaviour of"
        },
        {
            "chunk_id": 129,
            "summary": "Key words: kinematics, temperature, potential"
        },
        {
            "chunk_id": 130,
            "summary": "The following table shows the results of the following:"
        },
        {
            "chunk_id": 131,
            "summary": "In this paper, we present a novel architecture for the control of isothermal"
        },
        {
            "chunk_id": 132,
            "summary": "The following table shows the relationships between ANNs and SNNy."
        },
        {
            "chunk_id": 133,
            "summary": "SNNe, SNNF and SNNe-S are the three main solvers for the Euler"
        },
        {
            "chunk_id": 134,
            "summary": "Researchers at the University of California, Berkeley, have developed a new model for predicting the behaviour of"
        },
        {
            "chunk_id": 135,
            "summary": "A new sub-network of the Helmholtz-Zentrum Mnchen (SNNF"
        },
        {
            "chunk_id": 136,
            "summary": "The aim of this paper is to provide an overview of the state-of-the-art in the"
        },
        {
            "chunk_id": 137,
            "summary": "In this paper, we show how TANNS can overcome some of the challenges faced by artificial neural networks"
        },
        {
            "chunk_id": 138,
            "summary": "In this paper, we derive a set of predictions for the thermodynamics of two pse"
        },
        {
            "chunk_id": 139,
            "summary": "In this paper, we report on the performance of TANNS, a machine-"
        },
        {
            "chunk_id": 140,
            "summary": "The results of this study are presented in four parts:"
        },
        {
            "chunk_id": 141,
            "summary": "In this paper, we present a new approach to the problem of predicting the behaviour of hyper"
        },
        {
            "chunk_id": 142,
            "summary": "A procedure is used to generate data for kinematic hardening (Houlsby and Puzrin, 2000)."
        },
        {
            "chunk_id": 143,
            "summary": "In this paper, we show that the TANN architecture still holds even in materials for which the Ziegler"
        },
        {
            "chunk_id": 144,
            "summary": "The results of this study will be presented at the annual meeting of the American Physical Society (APS) in"
        },
        {
            "chunk_id": 145,
            "summary": "In this paper, we consider the case of a hypo-plastic material with a smooth response to TANNS encoded"
        },
        {
            "chunk_id": 146,
            "summary": "A new framework for the study of thermo-plasticity has been proposed."
        },
        {
            "chunk_id": 147,
            "summary": "We obtain the following non-linear incremental relations d=0eF-ét ZaF-5 (22a"
        },
        {
            "chunk_id": 148,
            "summary": "In this paper, we introduce a new theory of the Bose-Einstein condensate, in which the Bose-Einstein condensate"
        },
        {
            "chunk_id": 149,
            "summary": "The following table shows the relationship between the following indices:"
        },
        {
            "chunk_id": 150,
            "summary": "quantity (scalar or tensorial, depending on the dimensionality of the internal"
        },
        {
            "chunk_id": 151,
            "summary": "This paper presents a new theoretical model for the derivation of incremental material formulations with hypo-plasticity."
        },
        {
            "chunk_id": 152,
            "summary": "In this paper, we present a new von Mises model for the relaxation strain rate and stress increment of a"
        },
        {
            "chunk_id": 153,
            "summary": "The Euler equation for the elastic limit of a material is as follows:"
        },
        {
            "chunk_id": 154,
            "summary": "The following table lists the tensors used in this work."
        },
        {
            "chunk_id": 155,
            "summary": "The aim of this project is to develop a method for the generation of strains and stresses of hyer-plasticity"
        },
        {
            "chunk_id": 156,
            "summary": "This paper presents a new approach to the study of the buckling behaviour of porous materials."
        },
        {
            "chunk_id": 157,
            "summary": "This paper presents a new method for obtaining the state at the new time t + At, i"
        },
        {
            "chunk_id": 158,
            "summary": "The results of this study show that it is possible to predict the state of a material using"
        },
        {
            "chunk_id": 159,
            "summary": "This paper presents the results of a large-scale training operation on an artificial neural network (ANN"
        },
        {
            "chunk_id": 160,
            "summary": "The artificial neural network (ANN) has been used for many years to study the behaviour of"
        },
        {
            "chunk_id": 161,
            "summary": "strain-stress loading paths are used to train neural networks."
        },
        {
            "chunk_id": 162,
            "summary": "In this paper, we show how to improve the representativeness of artificial neural networks (ANNs) by generating"
        },
        {
            "chunk_id": 163,
            "summary": "In this paper, the initial state, E, and the strain increment, Emaxi, are"
        },
        {
            "chunk_id": 164,
            "summary": "In this paper, a constraint y' S 0 is used to constrain the coefficients of the coefficients of"
        },
        {
            "chunk_id": 165,
            "summary": "The properties of two-dimensional (3D) amorphous silicon nitride (ASI) have been investigated using"
        },
        {
            "chunk_id": 166,
            "summary": "The results of this study have been published in the Journal of the American Meteorological Society."
        },
        {
            "chunk_id": 167,
            "summary": "Strain Arain - Strain Arain 600- - Sval Kval 600 D Str 400 D Arain J 400"
        },
        {
            "chunk_id": 168,
            "summary": "Motorists in Northern Ireland are being urged to take care on the roads over the Easter weekend."
        },
        {
            "chunk_id": 169,
            "summary": "The latest weather data for the weekend in central and eastern Poland, as compiled by"
        },
        {
            "chunk_id": 170,
            "summary": "Figure 7: Sampling for material case H-1 (cf."
        },
        {
            "chunk_id": 171,
            "summary": "A new class of multi-dimensional elasto-plastic materials has been proposed and"
        },
        {
            "chunk_id": 172,
            "summary": "hyper- plasticity is a state-of-the-art technique for the study of plasticity in materials."
        },
        {
            "chunk_id": 173,
            "summary": "In paragraph 4.2, we introduce a new class of plastic material models, called hypo-plastic material models."
        },
        {
            "chunk_id": 174,
            "summary": "In this paper, we present a novel method for predicting the state-of-the-"
        },
        {
            "chunk_id": 175,
            "summary": "We use early-stopping to reduce the learning error on the set of test patterns, per each trial choice"
        },
        {
            "chunk_id": 176,
            "summary": "The aim of this Section is to provide a set of examples of how deep neural networks can be used to train"
        },
        {
            "chunk_id": 177,
            "summary": "In this paper, we discuss the performance of topological acoustic networks (TANNs)."
        },
        {
            "chunk_id": 178,
            "summary": "The coefficients of the shear parameters K, G, Ep and Vz are obtained from the coefficients of the"
        },
        {
            "chunk_id": 179,
            "summary": "The strain tensors e and Z are used to derive the yield surface of a plastic deformation model."
        },
        {
            "chunk_id": 180,
            "summary": "Material parameters for 3D elasto-plastic von Mises material."
        },
        {
            "chunk_id": 181,
            "summary": "This paper describes the derivation of a deformable deformation network based on a finite element method."
        },
        {
            "chunk_id": 182,
            "summary": "In this paper we present the results of a three-dimensional (3D) finite element simulation of a"
        },
        {
            "chunk_id": 183,
            "summary": "Table 1 shows the mean standard deviation and maximum values of the training data-sets."
        },
        {
            "chunk_id": 184,
            "summary": "The following table shows the mean absolute error (MAE) and Mean Square Error"
        },
        {
            "chunk_id": 185,
            "summary": "The coefficients of the loss functions are obtained by dividing the coefficients of the loss functions"
        },
        {
            "chunk_id": 186,
            "summary": "A novel sub-network Neuronal Network (SNN) has been proposed for the study of"
        },
        {
            "chunk_id": 187,
            "summary": "In this paper, we present a novel model of two sub-networks, one"
        },
        {
            "chunk_id": 188,
            "summary": "The loss functions of each output as the training is performed, for material case H-1 (perfect plasticity"
        },
        {
            "chunk_id": 189,
            "summary": "In this paper we show how to recover the \"hardening\" and \"softening\" properties of a set of"
        },
        {
            "chunk_id": 190,
            "summary": "TANNs can be trained to predict the response of a material to a random loading path."
        },
        {
            "chunk_id": 191,
            "summary": "In this paper, we show how to train a Markov chain Monte Carlo model on a large set of data-sets"
        },
        {
            "chunk_id": 192,
            "summary": ", , , , , , "
        },
        {
            "chunk_id": 193,
            "summary": "Figure 8: Errors in terms of the adimensional Mean Absolute Error (MAE) of the predictions of TANN ("
        },
        {
            "chunk_id": 194,
            "summary": "In this paper we introduce a new type of artificial intelligence (ANN) called TANN"
        },
        {
            "chunk_id": 195,
            "summary": "In this paper, we show how to select the best architecture for an artificial intelligence (AI) network."
        },
        {
            "chunk_id": 196,
            "summary": "Researchers at the University of California, Berkeley, have developed an artificial neural network (ANN) that"
        },
        {
            "chunk_id": 197,
            "summary": "In this paper we present a novel method for training and validation of neural networks."
        },
        {
            "chunk_id": 198,
            "summary": "In this paper, we investigate the potential of artificial intelligence networks (ANNs) and artificial neural networks (T"
        },
        {
            "chunk_id": 199,
            "summary": "The following table presents the results of a series of experiments on the effects of stress on the performance of the TANN"
        },
        {
            "chunk_id": 200,
            "summary": "Figure 9: Predictions of a uni-axial random loading path, compared with the target of"
        },
        {
            "chunk_id": 201,
            "summary": "This paper presents a new approach to the study of superconductivity, based on the idea that super"
        },
        {
            "chunk_id": 202,
            "summary": "In this paper, we develop a novel elastic material response network (TANNS) based on"
        },
        {
            "chunk_id": 203,
            "summary": "In this paper, we present a new approach to predict elastic deformations, based on the"
        },
        {
            "chunk_id": 204,
            "summary": "In this paper, we compare the performance of two different ANNS networks, one in cyclic mode and one in recall"
        },
        {
            "chunk_id": 205,
            "summary": "Figure 10: (a) and full architecture of the network, not based on thermodynamics, standard ANNS."
        },
        {
            "chunk_id": 206,
            "summary": "The results of the 10th ANN Adival, which was held at the"
        },
        {
            "chunk_id": 207,
            "summary": "The training and validation of artificial neural networks (ANNs) were compared with the"
        },
        {
            "chunk_id": 208,
            "summary": "The results of a study on the buckling behaviour of aluminium sheets have been published in"
        },
        {
            "chunk_id": 209,
            "summary": "We have computed the free-energy and dissipation rates of two elastic strains from the results of"
        },
        {
            "chunk_id": 210,
            "summary": "This study investigates the effects of artificial neural networks (ANNs) and artificial intelligence (AI"
        },
        {
            "chunk_id": 211,
            "summary": "In this paper, we show that standard artificial neural networks (ANNs) can predict the"
        },
        {
            "chunk_id": 212,
            "summary": "The aim of this paper is to improve the performance of ANNs in"
        },
        {
            "chunk_id": 213,
            "summary": "In this paper, I present a new class of artificial intelligence (ANNs)"
        },
        {
            "chunk_id": 214,
            "summary": "In this paper we present a new model for the dissipation of heat from a nuclear reactor."
        },
        {
            "chunk_id": 215,
            "summary": "In this paper, we show that ANNs can be used to predict the response of"
        },
        {
            "chunk_id": 216,
            "summary": "In this paper, we present the results of a uni-axial loading scenario, in which the"
        },
        {
            "chunk_id": 217,
            "summary": "The following table shows the results of TANNS and ANNs for different material cases."
        },
        {
            "chunk_id": 218,
            "summary": "In this paper, TANNS is presented for the first time the prediction of the dissipation rate of"
        },
        {
            "chunk_id": 219,
            "summary": "In this paper, we show how to use tensor artificial neural networks (TANNs) and"
        },
        {
            "chunk_id": 220,
            "summary": "In this paper, we develop two new approaches for assuring thermodynamically consistent quantities: one, i.e."
        },
        {
            "chunk_id": 221,
            "summary": "Model TANN ANN 51 () le-3 Ep 6p (-) le-3 Suku (-)"
        },
        {
            "chunk_id": 222,
            "summary": "strain increment As = 1 X 10-2 strain increment As = 1 X 10-2 strain increment As = 1 X 10"
        },
        {
            "chunk_id": 223,
            "summary": "Figure 12: Comparison of the stress, energy, and dissipation predictions of TANNS and standard "
        },
        {
            "chunk_id": 224,
            "summary": "This table shows the energy and dissipation values for the cyclic, isotropic loading path of the ANNS"
        },
        {
            "chunk_id": 225,
            "summary": "(a) strain increment As = 1 X 10-4."
        },
        {
            "chunk_id": 226,
            "summary": "le1 1.25 model 1 model 1.00 TANN TANN ANN Ea 0.5 50.25 6 0.00 0.0 -1"
        },
        {
            "chunk_id": 227,
            "summary": "le5 model model 6 TANN 6- TANN ANN 4 a 6 2 6 -1.0 -0.5 "
        },
        {
            "chunk_id": 228,
            "summary": "Figure 13: Comparison of the stress predictions of TANNS and standard ANNs with respect to"
        },
        {
            "chunk_id": 229,
            "summary": "This table lists all the models used to predict the likelihood of the UK leaving the European Union."
        },
        {
            "chunk_id": 230,
            "summary": "strain increment As = 1 X 10-3. le-3 (-) le-3 (b) strain increment As = 1"
        },
        {
            "chunk_id": 231,
            "summary": "strain increment As = 1 X 10-2 strain increment As = 1 X 10-2 strain increment As = 1 X"
        },
        {
            "chunk_id": 232,
            "summary": "Figure 14: Comparison of the internal variable predictions of TANNS and standard ANNs with respect"
        },
        {
            "chunk_id": 233,
            "summary": "Predicting the outcome of the UK general election."
        },
        {
            "chunk_id": 234,
            "summary": "strain increment As = 1 X 10-4 strain increment As = 1 X 10-4 strain increment As = 1 X"
        },
        {
            "chunk_id": 235,
            "summary": "le6 le3 1.50 model 1.25 TANN"
        },
        {
            "chunk_id": 236,
            "summary": "strain increment A model TANN 0.00-6 ANN 0.00 0.25 0.50 0.75 1.00 1.25 "
        },
        {
            "chunk_id": 237,
            "summary": "The energy and dissipation rate predictions of TANNS and computation according to Eq. (5.1) for standard AN"
        },
        {
            "chunk_id": 238,
            "summary": "In our paper, we show how TANNS can be used to predict the dissipation rate and energy potential"
        },
        {
            "chunk_id": 239,
            "summary": "The following tables show the distributions of the coefficients of the coefficients of the coefficients of the coefficients"
        },
        {
            "chunk_id": 240,
            "summary": "The aim of this study is to investigate the effects of hyper-plasticity on the performance of"
        },
        {
            "chunk_id": 241,
            "summary": "The TANNS model for the buckling and deformation of a hyper- plastic case has been"
        },
        {
            "chunk_id": 242,
            "summary": "In this paper, we discuss the choice of the inelastic strain as the internal variable in"
        },
        {
            "chunk_id": 243,
            "summary": "In this paper, we present a training scheme for the convergence of a set of four (adimensional)"
        },
        {
            "chunk_id": 244,
            "summary": "In this paper, we investigate the performance of topologicalANNs (TANNS) with respect to standard"
        },
        {
            "chunk_id": 245,
            "summary": "TANNs' predictions for the following bi-axial loading path ni AEi = -AE2 ="
        },
        {
            "chunk_id": 246,
            "summary": "The results of this study are very encouraging."
        },
        {
            "chunk_id": 247,
            "summary": "          "
        },
        {
            "chunk_id": 248,
            "summary": "The stress and internal variable predictions of TANNS and standard ANNs with respect to the target values, for"
        },
        {
            "chunk_id": 249,
            "summary": "The strain-controlled ratcheting (TANNS) method has been developed for the control of"
        },
        {
            "chunk_id": 250,
            "summary": "In this paper, we show that TANNS, a new type of finite element model, is more accurate than"
        },
        {
            "chunk_id": 251,
            "summary": "le-3 le-3 model model 61 TANN 2 TANN ANN 0 E  -2 a U S"
        },
        {
            "chunk_id": 252,
            "summary": "Figure 17: Comparison of the predictions of TANNS and standard ANNs with respect"
        },
        {
            "chunk_id": 253,
            "summary": "In this paper, we report the results of a new approach to the design of high-performance materials,"
        },
        {
            "chunk_id": 254,
            "summary": "In this paper we present a new method for measuring the performance of artificial neural networks (ANNs"
        },
        {
            "chunk_id": 255,
            "summary": "In this paper, we derive an additive noise model for the emission of carbon dioxide (CO"
        },
        {
            "chunk_id": 256,
            "summary": "The coefficients of two noise distributions are given in the table below."
        },
        {
            "chunk_id": 257,
            "summary": "The results of this study are as follows:"
        },
        {
            "chunk_id": 258,
            "summary": "In this paper, we study the effect of noise on the training of TANNS."
        },
        {
            "chunk_id": 259,
            "summary": "In this paper, we present a novel approach to the computation of gear values in a T"
        },
        {
            "chunk_id": 260,
            "summary": "The results of this study are similar to those of Section 4."
        },
        {
            "chunk_id": 261,
            "summary": "In this paper, we study the effects of noise on the training of time-varying neural networks (TANN"
        },
        {
            "chunk_id": 262,
            "summary": "Table 2 shows the MAEs of the predictions of TANNS with respect to the training"
        },
        {
            "chunk_id": 263,
            "summary": "In this paper we show that TANNS are unable to learn the noised signal, i.e"
        },
        {
            "chunk_id": 264,
            "summary": "In this paper, we show how a network of computers can be used to predict the energy potential of a"
        },
        {
            "chunk_id": 265,
            "summary": "In this paper, we present the results of a noised training and validation study of the 33 le-1 A"
        },
        {
            "chunk_id": 266,
            "summary": "Noise is added in stress to both training and validation data-sets, case (2)."
        },
        {
            "chunk_id": 267,
            "summary": "Table 4: MAEs of the predictions of TANNS with respect to the validation data-sets, for"
        },
        {
            "chunk_id": 268,
            "summary": "The following table shows the coefficients of the coefficients of the coefficients of the coefficients of the coefficients of the"
        },
        {
            "chunk_id": 269,
            "summary": "This graph shows the training error of the network's learning process in relation to the measurement noise."
        },
        {
            "chunk_id": 270,
            "summary": "In this paper, we present a new method for predicting the inelastic strains,"
        },
        {
            "chunk_id": 271,
            "summary": "In our paper, we show that the predictions of standard ANNs with respect to the validation data-"
        },
        {
            "chunk_id": 272,
            "summary": "In this paper, we study the effect of noisy training on the performance of an artificial neural network (ANN) trained"
        },
        {
            "chunk_id": 273,
            "summary": "Mean Absolute Error (MAES) of the predictions of standard ANNs with respect to the validation"
        },
        {
            "chunk_id": 274,
            "summary": "In this paper we present the results of a comparison of the predictions in recall mode of two unsupervised"
        },
        {
            "chunk_id": 275,
            "summary": "In this paper, we study the effect of noise on the performance of artificial neural networks (ANNs)."
        },
        {
            "chunk_id": 276,
            "summary": "In this paper, we show that artificial neural networks (ANNs) fail to minimize the loss function of"
        },
        {
            "chunk_id": 277,
            "summary": "In this paper, we show that the robustness of the ANN model is dependent on the"
        },
        {
            "chunk_id": 278,
            "summary": "In this paper, we present a novel approach to noise measurement, which is based on the idea of"
        },
        {
            "chunk_id": 279,
            "summary": "In this paper, we show that artificial intelligence (AI) can be used to predict the behaviour"
        },
        {
            "chunk_id": 280,
            "summary": "In our group, we have developed a new method for the automatic differentiation of neural networks"
        },
        {
            "chunk_id": 281,
            "summary": "This paper presents a novel approach to the design and activation of Neural Networks (ANNs)."
        },
        {
            "chunk_id": 282,
            "summary": "In this paper, a new problem has been presented in the field of differential equations of matter."
        },
        {
            "chunk_id": 283,
            "summary": "BBC Sport takes a look at some of the most popular sports cars of all time."
        },
        {
            "chunk_id": 284,
            "summary": "strain increment As = 1 X (a) strain increment As = 1 X (b) strain increment As = 1"
        },
        {
            "chunk_id": 285,
            "summary": "Figure 19: Influence of noise in the stress, Oi, for the predictions of the stress,"
        },
        {
            "chunk_id": 286,
            "summary": "This paper presents the results of a new approach to the theory of noise, called TANN, which"
        },
        {
            "chunk_id": 287,
            "summary": "Researchers at the University of California, Berkeley, and the Massachusetts Institute of Technology (MIT)"
        },
        {
            "chunk_id": 288,
            "summary": "We have developed a new type of artificial intelligence (ANN) that can predict the"
        },
        {
            "chunk_id": 289,
            "summary": "In this paper, we present a novel approach to the prediction of noise levels in a noisy environment, which is"
        },
        {
            "chunk_id": 290,
            "summary": "In this paper, we show how to train a new type of finite-element network, called tensor-"
        },
        {
            "chunk_id": 291,
            "summary": "We show that the Jacobian annealing entropy (AE) at the point level is better predicted than the Jacobian"
        },
        {
            "chunk_id": 292,
            "summary": "In this paper, we investigated the effects of noise on the training of TANNS, a"
        },
        {
            "chunk_id": 293,
            "summary": "TANN is a novel approach to the study of solid-state heat transfer."
        },
        {
            "chunk_id": 294,
            "summary": "The authors would like to acknowledge the support of the European Research Council (ERC) under the European Union Horizon"
        },
        {
            "chunk_id": 295,
            "summary": "In this paper, we investigate the performance and influence of different activation functions on"
        },
        {
            "chunk_id": 296,
            "summary": "The ANN has one hidden layer, with N, = 6 nodes,"
        },
        {
            "chunk_id": 297,
            "summary": "The coefficients of the coefficients of the coefficients of the coefficients of the coefficients of the coefficients of the coefficients of the"
        },
        {
            "chunk_id": 298,
            "summary": "In this paper we show how to train a gradient descent algorithm using a loss function E."
        },
        {
            "chunk_id": 299,
            "summary": "Table 5: Set of activation functions considered to investigate the performance of the network with outputs 0= x and VIo"
        },
        {
            "chunk_id": 300,
            "summary": "Function Z range S(z) '(z)'"
        },
        {
            "chunk_id": 301,
            "summary": "For each tested activation function, Table 6 shows the adimensional Mean Absolute Error."
        },
        {
            "chunk_id": 302,
            "summary": "This paper presents the development of a new training algorithm, i.e."
        },
        {
            "chunk_id": 303,
            "summary": "In this paper, we show how activation functions can be trained to learn a new form of"
        },
        {
            "chunk_id": 304,
            "summary": "In this paper we compare the predictions of the ANN for a selection of activation functions"
        },
        {
            "chunk_id": 305,
            "summary": "The results of the ELUZ predictions are published in the journal Physical Review Letters."
        },
        {
            "chunk_id": 306,
            "summary": "Activation function SA L Lo Lv,o no."
        },
        {
            "chunk_id": 307,
            "summary": "ELU40S, ReLU, ELUe, ELUOSB4z, ELUA, Re"
        },
        {
            "chunk_id": 308,
            "summary": "ReLU, ReLU, ELU, 20.15 0.25 0 0.14 C' 0 -0.25 0.05 "
        },
        {
            "chunk_id": 309,
            "summary": "Match predictions for the weekend's Premier League and Championship games."
        },
        {
            "chunk_id": 310,
            "summary": "(-) le-1 le-2 le-2 1.0 2 3 2z 0.8 ELU,"
        },
        {
            "chunk_id": 311,
            "summary": "Figure 20: Comparison of different activation functions for the prediction of the primary output, x"
        },
        {
            "chunk_id": 312,
            "summary": "In this paper, we derive the following:"
        },
        {
            "chunk_id": 313,
            "summary": "following non-linear incremental relations  = OsF-g+ > OEAF-5k + OF (28"
        },
        {
            "chunk_id": 314,
            "summary": "The dissipative stresses X+ = (X1, XN) with aD X,= ViE"
        },
        {
            "chunk_id": 315,
            "summary": "The first-order function in 5i the Legendre transformre,  to Xi, is"
        },
        {
            "chunk_id": 316,
            "summary": "The following flow rules are derived from the Legendre transform."
        },
        {
            "chunk_id": 317,
            "summary": "The following equation is used to solve the following problem:"
        },
        {
            "chunk_id": 318,
            "summary": "In this paper, we deal with the following:"
        },
        {
            "chunk_id": 319,
            "summary": "The coefficients of the Mos Moo equation, used to determine the coefficients of the coefficients of the coefficients of the coefficients of"
        },
        {
            "chunk_id": 320,
            "summary": "The simplest way to find out the age of a child is to use the following equation:"
        },
        {
            "chunk_id": 321,
            "summary": "In: COMPLAS 2019: XV International Conference on Computational Plasticity: Fundamentals and Applications, International Centre for Numerical"
        },
        {
            "chunk_id": 322,
            "summary": "Two-scale modelling of granular materials: a DEM-FEM approach."
        },
        {
            "chunk_id": 323,
            "summary": "In: N. Bakhvalov, G. Panasenko, Homogenisation: Aver"
        },
        {
            "chunk_id": 324,
            "summary": "In: Einav, I., Houlsby, G."
        },
        {
            "chunk_id": 325,
            "summary": "The aim of this project is to develop a new approach to the theory of plasticity, based on"
        },
        {
            "chunk_id": 326,
            "summary": "A study on the effects of an explosion in the Pantheon of Rome."
        },
        {
            "chunk_id": 327,
            "summary": "The importance ofmechanical couplings and microstructure to strain in 3d structures with application"
        },
        {
            "chunk_id": 328,
            "summary": "The importance of couplings and microstructure to strain in 3da with application to seismic faults."
        },
        {
            "chunk_id": 329,
            "summary": "i: A cosserat breakage mechanics model for brittle, granular media, Journal of the Mechanics and Physics of"
        },
        {
            "chunk_id": 330,
            "summary": "Modelling the strain around an underground gallery with a hydro-mechanical scale model ; effect of anisotropy"
        },
        {
            "chunk_id": 331,
            "summary": "In: Mitchell, T.M., and Cybenko, G."
        },
        {
            "chunk_id": 332,
            "summary": "Ghaboussi, J., Garrett, H."
        },
        {
            "chunk_id": 333,
            "summary": "Artificial neural networks for modelling finite element code."
        },
        {
            "chunk_id": 334,
            "summary": "The deformation of open-cell foam structures using neural networks."
        },
        {
            "chunk_id": 335,
            "summary": "Researchers at the University of California, Los Angeles (UCLA), have been working on the development of"
        },
        {
            "chunk_id": 336,
            "summary": "In: Xu K., Darve E."
        },
        {
            "chunk_id": 337,
            "summary": "Researchers at the Massachusetts Institute of Technology (MIT) have developed a novel method for learning from indirect observations using deep"
        },
        {
            "chunk_id": 338,
            "summary": "In: Gorji, M., Mozaffar, M., Cao, J., Heidenreich"
        },
        {
            "chunk_id": 339,
            "summary": "Accelerating multiscale finite element simulations of history-dependent materials using a recurrent neural network."
        },
        {
            "chunk_id": 340,
            "summary": "Predicting the mechanical response of oligocrystals."
        },
        {
            "chunk_id": 341,
            "summary": "D. Gonzalez, F. Cueto, Learning corrections for hyperelastic models from data, Frontiers"
        },
        {
            "chunk_id": 342,
            "summary": "Federico Chinesta, T. Kirchdoerfer, M. Ortiz, Data-driven computational mechanics, Computer Methods"
        },
        {
            "chunk_id": 343,
            "summary": "A data-driven computing approach to data-driven computational elasticity and inelasticity, International Journal for Numerical Methods in"
        },
        {
            "chunk_id": 344,
            "summary": "Stefan Reese, \"Model-free data-driven inelasticity, Computer Methods in Applied Mechanics"
        },
        {
            "chunk_id": 345,
            "summary": "Baydin, A. G., Pearlmutter, B. A., Radul, A"
        },
        {
            "chunk_id": 346,
            "summary": "In: Maugin, G. A., and Muschik, W."
        },
        {
            "chunk_id": 347,
            "summary": "Y. H Hu, J.-N. Hwang, Handbook of neural network processing, Handbook of Machine Learning"
        },
        {
            "chunk_id": 348,
            "summary": "A framework for data-driven analysis of materials uncertainty: Countering the curse of dimensionality."
        },
        {
            "chunk_id": 349,
            "summary": "In: Karpatne, A., Watkins, W., Read, V., Kumar, V"
        },
        {
            "chunk_id": 350,
            "summary": "A look back at some of the key moments in"
        }
    ]
}