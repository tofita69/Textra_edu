{
    "chunks": [
        "UNIVERSITE DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématique et Informatique Année universitaire : 2018-2019 Interrogatoire : Théorie et pratique du Datamining Durée : 75 mn Exercice 01 (03 pts : 10 Mn) : Répondez brièvement aux questions suivantes :",
        "1. Expliquer le fonctionnement de la méthode de rééchantionnage Bootstrap ?. 2. Plusieurs domaines sont la base des techniques de Datamining. Citez cinq domaines ? 3. Es ce qu'une règle d' association avec un support et une confiance acceptable veut dire que cette règle",
        "est pertinente ? expliquez notamment avec des exemples ? Exercice 02 (08 pts : 40 Mn) : Le tableau suivant contient des données sur les No Doublant Série Mention Classe résultats obtenus par des étudiants de Tronc Commun 01 Non Maths ABien Admis",
        "(première année à Université). Chaque étudiant est 02 Non Techniques ABien Admis décrit par 3 attributs : Est-il doublant ou non, la série 03 Oui Sciences ABien Non Admis du Baccalauréat obtenu et la mention. Les étudiants 04 Oui Sciences Bien Admis",
        "sont répartis en deux classes : Admis et Non Admis. 05 Non Maths Bien Admis On veut construire un arbre de décision à partir des 06 Non Techniques Bien Admis données du tableau, pour rendre compte des éléments 07 Oui Sciences Passable Non Admis",
        "qui influent sur les résultats des étudiants en Tronc 08 Oui Maths Passable Non Admis Commun. Les lignes de 1 à 12 sont utilisées comme 09 Oui Techniques Passable Non Admis données d'apprentissage. Les lignes restantes (de 13 à 10 Oui Maths TBien Admis 11 Oui Techniques TBien Admis",
        "16) sont utilisées comme données de tests. 12 Non Sciences TBien Admis 1. Utiliser les données d'apprentissage pour 13 Oui Maths Bien Admis construire I'(les) arbre(s) de décision en utilisant 14 Non Sciences ABien Non Admis l'algorithme ID3. Montrez toutes les étapes et 15 Non Maths TBien Admis",
        "formules de calcul. Dessinez l'arbre final. 16 Non Maths Passable Non Admis 2. Déduire de l'arbre trouvé la petite règle correspondante. 3. Classer l'instance N°17: Doublant-Oui, Série-Maths, Mention-ABien. Que remarquez-vous ?",
        "4. Quels sont les résultats de test de l'arbre obtenu sur les données de tests ? déduisez le taux d'erreur ? En comparant les résultats obtenus, que suggérez-vous concernant l'arbre résultante ?",
        "5. En se basant sur la comparaison et la suggestion de la question 5, que pouvez dire sur la prédiction de l'avenir des étudiants de tronc commun par rapport aux résultats obtenus au baccalauréat. Exercice 03 (04 pts : 10 Mn). :",
        "Soit l'ensemble D des entiers suivants : D= £2,5,8, 10, 11, 18, 20 j. On veut répartir les données de D en trois (3) clusters en utilisant l'algorithme Kmeans et la distance de manhathan",
        "1/ Appliquez Kmeans en choisissant comme centres initiaux des 3 clusters respectivement : 8, 10 et 11. Montrez toutes les étapes de calcul. 2/ Donnez le résultat final et précisez le nombre d'itérations qui ont été nécessaires. 3/ Peut-on avoir un nombre d'itérations inférieur pour ce problème ? Discutez. Good",
        "To succeed in life one must have the courage to pursue what he wants CK Enseignant : M' K. Boudjebbour Page 1/1 1 UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématiques et Informatique Année universitaire : 2018-2019",
        "Corrigé type EFS S1 : Théorie et pratique du Datamining Exercice 01 (06 Pts) : a) Appliquer la classification hiérarchique ascendante sur l'ensemble X X 1 2 9 12 20 On va utilisé la distance de Manhattan entre instances : D(X,Y) = Li-Xi - Yil",
        "Et la distance minimale entre toutes les paires de données des 2 clusters (single link method) : Dsinglel(i.j) = MinxEi y€j D(X,Y) 0.5.PE Les tableaux suivants représentent les différentes distances Dsingle entre différents clusters : Etape 1 : 1 2 9 12 20 1 1 8 11 19 2",
        "7 10 18 Regroupement des clusters (1) et [2) en (1,2) 9 3 11 12 8 Etape 2 : 1,2 9 12 20 3.Pt 1,2 7 10 18 Regroupement des clusters [9) et (12) en [9,12) 9 3 11 12 8 Etape 3: 1,2 9,12 20 1,2 7 18",
        "Regroupement des clusters (1,2) et [9,12) en (1,2,9,12) 9,12 8 Etape 4 : 1,2,9,12 20 1,2,9,12 8 Regroupement des clusters (1,2,9,12) et (20) en (1,2,9,12,20) Dsingle Dendrogramme : b) L'inertie intra-cluster IA = Ek ENK D?(i,Gk) i: instance ; Gk : centroid du groupe k ;",
        "Nk : Nombre d'instance du groupe k 1Pt Un regroupement en 2 clusters : CI=(1,2,9,12) centroid C1 =6 6 C2=(20) centroid C2 = 20 1.5.Pti IA= ((1-6)2+ (2-6)2+ (9-6)2+ (12-6)2)+ (20-20)2-86 Données 1 2 9 12 20 Uni regroupement en 3 clusters :",
        "Cl-(1,2)-centroid C1=1,5 C2-19,12)centroid C2 = 10,5 et C3-(20)centroid C3 = 20 IA= ((1-1,5)2+ 2-1,5)4(9-105)4 (12-10,5)2)+ (20-20)2-5 Donc le meilleur regroupement est celui de 3 clusters car son inertie intra-cluster IA est la plus petite. Enseignant : M' K. Boudjebbour Page 1/4 UNIVERSITE: DR < YAHIA FARES > DE MEDEA",
        "Faculté des Sciences Département de Mathématiques et Informatique Année universitaire : 2018-2019 Exercice 02 (09 Pts) : 11 11 7 7 1) On calcul l'entropie sur l'ensemble des données : I(11,7)= log log - 0,964 0,5PE 18 18 18 18 Ensuite on calcul le gain de chaque attribut : 6",
        "6 6 Gain (DegStr)= d1,72EDe5-0964G I(3, 3)+ I(3,3) + I(5,1))= 0,081 18 18 18 6 6 6 Gain (HrSom)= d1,7-Edlisom)-096H I(6,0)+ I(1,5) + I(4,2))= 0,441 1Pt 18 18 18 9 9 Gain (Fum)= d17-E(um)-0.964-G I(4,5)+ I(7,2))= 0,086 18 18",
        "Donc on choisit l'attribut < HrSom > avec le gain le plus grand (Gain-0.411) qui représente la racine de l'arbre, Donc l'arbre initial sera : HrSom 0.5.Pt Egal Supérieur Moins * Inst : 5, à 12 ??? - ??? Inst : 13 à 18 Yes",
        "Les valeurs Egal et Supérieur donnent deux valeurs de la classe, donc, il faut refaire le même travail (calcul du gain) pour l'ensemble des données S#13.4.9.10,15,16) et 515.611,2.7.8. I(SEg) =I(1,5)-0,650 2 Gain (SEg, DegStr)= I(1,5)-E(SEg, DegStr)= 0,650-C I(0,2)+ I(0,2) + 2 I(1,1))= 0,317 6 6 6 3 3",
        "Pt Gain (SEg: Fum)= I(1,5)-E(SEg Fum)= 0,650-C I(0,3)+ I(1,2))= 0,191 6 6 HrSom Donc on choisit l'attribut < DegStr > avec Egal Supérieur K a le gain le plus grand (Gain-0.317), et l'arbre devient : DegStr Moins y ??? Petit ou Normal Fort Yes Inst : 13 à 18 A",
        "No Fum Non Oui I(SSup) I(4,2)-0,919 4 No Yes Gain (SSup, DegStr)= I(4,2)-E(SSup, DegStr)= 0,919-614.1) 2 2 - I(1,1)+ - I(2,0))= 0,252 HrSom 6 6 Gain (SSup, Fum)= I(4,2)-E(SSup: Fum) 1Pt Egal Supérieur 3 3 Moins 4 = 0,919-C I(1,2)+ = I(3,0))= 0,495 DegStr Fum 6 6 Petit ou Normal",
        "Fort Yes Oui Non Donc on choisit l'attribut < Fum > avec - 4 K le gain le plus grand (Gain-0.495), No Fum Yes DegStr et l'arbre final devient : Non Oui Petit ou Normal Fort K A No Yes No Yes 2) Règle :",
        "(HrSom = Moins) ou ((HrSom + Moins) et (DegStr-Fort) et (Fum=Oui) ou ((Fum-Non) et (DegStr-Fort)) 1Pt Enseignant : M' K. Boudjebbour Page 2/4 UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématiques et Informatique Année universitaire : 2018-2019",
        "3) On applique l'ensemble test' T sur l'arbre de décision et on trouve la classe prédite : Instance DegStr HrSom Fum Classe réelle Classe prédite 19 Petit Supérieur Oui Yes Yes 20 Fort Superieur Non Yes Yes 21 Petit Egal Non No No 22 Fort Egal Non Yes No 23",
        "Normal Supérieur Oui No Yes 24 Petit Egal Oui No No Matrice de 1Pt Prédite (Yes) Prédite (No) Total confusion : Classe réelle (Yes) 2 1 3 Classe réelle (No) 1 2 3 Total 3 3 6",
        "Taux d'erreur = b+c / n, Donc le taux d'erreur est : 2/6 = 0,3333 = 33,33 % 0,5.Pt: Précision = a/(a+c) = 66,66 % : représente le pourcentage des colopathies positivement prédites par rapport aux total des colopathies prédites",
        "0.5.Pt. Spécificité = d/(c+d) = 66,66 % représente le pourcentage des non colopathies positivement prédite par rapport aux total des non colopathies réelles. 4) II faut calculer la distance entre l'instance No19 et les 18 autres instances tel que :",
        "D1(Xi,Yi)= (P-M) /1 P tel que : P est le nombre total d'attributs (-2) et M le nombre de ressemblance entre les deux attributs énumératifs < DegStr > et < HrSom > D2(Xi,Yi)= 0 si Xi = Yi Concerne l'attribut binaire < Fum > 0,5 Pt 1 sinon",
        "Ensuite, calculer la distance global D avec une distance d'attributs numériques par exemple avec la distance de manhattan : D(X,Y)= Z-1Xi = Yil Donc : DX,Y)-DICX,) + D2(Xi,Yi) Instance 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 D1",
        "0,5 0,5 0,5 0,5 0 0 1 1 1 1 0,5 0,5 1 1 1 1 0,5 0,5 D2 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 D",
        "1,5 0,5 1,5 0,5 1 0 2 1 2 1 1,5 0,5 2 1 2 1 1,5 0,5 rang 4 2 4 2 3 1 5 3 5 3 4 2 5 3 5 3 4 2 1,5 Pt Enseignant : M' K. Boudjebbour Page 3/4",
        "UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématiques et Informatique Année universitaire : 2018-2019 Exercice 03 (06 Pts) : On génère d'abord les itemsets fréquents de support minimum = 2: C1 itemset [A) (B) (C) (D) (E) Support 3 3 4 1 4",
        "F1 itemset Oui Oui Oui Non Oui 2Pt C2 itemset [A,B) [A,C) [A,E) B,C) (B,E) C,E) Support 1 3 2 2 3 3 F2 itemset Non Oui Oui Oui Oui Oui C3 itemset (A,B,C) [A,B,E) [A,C,E) B,C,E) C4 itemset A,B,C,E) Support / / 2 2 Support / 4 F3 itemset",
        "Non Non Oui Oui = F4 itemset Non Cause A,B) non Fréquent A,B,C) Cause non Fréquent On génère maintenant les règles d'associations d'une confiance minimale = 60 % pour tout sous ensembles non vides fréquents : Pour l'itemset fréquent A,C,E) Règle (A,C)E (A,E)C [C,E)A A(C,E) C[A,E) E[A,C) Confiance 66,66 %",
        "100 % 66,66 % 66,66 % 50 % 50 % Conclusion Acceptée Acceptée Acceptée Acceptée Rejetée Rejetée Pour l'itemset fréquent [B,C,E) Règle (B,C)-E (B,E)-C (C,E)->B B[C,E) C(B,E) E>(B,C) Confiance 100 % 66,66 % 66,66 % 66,66 % 50 % 50 % Pt: Conclusion Acceptée Acceptée Acceptée Acceptée Rejetée Rejetée",
        "Pour les autres itemset AC,AE sont: redondantes par rapport à A[C,E) BC,BE sont redondantes par rapport à B[C,E) L Règle CA EA CB EB CE EC Confiance 75 % 50 % 50 % 75 % 75 % 75 % Conclusion Acceptée Rejetée Rejetée Acceptée Acceptée Acceptée",
        "- Un motif fréquent est dit fermé s'il ne possède aucun sur-motif qui a le même support, exp : (A,C) Pt; Un motif fréquent est dit Maximal si aucun de ses sur-motifs immédiats n'est fréquent, exp:A,C,E!\" Enseignant : M' K. Boudjebbour Page 4/4",
        "UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématique et Informatique Année universitaire : 2017-2018 Interrogatoire : Théorie et pratique du Datamining Exercice 01 (02 pts) : Supposons qu'on veut utiliser des données binaires dans un processus de",
        "clustering. Citer (ou proposer) une (des) mesure(s) de similarité (distance(s)) pour ce type de données. Evaluer la (les) distance(s) entre les objets X = 0101010001 et Y = 0100011000. Que remarquez-vous ? Déduisez la distance de Hamming associée. A quoi la valeur trouvée correspond-elle ?",
        "Exercice 02 (03 pts) : Répondez brièvement aux questions suivantes : 1. Que signifie l'élagage et quel est son objectif ? 2. Quelle est la différence entre les techniques descriptives et les techniques prédictives de datamining ?",
        "3. Dans le processus ECD, une phase de préparation des données est nécessaire. Que signifie la transformation des données ? expliquer en donnant des exemples Exercice 03 (05 pts) : Soit l'ensemble d'apprentissage ci-dessous. La classe est < Edible >. No Shape Color Odor Edible 1 C B 1 Y",
        "2 D B 1 Y 3 D W 1 Y 4 D W 2 Y 5 C B 2 Y 6 D B 2 N 7 D G 2 N 8 C U 2 N 9 C B 3 N 10 C W 3 N 11 D W 3 N",
        "1. En utilisant l'algorithme ID3 et le gain d'information, construire l'arbre de décision du dataset. Donner les détails des calculs. 2. Déduire de l'arbre trouvé une seule règle comportant 2 disjonctions et 2 conjonctions au maximum. 3. En utilisant l'arbre construit, classer l'instance No12: Shape-C, Color-G, Odor-2.",
        "4. En utilisant l'ensemble des onze instances, et en supposant que les attributs < Color > et < Odor > sont des variables énumératives, dites lequel des instances est plus proche de l'instance No 12 ? quelle est la",
        "distance utilisée ? Que représentent ces calculs (donner le nom de ces calculs) ? BON - To succeed in life one must have the courage to pursue what he wants CRRIG Enseignant : M' K. Boudjebbour Page 1/1 UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences",
        "Département de Mathématique et Informatique Année universitaire : 2017-2018 Corrigé Interrogatoire : Théorie et pratique du Datamining Exercice 01 (02,50 Pts): il faut dessiner la table de dissimilarité (contingence) Y on a trois cas 0,5Pt possibles : 1 0 1. Similarité invariante, si toutes les variables sont symétriques (Coefficient de",
        "correspondance simple) : b+c 3 1 2 2 D1(X,Y) 0,3 0.5Pt a+b+c+d 10 X 2. Similarité non invariante, si toutes les variables sont asymétriques (Coefficient 0 1 5 de Jaccard): b+c 3 D2(X,Y) 0,6 0,5PL a+b+c 5",
        "3. Si les variables sont symétriques et asymétriques : il faut spécifier la nature de chaque variable. 0,25Pt. E0.5.P:Malgré que D1 et D2 représentent deux distances entre les mêmes instances, on remarque que qu'elles sont très éloignées car D2=2*D1",
        "Distance de hamming = b+c = 3. Elle représente le nombre de caractéristiques différentes entre X et y.:0,25 Pt Exercice 02 (03 Pts). : 1. L'élagage est la suppression de quelques sous-arbres dans la l'arbre de décision. Son objectif principal est la réduction de l'arbre afin d'améliorer le taux d'erreur.",
        "2. les techniques descriptives de datamining visent à mettre en évidence des informations présentes mais cachées par le volume de données alors que les techniques prédictives visent à extrapoler de nouvelles informations à partir des informations présentes. Elles se basent essentiellement sur des",
        "modèles qui utilisent des données présentes ou passées pour construire des scénarios futurs. 3. La transformation des données est la transformation d'un attribut A en une autre variable A' qui serait selon les objectifs de l'étude, plus appropriée. Exp 1: Variable continue en variable discrète et vice versa",
        "Exp 2: La construction d'agrégats par exemple, le prix au mètre-carré d'un appartement Exercice 03 (05,50 Pts): - 5 6 1) On calcul l'entropie sur l'ensemble des données : I(5,6)= log log = 0,994 0,5Pt 11 11 11 11 Ensuite on calcul le gain de chaque attribut : 5 6",
        "Gain (Shape)= (5,6)-E(Shape)- I(5,6)-( I(2,3)+ I(3,3))-0,008 11 11 5 4 1 1 Gain (Color)= (5,6)-E(Color)- I(5,6)-( I(3,2)+ I(2,2)+ I(0,1)+ I(0,1)-0,189 11 11 11 11 3 5 3 Gain (Odor)= (5,6)-E(Odor)- I(5,6)-( I(3,0)+ I(2,3)+ I(0,3))-0,553 11 11 11",
        "Donc on choisit l'attribut < Odor > avec le gain le plus grand (Gain-0.553) qui représente le noeud la racine de l'arbre, Donc l'arbre initial sera : Odor 1Pt: 2 3 1 ????? N Y Instances : 4,5,6,7,8 Instance : 9,10,11 Instances : 1,2,3 Page 1/2",
        "UNIVERSITE: DR < YAHIA FARES > DE MEDEA Faculté des Sciences Département de Mathématique et Informatique Année universitaire : 2017-2018 La valeur Odor = 2 donne plusieurs valeurs de l'attribut classe, donc, il faut refaire le même travail (calcul du gain) pour l'ensemble des données S2 (4,5,6,7,8). I(S2) =I(2,3)-0,971 1",
        "2 1 1 Gain (S2, Color)= I(2, 3)-E(S2, Color)-0,971-E I(1,0) + I(1,1)+ I(0,1)+ - I(0,1))-0,571 5 5 5 3 Z Gain (S2, Shape)= I(2, 3)-E(S2, Shape)-0,971-E I(1,2) + I(1,1))-0,020 5 Donc on choisit l'attribut < Color> avec le gain le plus grand (Gain=0,571). On aura deux branches",
        "avec des noeuds terminaux et la branche B qui sera nécessairement départagée par le seul attribut restant à savoir <<Shapex et l'arbre final sera : Odor 2 3 1 Color N Y Instance : 9,10,11 B G ou U W Instances : 1,2,3 - Shape N Y Pt",
        "D Instances : 7,8 Instance : 4 - Y N Instance : 5 Instance : 6 2) La règle qu'on peut déduire est : (Odor = 1) V ((Odor = 2) A ((Color = W) V ((Color = B) A (Shape = C))) Pt:",
        "3) La classe est : N 0,25Pt 4) II faut calculer la distance entre l'instance No12 et les 11 autres instances : D,Y)-DICXi,Y) + D2(Xi,Yi) D1(Xi,Yi)= (P-M) /1 P tel que : P est le nombre total d'attributs et M le nombre de ressemblance",
        "Qui concerne les deux attributs énumératifs < Odor > et < Color > D2(Xi,Yi)= 0 si Xi = Yi; 1 sinon Concerne l'attribut binaire < Shape > 01 Pt: No Instance D1 D2 D No Instance D1 D2 D 1 1 1 2 7 0,5 0 0,5 2 1 0",
        "1 8 0 1 1 3 1 0 1 9 1 1 2 4 0,5 0 0,5 10 1 1 2 5 0,5 1 1,5 11 1 0 1 6 0,5 0 0,5",
        "Donc, les instances les plus proches de l'instance No12 sont : l'instance No 4, No 6 et No 7. 0,25 Pt La distance utilisée est la distante mixte entre deux type d'attributs (dans notre cas, on a utilisé la",
        "distance de Manhattan) qui représente un calcul de similarité entre instantes afin d'appliquer une méthode de clustering. 0,5 Pt: Page 2/2"
    ],
    "summaries": [
        {
            "chunk_id": 1,
            "summary": "Do you have a question about data mining?"
        },
        {
            "chunk_id": 2,
            "summary": "Une fonctionnement de la méthode de rééchantionnage s"
        },
        {
            "chunk_id": 3,
            "summary": "Les Etats-Unis peuvent tre tre "
        },
        {
            "chunk_id": 4,
            "summary": "Oui Sciences ABien Non Admis du Baccalauréat obtenu et la"
        },
        {
            "chunk_id": 5,
            "summary": "All photographs courtesy of AFP, EPA, Getty Images and Reuters"
        },
        {
            "chunk_id": 6,
            "summary": "Les Etats-Unis peut--tre tre  l'"
        },
        {
            "chunk_id": 7,
            "summary": "All photographs  AFP, EPA, Getty Images and Reuters"
        },
        {
            "chunk_id": 8,
            "summary": "Doublant-Oui, Série-Maths,"
        },
        {
            "chunk_id": 9,
            "summary": "Le taux d'erreur de l'arbre"
        },
        {
            "chunk_id": 10,
            "summary": "L'étudiant de l'avenir des étudiants de tron"
        },
        {
            "chunk_id": 11,
            "summary": "Les entiers suivants : D= £2,5,"
        },
        {
            "chunk_id": 12,
            "summary": "C'est un peu t--tre  l'arrivée de l'"
        },
        {
            "chunk_id": 13,
            "summary": "In our series of letters from African journalists, novelist and writer M' K. Boudjeb"
        },
        {
            "chunk_id": 14,
            "summary": "The European Space Agency (ESA) has announced the launch of the European Deep Space Observatory (EDS"
        },
        {
            "chunk_id": 15,
            "summary": "The following table shows the distances between clusters of two or more people."
        },
        {
            "chunk_id": 16,
            "summary": "Regroupement des clusters (1) et [2) en (1,2) 9 3 11 12 8 Etape 2 "
        },
        {
            "chunk_id": 17,
            "summary": "Regroupement des clusters (1,2) et [9,12) en (1,2,9,12) 9,12 8"
        },
        {
            "chunk_id": 18,
            "summary": "Le groupe k 1Pt Un regroupement en 2 clusters : CI=(1,2"
        },
        {
            "chunk_id": 19,
            "summary": "Une meilleur meilleur regroupement s'il avait tre "
        },
        {
            "chunk_id": 20,
            "summary": "Researchers at the University of California, Los Angeles (UCLA) have discovered a new way to measure the amount of time"
        },
        {
            "chunk_id": 21,
            "summary": "Reports from the final round of the British Open at Royal Birkdale."
        },
        {
            "chunk_id": 22,
            "summary": "Donc l'arbre initial sera : HrSom 0.5.Pt Egal Sup"
        },
        {
            "chunk_id": 23,
            "summary": "Les valeurs de la classe, donc, donnent deux valeurs de la classe,"
        },
        {
            "chunk_id": 24,
            "summary": "DegStr Moins y Normal Fort Yes Inst : 13  18 A"
        },
        {
            "chunk_id": 25,
            "summary": "Match reports from the Champions League quarter-final first leg between Real Madrid and Paris St-Germain at the Bernabeu."
        },
        {
            "chunk_id": 26,
            "summary": "Fort Yes Oui Non Donc on choisit l'attribut  Fum >"
        },
        {
            "chunk_id": 27,
            "summary": "The aim of this study is to investigate the relationship between the number of Moins and the number"
        },
        {
            "chunk_id": 28,
            "summary": "The BBC News website asks you to vote on the following:"
        },
        {
            "chunk_id": 29,
            "summary": "Voters in France go to the polls on Thursday to choose their preferred candidate to contest"
        },
        {
            "chunk_id": 30,
            "summary": "The winning numbers in Saturday evening's drawing of the French Open game between Paris St-"
        },
        {
            "chunk_id": 31,
            "summary": "The World Health Organization (WHO) has released the results of its study on the impact of"
        },
        {
            "chunk_id": 32,
            "summary": "Xi Jinping, president of the People's Republic of China, and Yi Gang, prime minister of China,"
        },
        {
            "chunk_id": 33,
            "summary": "Find out how to calculate the distance between two points on the Earth."
        },
        {
            "chunk_id": 34,
            "summary": "BBC Sport takes a look at some of the key talking points from this weekend's"
        },
        {
            "chunk_id": 35,
            "summary": "Match reports from the weekend's Premier League and Championship matches."
        },
        {
            "chunk_id": 36,
            "summary": "On gre d'abord les itemsets fréquents de support minimum = 2: C1"
        },
        {
            "chunk_id": 37,
            "summary": "Match reports from the weekend's Premier League and Championship games."
        },
        {
            "chunk_id": 38,
            "summary": "Pour l'itemset fréquent A,B,C,E) Rgle (A,C"
        },
        {
            "chunk_id": 39,
            "summary": "French President Francois Hollande has accepted the resignation of Ecology Minister Segolene Royal."
        },
        {
            "chunk_id": 40,
            "summary": "Rejetée Rejetée Acceptée Acceptée Acceptée"
        },
        {
            "chunk_id": 41,
            "summary": "- Un motif fréquent est dit fermé s'il ne possde"
        },
        {
            "chunk_id": 42,
            "summary": "Researchers at the Massachusetts Institute of Technology (MIT) and the Massachusetts Institute of Technology (MIT)"
        },
        {
            "chunk_id": 43,
            "summary": "The distance between two points on a Cartesian plane is the same as the distance between two points on a"
        },
        {
            "chunk_id": 44,
            "summary": "The World Health Organization (WHO) and the International Committee of the Red Cross ("
        },
        {
            "chunk_id": 45,
            "summary": "Dans le processus EC, nous avons tre tre  l'"
        },
        {
            "chunk_id": 46,
            "summary": "The winning numbers in Saturday evening's drawing of the \"Powerball\" game were:"
        },
        {
            "chunk_id": 47,
            "summary": "Une dataset de shape- C, Color-G, and Odor-2 s'est util"
        },
        {
            "chunk_id": 48,
            "summary": "C'est tre un peu t--tre"
        },
        {
            "chunk_id": 49,
            "summary": "Le nom de ces calculs (donner le nom de ces calculs)"
        },
        {
            "chunk_id": 50,
            "summary": "Datamining exercice 01 (02,50 Pts): il faut dessiner la table de dissimilar"
        },
        {
            "chunk_id": 51,
            "summary": "Les variables sont asymétriques (Coefficient 0 1 5"
        },
        {
            "chunk_id": 52,
            "summary": "Si les variables sont symétriques et asymétriques"
        },
        {
            "chunk_id": 53,
            "summary": "L'élagage est la suppression de quelques sous-arbres dans la l'ar"
        },
        {
            "chunk_id": 54,
            "summary": "This paper presents the results of a two-year study on the effects of datamining on the"
        },
        {
            "chunk_id": 55,
            "summary": "La transformation des dons qui utilisent des donsariosentes qui passées pour construr"
        },
        {
            "chunk_id": 56,
            "summary": "Exp 1: La construction d'agrégats par exemple, le prix au mtre-car"
        },
        {
            "chunk_id": 57,
            "summary": "The winning numbers in Saturday evening's drawing of the UK lottery's draw"
        },
        {
            "chunk_id": 58,
            "summary": "Donc on choisit l'attribut  Odor avec le gain le plus grand (Gain"
        },
        {
            "chunk_id": 59,
            "summary": "The results of a study by researchers at the University of Rennes on the impact of noise pollution on children's behaviour"
        },
        {
            "chunk_id": 60,
            "summary": "The winning numbers in Friday evening's drawing of the French national lottery are:"
        },
        {
            "chunk_id": 61,
            "summary": "All photographs courtesy of AFP, EPA, Getty Images and Reuters"
        },
        {
            "chunk_id": 62,
            "summary": "Match of the Day 1 - Match of the Day 2 - Match of the Day 3 - Match of"
        },
        {
            "chunk_id": 63,
            "summary": "The following is a list of the most recent earthquakes in the US:"
        },
        {
            "chunk_id": 64,
            "summary": "L'attribut binaire  Shape > 01 Pt: No Instance D1 D2 D No Instance D"
        },
        {
            "chunk_id": 65,
            "summary": "The winning numbers in Saturday evening's drawing of the UK's"
        },
        {
            "chunk_id": 66,
            "summary": "La distance utilisée est la distante mixte entre deux type d'"
        },
        {
            "chunk_id": 67,
            "summary": "C'est un peut--t"
        }
    ]
}