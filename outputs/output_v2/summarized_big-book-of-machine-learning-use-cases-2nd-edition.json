{
    "chunks": [
        "0 eBook Big Book of Machine Learning Use Cases A collection of technical blogs, including code samples and notebooks - databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 2 Contents CHAPTER 1: Introduction 3 CHAPTER 2: Moneyball 2.0: Improving Pitch-by-Pitch Decision-Making With MLB's Statcast Data 4",
        "CHAPTER 3: Improving On-Shelf Availability for Items With Out-of-Stock Modeling 14 CHAPTER 4: Using Dynamic Time Warping and MLflow to Detect Sales Trends Part 1: Understanding Dynamic Time Warping 20 Part 2: Using Dynamic Time Warping and MLflow to Detect Sales Trends 26 CHAPTER 5:",
        "Detecting Financial Fraud at Scale With Decision Trees and MLflow on Databricks 34 CHAPTER 6: Fine-Grained Time Series Forecasting at Scale With Prophet and Apache Spark\" 45 CHAPTER 7: Applying Image Classification With PyTorch Lightning on Databricks 52 CHAPTER 8: Processing Geospatial Data at Scale With Databricks 63 CHAPTER 9:",
        "Exploring Twitter Sentiment and Crypto Price Correlation Using Databricks 77 CHAPTER 10: Customer Case Studies 86 databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 3 CHAPTER 1: Introduction Organizations across many industries are using machine learning to power new customer experiences, optimize business processes and improve",
        "employee productivity. From detecting financial fraud to improving the play-by-play decision-making for professional sports teams, this book brings together a multitude of practical use cases to get you started on your machine learning journey. The collection also serves as a guide -",
        "including code samples and notebooks - SO you can roll up your sleeves and dive into machine learning on the Databricks Lakehouse. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 4 CHAPTER 2: Moneyball 2.0: Introduction Improving Pitch-by-Pitch",
        "The Oakland Athletics baseball team in 2002 used data analysis and quantitative modeling to identify Decision-Making With undervalued players and create a competitive lineup on a limited budget. The book \"Moneyball,\" written MLB's Statcast Data",
        "by Michael Lewis, highlighted the A's '02 season and gave an inside glimpse into how unique the team's strategic data modeling was for its time. Fast-forward 20 years - the use of data science and",
        "quantitative modeling is now a common practice among all sports franchises and plays a critical role in scouting, roster construction, game-day operations and season planning. KEY PlayerTacking PachtoPweTading PowerCable 110V FbreCable -r dvidual Supply Tecr nov 0 By Max Wittenberg 1oV nev Core S noV 1",
        "Figure 1: Position and scope of Hawkeye cameras at ab baseball stadium databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION In 2015, Major League Baseball (MLB) introduced Statcast, a set of cameras and teams are now able to make decisions that influence the outcome of the game,",
        "radar systems installed in all 30 MLB stadiums. Statcast generates up to seven pitch by pitch. It's been 20 seasons since the A's first introduced the use of data terabytes of data during a game, capturing every imaginable data point and",
        "modeling to baseball. Here's an inside look at how professional baseball teams metric related to pitching, hitting, running and fielding, which the system collects use technologies like Databricks to create the modern-day \"Moneyball\" and gain and organizes for consumption. This explosion of data has created opportunities",
        "competitive advantages that data teams provide to coaches and players on to analyze the game in real time, and with the application of machine learning, the field. 2020-09- T00,00.00.000-0000 2020-09- :0 0000 2020-09- BT00:00:00 +0000 2020-09- 0000 2020-09- TO0:00:0 2020.09-1 18T00:00:00.000 0-0000 AROAISTDAOAOAONOAN 1.8 2020-09- 18100.0000.000-0000 39.7",
        "SI 2020-09-1 18100,00.00.000-0000 9.8 578428 35411 FF EARGAHISTDARAOAROAGQON A Figure 3: Sample of data collected by Statcast 6 . A3 Figure 2: Numbers represent events during a play captured by Statcast databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 6 Background",
        "Data teams need to be faster than ever to provide analytics to coaches and 23 seconds, and this window of time represents a benchmark from which players SO they can make decisions as the game unfolds. The decisions made from",
        "Statcast data can be ingested and processed for coaches to use to make real-time analytics can dramatically change the outcome of a game and a team's decisions that can impact the outcome of the game.",
        "season. One of the more memorable examples of this was in game six of the 2020 2. Real-Time Analytics: Another competitive advantage for teams is the World Series. The Tampa Bay Rays were leading the Los Angeles Dodgers 1-0 in",
        "creation of insights from their machine learning models in real time. the sixth inning when Rays pitcher Blake Snell was pulled from the mound while An example of this is knowing when to substitute out a pitcher from fatigue,",
        "pitching arguably one of the best games of his career, a decision head coach Kevin where a model interprets pitcher movement and data points created Cash said was made with the insights from their data analytics. The Rays went on",
        "from the pitch itself and is able to forecast deterioration of performance to lose the game and World Series. Hindsight is always 20-20, but it goes to show pitch by pitch. how impactful data has become to the game. Coaching staff task their data teams",
        "with assisting them in making critical decisions - for example, should a pitcher 3. Ease of Use: Analytics teams run into problems ingesting the volumes of throw another inning or make a substitution to avoid a potential injury? Does a",
        "data Statcast produces when running data pipelines on their local computers. player have a greater probability of success stealing from first to second base, This gets even more complicated when trying to scale their pipelines to or from second to third?",
        "capture minor league data and integrate with other technologies. Teams want a collaborative, scalable analytics platform that automates data ingestion I have had the opportunity to work with many MLB franchises and discuss what with performance, creating the ability to impact in-game decision-making.",
        "their priorities and challenges are related to data analytics. Typically, I hear three recurring themes their data teams are focused on that have the most value in Baseball teams using Databricks have developed solutions for these priorities helping set their team up for success on the field:",
        "and several others. They have shaped what the modern-day version of \"Moneyball\" looks like. What follows is their successful framework explained 1. Speed: Since every MLB team has access to the Statcast data during a game, in an easy-to-understand way.",
        "one way to create a competitive advantage is to ingest and process the data faster than your opponent. The average length of time between pitches is databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION Getting the data",
        "When a pitcher throws a baseball, Hawkeye cameras collect the data and save it collected and processed. Just as a waiter can quickly become overworked to an application that teams are able to access using an application programming fulfilling customers' needs, making continuous API requests for data creates some",
        "interface (API) owned by MLB. You can think of an API as an intermediate challenges in data pipelines. With the assistance from these data teams, however, connection between two computers to exchange information. The way this works we have created code to accommodate continuously collecting Statcast data",
        "is: a user sends a request to an API, the API confirms that the user has permission during a game. You can see an example of the code using a test API below. to access the data and then sends back the requested data for the user to",
        "consume. To use a restaurant as an analogy - a customer tells a waiter what they from pathlib import Path import json want to eat, the waiter informs the kitchen what the customer wants to eat, the",
        "waiter serves the food to the customer. The waiter in this scenario is the API. class def sports_api: init_(self, endpoint, api_key) : self.endpoint = endpoint self.api_key = api_key self.connection = self.endpoint + self.api_key API User Application def fetch_payload (self, request_ L, request_2, adls_path) url = Flaaif.comnectionlsse d= (reques 1(request_2)- Request",
        "Request 99.M\" r = requests. get (url) Response Response json_data - r.json () now = time. .strftime CaYamid-HR.MIS? file_name = f'json_data_out, (now)\" file_path = Path (\"dbfs:/\") Path (adls_path) / Path (file_name) dbutils.fs.put (str (file_path), json.dumps (json_data), True) Figure 4: Example of how an API works, using a restaurant analogy",
        "return str (file_path) Figure 5: Interacting with an API to retrieve and save data This simple method of retrieving data is called a \"batch\" style of data collection and processing, where data is gathered and processed once. As noted earlier,",
        "This code decouples the steps of getting data from the API and transforming however, data is typically available through the API every 23 seconds (the average it into usable information, which in the past, we have seen, can cause latency",
        "time between pitches). This means data teams need to make continuous requests in data pipelines. Using this code, the Statcast data is saved as a file to cloud to the API in a method known as \"streaming\" where data is continuously",
        "storage automatically and efficiently. The next step is to ingest it for processing. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 8 Automatically load data with Auto Loader As pitch and play data is continuously saved to cloud storage, it can be ingested",
        "automatically using a Databricks feature called Auto Loader. Auto Loader scans CSV JSON files in the location they are saved in cloud storage and loads the data into A C D \"Employee\": [ ID Gender City Monthly. Databricks where data teams begin to transform it for their analytics. Auto Loader",
        "2 ID000002(Female Delhi 20000 3 ID000004E Male Mumbai 35000 t is easy to use and incredibly reliable when scaling to ingest larger volumes of data 4 ID000007H Male Panchkula 22500 5 ID0000081Male Saharsa 35000 \"id\":\"1\", in batch and streaming scenarios. In other words, Auto Loader works just as well",
        "6 ID0000091N Male Bengaluru 100000 7 ID000010KMale Bengaluru 45000 for small and large data sizes in batch and streaming scenarios. The Python code 8 ID000011LF Female Sindhudui 70000 \"Name\": \"Ankit\", 9 ID000012NN Male Bengaluru 20000 below shows how to use Auto Loader for streaming data. 10 ID000013NMale Kochi 75000",
        "\"Sal\": \"1000\", 11 1D000014CFemale Mumbai 30000 12 ID000016CMale Mumbai 25000 df = spark.readtream. format (\"cloudFiles\") I 3 1D000018SFemale Surat 25000 ), f option (,) 14 ID000019TF Female Pune 24000 schema () I 15 ID000021VMale Bhubanes 27000 ID000022VFemale Howrah 28000 \"id\":\"2\", .load() \"Name\": \"Faizv\". df.writestream. format (\"delta\") I option rehechpointlocation? )",
        "Figure 7: Comparison of CSV and JSON formats trigger () 1 .start () It should be obvious which of these two formats data teams prefer to work Figure 6: Setup of Auto Loader to stream data",
        "with. The goal then is to load Statcast data in the JSON format and transform it into the friendlier CSV format. To do this, we can use the semi-structured data One challenge in this process is working with the file format in which the Statcast",
        "support available in Databricks, where basic syntax allows us to extract and is saved, a format called JSON. We are typically privileged to work with data that is transform the nested data you see in the JSON format to the structured CSV style",
        "already in a structured format, such as the CSV file type, where data is organized format. Combining the functionality of Auto Loader and the simplicity of semi- in columns and rows. The JSON format organizes data into arrays and despite its",
        "structured data support creates a powerful data ingestion method that makes the wide use and adoption, I still find it difficult to work with, especially in large sizes. transformation of JSON data easy. Here's a comparison of data saved in a CSV format and a JSON format. databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 9 Using Databricks' semi-structured data support with Auto Loader Auto Loader writing data to a Delta table as a stream spark.readstream. format (\"cloudFiles\" I # Define the schema and the input, checkpoint, and output pai ths. .option reloudflles.format, \"json\") 1",
        "read_schema = (\"id int, .option \"cloudriles.schematocation\", 1 \"firstName string, .load (\"\") I \"middleName string, .selectExpr \"lastName string, + \"gender string, + \"tags:page.name\", # extracts (\"tags\": \"page\": \"name' 113 \"birthDate timestamp, \"taga-page.idtint\", # extracts tagaraCpagercian 133 and \"ssn string, casts to int \"salary int\") \"tagsieventrype\" # extracts tagarteventrype? 13",
        "json_read.path = /FileStore/streaming-uploads/people-10m' checkpoint_path - /mt/delta/people-10n/checepoints save_path = me/daltapeople-o people_stream (spark I readStream I As the data is loaded in, we save it to a Delta table to start working with it further. .schema (read_schema) I .option Charleaerrigger, 1) I",
        "Delta Lake is an open format storage layer that brings reliability, security and .option (\"multiline', True) performance to a data lake for both streaming and batch processing and is the -json Gson_read.path)) foundation of a cost-effective, highly scalable data platform. Semi-structured people_stream.\" writeStream I",
        "support with Delta allows you to retain some of the nested data if needed. The .format( ('delta') I outputMode ('append') I syntax allows flexibility to maintain nested data objects as a column within a Delta option checkpointlocation, checkpoint_path) I",
        "table without the need to flatten out all of the JSON data. Baseball analytics teams .start (save_path) use Delta to version Statcast data and enforce specific needs to run their analytics on while organizing it in a friendly structured format.",
        "With Auto Loader continuously streaming in data after each pitch, semi-structured data support transforming it into a consumable format, and Delta Lake organizing it for use, data teams are now ready to build analytics that gives their team the competitive edge on the field. databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 10 Machine learning for insights Recall the Rays pulling Blake Snell from the mound during the World Series = that Machine learning models are relatively easy to build and use, but data teams often",
        "decision came from insights coaches saw in their predictive models. Statistical struggle to implement them into streaming use cases. Add in the complexity of analysis of Snell's historical Statcast data provided by Billy Heylen of sportingnews. how models are managed and stored and machine learning can quickly become",
        "com indicated Snell had not pitched more than six innings since July 2019, had a out of reach. Fortunately, data teams use MLflow to manage their machine learning lower probability of striking out a batter when facing them for the third time in a",
        "models and implement them into their data pipelines. MLflow is an open source game, and was being relieved by teammate Nick Anderson, whose own pitch data platform for managing the end-to-end machine learning lifecycle and includes suggests was one the strongest closers in MLB, with a 0.55 earned run average",
        "support for tracking predictive results, a model registry for centralizing models that (ERA) and 0.49 walks and hits per innings pitched (WHIP) during the 19 regular- are in use and others in development, and a serving capability for using models in",
        "season games he pitched in 2020. Predictive models analyze data like this in real data pipelines. time and provide supporting evidence and recommendations coaches use to make critical decisions. MLflow Tracking MLflow Projects MLflow Models Model Registry Record and query Package data science code Deploy machine learning Store, annotate, discover,",
        "experiments: code, data, in a format to reproduce runs models in diverse serving and manage models in a config, and results on any platform environments central repository Read more Read more Read more Read more Figure 8: MLflow overview databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION",
        "11 To implement machine learning algorithms and models to real-time use cases, The outputs a machine learning model creates can then be displayed in a data data teams use the model registry where a model is able to read data sitting in",
        "visualization or dashboard and used as printouts or shared on a tablet during a a Delta table and create predictions that are then used during the game. Here's game. MLB franchises working on Databricks are developing fascinating use cases",
        "an example of how to use a machine learning model while data is automatically that are being used during games throughout the season. Predictive models are loaded with Auto Loader: proprietary to the individual teams, but here's an actual use case running on",
        "Databricks that demonstrates the power of real-time analytics in baseball. Getting a machine learning model from the registry and using it with Auto Loader #get model from the model registry model = mlflow.spark. load_model ( model_uri \"model (moo name \"Production')\"? #read data from bronze table as a stream",
        "events = spark.readtream I .format (\"delta\") I #.option reloudFies.marilesPerrigger\", 1)1 .schema (schema) I .table Chaseatre7 #pass stream through model model_output model.transform: (events) #write stream to silver delta table events.writestrean I .format (\"delta') I .outputMode (\"append\") I option('checkpointlocation', \"/tmp/baseball/\"? I table Cdefault.baseball.treamsilver? databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 12 Bringing it all together with spin ratios and sticky stuff Average adjusted fastball spin rate per season MLB introduced a new rule for the 2021 season meant to discourage pitcher's use 20 21 22 23 24 25",
        "of \"sticky stuff,\" a substance hidden in mitts, belts or hats that when applied to a 2017 baseball can dramatically increase the spin ratio of a pitch, making it difficult for 2018 batters to hit. The rule suspends for 10 games pitchers discovered using sticky",
        "stuff. Coaches on opposing teams have the ability to request an umpire check for 2019 the substance if they suspect a pitcher to be using it during a game. Spin ratio is a 2020 data point that is captured by Hawkeye cameras, and with real-time analytics and 2021",
        "machine learning, teams are now able to make justified requests to umpires with the hopes of catching a pitcher using the material. After June 3 Source: Baseball Prospectus How spin affects a pitch Direction of pitch Figure 10: Trending spin rate of fastballs per season and after rule introduction on",
        "June 3, 2021 The ball is pushed in Following the same framework outlined above, we ingest Statcast data pitch by the direction of the spin, pitch and have a dashboard that tracks the spin ratio of the ball for all pitchers Friction pulls air making it harder to hit.",
        "during all MLB games. Using machine learning models, predictions are sent to the around the ball. 334 dashboard that flag outliers against historical data and the pitcher's performance in the active game, which can alert coaches when they fall outside of ranges",
        "anticipated by the model. With Auto Loader, Delta Lake and MLflow, all data ingestion and analytics happen in real time. Figure 9: Illustration of how spin affects a pitch databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 13",
        "Technologies like Statcast and Databricks have brought real-time analytics to Lovelytics Defense Analysis: Pitch Spin Rate sports and changed the paradigm of what it means to be a data-driven team. As data volumes continue to grow, having the right architecture in place to capture 24.63 MW ) -",
        "real-time insights will be critical to staying one step ahead of the competition. Mwyw MV wwww Real-time architectures will be increasingly important as teams acquire and Historical Trend 2017/20181 12019j202012021 GameTrend ThisS Seas develop players, plan for the season and develop an analytically enhanced",
        "approach to their franchise. Ask about our Solution Accelerator with Databricks 25.50 partner Lovelytics, which provides sports teams with all the resources they need 25.0 to quickly create use cases like the ones described in this blog. te 10.6496 A11516 A0.6296 V21185 V25516 V2976 V31186 A03316 Y0486 2463 V2615",
        "Figure 11: Dashboardi for \"sticky stuff\" detection in real time databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 14 CHAPTER 3: Improving On-Shelf Introduction Availability for Items With Retailers are missing out on nearly $1 trillion in global sales because they don't have on hand what",
        "AIOut-of-Stock Modeling customers want to buy in their stores. Adding to the challenge, a study of 600 households and several retailers by research firm IHL Group details that shoppers encounter out-of-stocks (0OS) as often as This post was written in collaboration with Databricks partner",
        "one in three shopping trips, according to the report. And a IRI found that 20% of all out-of- Tredence. We thank Rich Williams, Vice President Data study by Engineering, and Morgan Seybert, Chief Business Officer, of stocks remain unresolved for more than 3 days. Tredencei for their contributions.",
        "Overall, studies show that the average OOS rate is about 8%. That means that one out of 13 products is not purchasable at the exact moment the customer wants to get it in the store. OOS is one of the",
        "biggest problems in retail, but thankfully it can be solved with real-time data and analytics. In this write-up, we showcase the new Tredence-Databricks combined On-Shelf Availability Solution Accelerator. The accelerator is a robust quick-start guide that is the foundation for a full out-of-stock",
        "or supply chain solution. We outline how to approach out-of-stocks with the Databricks Lakehouse to solve for on-shelf availability in real time. By Rich Williams, Morgan Seybert, Rob Saker and Bryan Smith And the impact of solving this problem? A 2% improvement in on-shelf availability is worth 1% in",
        "increased sales for retailers. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 15 Growth in e-commerce makes item availability more important The significance of this problem has been amplified by the availability of e-commerce for delivery and curbside pickup orders. While customers that face",
        "an out-of-stock at the store level may just not purchase that item, they are likely to purchase other items in the store. Buying online means that they may just switch to a different retailer. The impact is not just limited to a bottom line loss in revenue. Research from",
        "NielsenIQ shows that 30% of shoppers will visit new stores when they can't find the product they are looking for, leading to a loss in long-term loyalty. Members of e-commerce membership programs are most likely to switch retailers in the event",
        "of an out-of-stock. IHL estimates that \"upwards of 24% of Amazon's current retail revenue comes from customers who first tried to buy the product in-store.\" Retailers have responded to this with a variety of tactics including over-ordering of items, which increases carrying costs and lowers margins when they are forced",
        "to sell excess inventory at a discount. In some instances, retailers and distributors It's not just retailers that are impacted by OOS. Retailers, consumer goods will rush order products or use intra-delivery \"hot shots\" for additional deliveries, companies, distributors, brokers and other firms each invest in third-party audits,",
        "which come at an additional cost. Some retailers have invested in robotics, but which typically involve employees visiting stores to identify gaps on the shelf. On",
        "many pull out of their pilots citing costs. And other retailers are experimenting with any given day, tens of thousands of individuals are visiting stores to validate item computer vision, although these approaches merely notify them when an item is",
        "availability. Is this really the best use of time and resources? unavailable and don't predict item availability. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 16 Why hasn't technology solved out-of-stocks yet? Introducing the On-Shelf Availability Solution Accelerator",
        "Out-of-stock issues have been around for decades, SO why hasn't the retail Our partners at Tredence approached us with the idea of publishing a Solution industry been able to solve an issue of this magnitude that impacts shoppers,",
        "Accelerator that they've created as the core of a broader Supply Chain Control retailers and brands alike? The seemingly simple solution is to require employees Tower offering. Tredence works with the largest retailers on the planet and to manually count the items on hand. But with potentially hundreds of thousands",
        "understands the nuances of modeling OOS and knew that Databricks' processing of individual SKUS distributed across a large format retail location that may be and their advanced data science capabilities were a winning combination. servicing customers nearly 24 hours a day, this simply isn't a realistic task to",
        "perform on a regular basis. While the OSA solution focuses on driving sales through improved stock availability on the shelves, the broader Retail Supply Chain Control Tower solves for multiple Individual stores do perform inventory counts periodically and then rely on point-",
        "adjacent merchandising problems - inventory design for the stores, efficient store of-sale (POS) and inventory management software to track changes that drive unit replenishments, design of store network for omnichannel operations, etc. Knowing",
        "counts up and down. But with sO much activity within a store location, some of the how big a problem this is in retail, we immediately took them up on their offer. day-to-day recordkeeping falls through the cracks, not to mention the impact of",
        "shrinkage, which can be hard to detect, on in-store supplies. The first step in addressing OSA challenges is to examine their occurrence in the historical data. Past occurrences point to systemic issues with suppliers and So the industry falls back on modeling. But given fundamental problems in data",
        "internal processes, which will continue to cause problems if not addressed. accuracy, these approaches can drive a combination of false positives and false negatives that make model predictions difficult to employ. Time sensitivities To support this analysis, Tredence made available a set of historical inventory and",
        "further exacerbate the problem, as the large volume of data that often must be sales data. These data sets were simulated, given the obvious sensitivities any",
        "crunched in order to arrive at model predictions must be handled fast enough for retailer would have around this information, but were created in a manner that",
        "the results to be actionable. The problem of building a reliable system for stockout frequently observed OSA challenges manifested in the data. These challenges were: prediction and alerting is not as straightforward as it might appear. 1. Phantom inventory 3. Zero-sales events 2. Safety stock violations 4. On-shelf availability databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 17 Phantom inventory Safety stock violations In a phantom inventory scenario, the units reported to be on hand do not align Most organizations establish a threshold for a given product's inventory, below",
        "with units expected based on reported sales and replenishment. which replenishment orders are triggered. If set too low, inadequate lead times or even minor disruptions to the supply chain may lead to an out-of-stock scenario 160 Alert while new units are moving through the replenishment pipeline. 140 20 120 Alert",
        "- 5 - 1111 1-May 2-May 3-May 4-May 5-May 6May 7-May 8-May 9-May 10-May 11-May 12-May 13-May 14-May 15-May -Replenished Units -0-On-Handl Inventory -0-Sales Units 4 Phantoml Inventory 1-May 2-May 3-May 4-May S-May 6-May 7-May 8-May 9-May 10-May 11-May 12-May 13-May 14-May 15-May",
        "Figure 1: The misalignment of reported inventory, with inventory expected based on sales and LSafety stock -Replenishedu Units -0-On-Handi Inventory replenishment, creating phantom inventory Figure 2: Safety stock levels not providing adequate lead time to prevent out-of-stocki issues Poor tracking of replenishment units, unreported or undetected shrinkage,",
        "and out-of-band processes coupled with infrequent and sometimes inaccurate The flip side of this is that if set too high, retailers risk overstocking products that inventory counts create a situation where retailers believe they have more may expire, risk damage or theft, or otherwise consume space and capital that",
        "units on hand than they actually do. If large enough, this phantom inventory may be better employed in other areas. Finding the right safety stock level for a may delay or even prevent the ordering of replenishment units, leading to an",
        "product in a specific location is a critical task for effective inventory management. out-of-stock scenario. databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 18 Zero-sales events On-shelf availability Phantom inventory and safety stock violations are the two most common",
        "While understanding scenarios in which items are not in stock is critical, it's causes of out-of-stocks. Regardless of the cause, out-of-stock events manifest equally important to recognize when products are technically available for sale themselves in periods when no units of a product are sold.",
        "but underperforming because of non-optimal inventory management practices. These merchandising problems may be due to poor placement of displays within Not every occurrence of a zero-sales event reflects an out-of-stock concern. the store, the stocking of products deep within a shelf, the slow transfer of product",
        "Some products don't sell every day, and for some slow-moving products, from the backroom to shelves, or a myriad of other scenarios in which inventory multiple days may go by within which zero units are sold while the product",
        "is adequate to meet demand but customers cannot easily view or access it. remains adequately stocked. Alert .35 .25 5 E 0.05 3-May 4-May 5.May 6-Ma 9-May 10-May 11-May 12-May 13-May 14- May - Threshold (Probability) 0-SalesUnits Cumulative Probablity vofZeroSales Alert,",
        "ActualSales Expecteds Sales Units ReplenishmentUnits Deviation! belowt threshold Deviationa abovet threshold Figure 3: Examining the cumulative probability of consecutive: zero-sales events to identify potential OSAC alertsb basedon deviationo observedb betweenActualond: ExpectedSalesUnits out-of-stocki issues Figure 4: Depressed sales due to poor product placement leading to an on-shelf availability problem",
        "The trick for scrutinizing zero-sales events at the item level is to understand the probability of which at least one unit of a product sells on a given day and to then To detect these kinds of problems, it is helpful to compare actual sales to",
        "set a cumulative probability threshold for consecutive days reflecting zero-sales. those forecasted for the period. While not every missed sales goal indicates When the cumulative probability of back-to-back zero-sales events exceeds the an on-shelf availability problem, a sustained miss might signal a problem that",
        "threshold, it's time for the inventory of that product to be examined. requires further attention. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 19 How we approach out-of-stocks with the Databricks forecast for the business's needs. Scalable patterns ensure data science tasks Lakehouse Platform",
        "are also tackled in an efficient and timely manner with little deviation from the standard approaches data scientists typically employ. The evaluation of phantom inventories, safety stock violations, zero-sales events and on-shelf availability problems requires a platform capable of performing a",
        "And the SQL Analytics interface, as well as robust integrations with Tableau and wide range of tasks. Inventory and sales data must be aggregated and reconciled Power BI, allows analysts to consume the results of the data scientists' and data",
        "at a per-period level. Complex logic must be applied across these data to examine engineers' work without having to first port the data to alternative platforms. aggregate and series patterns. Forecasts may need to be generated for a wide",
        "range of products across numerous locations. And the results of all this work Getting started must be made accessible to the business analysts responsible for scrutinizing the findings before soliciting action from those in the field. Be sure to check out and download the notebooks for out-of-stock modeling.",
        "As with any of our Solution Accelerators, these are a foundation for a full solution. Databricks provides a single platform capable of all this work. The elastic If you would like help with implementing a full out-of-stock or supply chain",
        "scalability of the platform ensures that the processing of large volumes of solution, go visit our friends at Tredence. data can be performed in an efficient and timely manner. The flexibility of its development environment allows data engineers to pivot between common",
        "To see these features in action, please check out the following notebooks languages, such as SQL and Python, to perform data analysis in a variety of modes. demonstrating how Tredence tackled out-of-stocks on the Databricks platform: Pre-integrated libraries provide support for classic time series forecasting",
        "algorithms and techniques, and easy programmatic installations of alternative OSA 1: Data Preparation e libraries such as Facebook Prophet allow data scientists to deliver the right OSA 2: Out-of-Stocks e OSA 3: On-Shelf Availability e databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 20 CHAPTER 4:",
        "Using Dynamic Time Introduction Warping and MLflow to The phrase \"dynamic time warping,\" at first read, might evoke images of Marty McFly driving his DeLorean at Detect Sales Trends 88 MPH in the \"Back to the Future\" series. Alas, dynamic time warping does not involve time travel; instead,",
        "it's a technique used to dynamically compare time series data when the time indices between comparison Part 1of our Using Dynamic Time Warping data points do not sync up perfectly. and MLflow to Detect Sales Trends series",
        "As we'll explore below, one of the most salient uses of dynamic time warping is in speech recognition - - determining whether one phrase matches another, even if the phrase is spoken faster or slower than its",
        "comparison. You can imagine that this comes in handy to identify the \"wake words\" used to activate your Google Home or Amazon Alexa device - even if your speech is slow because you haven't yet had your daily cup(s) of coffee.",
        "Dynamic time warping is a useful, powerful technique that can be applied across many different domains. Once you understand the concept of dynamic time warping, it's easy to see examples of its applications in daily life, and its exciting future applications. Consider the following uses: By Ricardo Portilla, Brenner Heintz",
        "Financial markets: comparing stock trading data over similar time frames, even if they do not match and Denny Lee up perfectly. For example, comparing monthly trading data for February (28 days) and March (31 days). Try this notebook in Databricks *",
        "Wearable fitness trackers: more accurately calculating a walker's speed and the number of steps, even if their speed varied over time Route calculation: calculating more accurate information about a driver's ETA, if we know something",
        "about their driving habits (for example, they drive quickly on straightaways but take more time than average to make left turns) Data scientists, data analysts and anyone working with time series data should become familiar with this",
        "technique, given that perfectly aligned time series comparison data can be as rare to see in the wild as perfectly \"tidy\" data. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 21 In this blog series, we will explore: The basic principles of dynamic time warping",
        "Running dynamic time warping on sample audio data Running dynamic time warping on sample sales data using MLflow Dynamic time warping The objective of time series comparison methods is to produce a distance metric between two input time series. The similarity or dissimilarity of two time series",
        "is typically calculated by converting the data into vectors and calculating the EUCLIDEAN MATCHING Euclidean distance between those points in vector space. Dynamic time warping is a seminal time series comparison technique that has been used for speech and word recognition since the 1970s with sound waves",
        "as the source; an often cited paper is \"Dynamic time warping for isolated word recognition based on ordered graph searching techniques.\" Background This technique can be used not only for pattern matching, but also anomaly Source: Wikimedia Commons detection (e.g, overlap time series between two disjoint time periods to",
        "DYNAMIC TIME WARP MATCHING File: Euclidean.v.DTW.pe understand if the shape has changed significantly, or to examine outliers). For example, when looking at the red and blue lines in the following graph, note the Two time series (the base time series and new time series) are considered similar",
        "traditional time series matching (i.e., Euclidean matching) is extremely restrictive. when it is possible to map with function f(x) according to the following rules SO as On the other hand, dynamic time warping allows the two curves to match up to match the magnitudes using an optimal (warping) path.",
        "evenly even though the X-axes (ie., time) are not necessarily in sync. Another way f(x) maps to f(x) when i < =j to think of this is as a robust dissimilarity score where a lower number means the series is more similar.",
        "f(x) maps to f(x) only when (j - i) is within fixed range databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 22 Sound pattern matching Below are visualizations using matplotlib of the four audio clips: Traditionally, dynamic time warping is applied to audio clips to determine the",
        "Clip 1: This is our base time series based on the quote \"Doors and corners, similarity of those clips. For our example, we will use four different audio clips kid. That's where they get you.\" based on two different quotes from a TV show called The Expanse. There are four",
        "audio clips (you can listen to them below, but this is not necessary) = three of Clip 2: This is a new time series [v2] based on clip 1 where the intonation and them (clips 1, 2 and 4) are based on the quote speech pattern are extremely exaggerated",
        "Clip 3: This is another time series that's based on the quote \"You walk into a \"Doors and corners, kid. That's where they get you.\" room too fast, the room eats you.\" with the same intonation and speed as clip 1 And in one clip (clip 3) is the quote",
        "Clip 4: This is a new time series [v3] based on clip 1 where the intonation and speech pattern is similar to clip 1 \"You walk into a room too fast, the room eats you.\" Clip1 Doors and corners, kid. Clip 2 Doors and kid. 1 corners,",
        "Clip1 Doors and corners, kid. Clip 2 Doors and corners, kid. That's where they get you. M] That's where they get you. [v2] That's where they get you. [v] That's where they get you. [v2] 0:00/0:06 4 : 0:00/0:08 4) : Clip 3 You walk into a room too fast,",
        "Clip 4 Doors and corners, kid. the room eats you. That's where they get you. [v3] 0:00/0:07 4) : 0:00/0:07 : Clip 3 You walk into a room too fast, Clip 4 Doors and corners, kid. the room eats you. That's where they get you. [v3]",
        "Quotes are from \"The Expanse\" databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 23 The code to read these audio clips and visualize them using Matplotlib can be As noted below, the two clips (in this case, clips 1 and 4) have different intonations",
        "summarized in the following code snippet. (amplitude) and latencies for the same quote. from scipy.io import wavfile from matplotlib import pyplot as plt from mat tplotlib.pyplot import figure # Read stored audio files for comparison fs, data = wavile.read(\"/dbfs/Eolder/clipl.wav\") # Set plot style plt.style.use Cseabora-whitegria? # Create subplots",
        "ax plt.subplot (2, 2, 1) ax.plot (datal, color-/#67A0DA) # Display created figure fig-plt.show () Doors and corners, kid. That's where they get you. display (fig) The full code base can be found in the notebook Dynamic Time Warping Background. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION",
        "24 If we were to follow a traditional Euclidean matching (per the following graph), even With dynamic time warping, we can shift time to allow for a time series comparison if we were to discount the amplitudes, the timings between the original clip (blue) between these two clips.",
        "and the new clip (yellow) do not match. EUCLIDEAN MATCHING DYNAMIC TIME WARPING corners' that's kid where they where, they doors and doors kid corners get and that's get you you databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 25",
        "For our time series comparison, we will use the fastdtw PyPi library; the 20000 instructions to install PyPi libraries within your Databricks workspace can be found here: Azure IAWS. By using fastdtw, we can quickly calculate the distance between 15000 the different time series. 10000 5000 from fastdtw import fastdtw",
        "# Distance between clip 1 and clip 2 distance = fastdtw (data_clipl, data_clip2) [0] -5000 print (\"The distance between the two clips is 8s\" 8 distance) -10000 -15000 50000 100000 150000 200000 250000 300000 350000 400000 450000 The full code base can be found in the notebook Dynamic Time Warping",
        "Background. Some quick observations: BASE QUERY DISTANCE Clip 1 Clip 2 480148446.0 As noted in the preceding graph, clips 1 and 4 have the shortest distance, as the audio clips have the same words and intonations Clip 3 310038909.0 The distance between clips 1 and 3 is also quite short",
        "longer Clip 4 293547478.0 (though than when compared to clip 4) = even though they have different words, they are using the same intonation and speed Clips 1 and 2 have the longest distance due to the extremely exaggerated intonation and speed even though they are using the same quote",
        "As you can see, with dynamic time warping, one can ascertain the similarity of two different time series. Next Now that we have discussed dynamic time warping, let's apply this use case to detect sales trends. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 26 CHAPTER 4:",
        "Using Dynamic Time Background Warping and MLflow to Imagine that you own a company that creates 3D printed products. Last year, you knew that drone Detect Sales Trends propellers were showing very consistent demand, SO you produced and sold those, and the year before you",
        "sold phone cases. The new year is arriving very soon, and you're sitting down with your manufacturing team Part 2 of our Using Dynamic Time Warping to figure out what your company should produce for next year. Buying the 3D printers for your warehouse",
        "and MLflow to Detect Sales Trends series put you deep into debt, SO you have to make sure that your printers are running at or near 100% capacity at all times in order to make the payments on them.",
        "Since you're a wise CEO, you know that your production capacity over the next year will ebb and flow - there will be some weeks when your production capacity is higher than others. For example, your capacity",
        "might be higher during the summer (when you hire seasonal workers), and lower during the third week of every month (because of issues with the 3D printer filament supply chain). Take a look at the chart below to see your company's production capacity estimate: By Ricardo Portilla, Brenner Heintz",
        "and Denny Lee Optimal Weekly Product Sales 35 30 Try this notebook series 25 (in DBC format) in Databricks * 20 I 15 10 10 20 30 40 50 Week databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 27",
        "Your job is to choose a product for which weekly demand meets your production capacity as closely as possible. You're looking over a catalog of products which includes last year's sales numbers for each product, and you think this year's sales will be similar.",
        "If you choose a product with weekly demand that exceeds your production capacity, then you'll have to cancel customer orders, which isn't good for business. On the other hand, if you choose a product without",
        "enough weekly demand, you won't be able to keep your printers running at full capacity and may fail to make the debt payments. Dynamic time warping comes into play here because sometimes supply and demand for the product you",
        "choose will be slightly out of sync. There will be some weeks when you simply don't have enough capacity to meet all of your demand, but as long as you're very close and you can make up for it by producing",
        "more products in the week or two before or after, your customers won't mind. If we limited ourselves to comparing the sales data with our production capacity using Euclidean matching, we might choose a",
        "product that didn't account for this and leave money on the table. Instead, we'll use dynamic time warping to choose the product that's right for your company this year. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 28 Load the product sales data set",
        "Calculate distance to optimal time series by product code We will use the weekly sales transaction data set found in the UCI Data Set # Calculate distance via dynamic time warping between product code and Repository to perform our sales-based time series analysis. (Source attribution: optimal time series",
        "import numpy as np James Tan, emestanscsussedusg Singapore University of Social Sciences) import ucrdtw def get_ keyed_values (s) : import pandas as pd return (s[01, s[1:1) # Use Pandas to read this data def compute.distance (row) : sales_pdf - pd.read_csv (sales_dbfspath, header=': infer')",
        "return (row [0], ucrdtw.ucrdtw (list (row [1J[0:521), list (optimal pattern), 0.05, True) [1]) # Review data display spark.createatarame (sales_pdf)) ts_values = pd.DataFrame (np. apply_along_axis (get.keyed.values, 1, sales.paf.values)) distances = pd.DataFrame (ap.apply_along.axis (compute.distance, 1, ts",
        "Product_Code V wo W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 V W11 W12 W13 values.values) distances.columns = 'dtw_dist'l ['pcode', P1 11 12 10 13 12 14 21 14 11 14 16 9 P2 7 o 3 2 P3 7 11 10 13 12 14 P4",
        "12 8 13 13 13 Using the calculated dynamic time warping \"distances\" column, we can view the P5 8 5 13 11 14 11 18 distribution of DTW distances in a P6 3 3 2 7 6 3 6 6 5 histogram. P7 8 3 8 3 10 2 3",
        "P8 8 10 10 0 15 0 DTW Distances for Each Pairwise Product Sales Comparison 30 70 60 Each product is represented by a row, and each week in the year is represented 50 by a column. Values represent the number of units of each product sold per week. 40",
        "30 There are 811 products in the data set. 20 10 10 Distances databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 29 From there, we can identify the product codes closest to the optimal sales trend",
        "After running this query, along with the corresponding query for the product codes (i.e., those that have the smallest calculated DTW distance). Since we're using that are furthest from the optimal sales trend, we were able to identify the two",
        "Databricks, we can easily make this selection using a SQL query. Let's display those products that are closest and furthest from the trend. Let's plot both of those that are closest. products and see how they differ. 8sql Top 10 product codes closest to the optimal sales trend",
        "Comparing Optimal Sales Trends With P675 and P716 select pcode, cast (dtw_ dist as float) as dtw_dist from distances order 35 Optimal Sales Trend by cast (dtw_dist as float) limit 10 30 P716 P675 25 20 6.0 15 5.5 5.0 10 4.5 4.0 6 3.5 à 3.0 10 20 30",
        "2.5 2.0 1.5 1.00 As you can see, Product #675 (shown in the orange triangles) represents the - 0.50 0.00 best match to the optimal sales trend, although the absolute sales are P675 P703 weekly P358 P697 P816 P601 P674 P372 P476 P694 pcode",
        "lower than we'd like (we'll remedy that later). This result makes sense since we'd expect the product with the closest DTW distance to have peaks and valleys that somewhat mirror the metric we're comparing it to. (Of course, the exact time",
        "index for the product would vary on a week-by-week basis due to dynamic time warping.) Conversely, Product #716 (shown in the green stars) is the product with the worst match, showing almost no variability. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 30",
        "Finding the optimal product: Small DTW distance and Using MLflow to track best and worst products, similar absolute sales numbers along with artifacts Now that we've developed a list of products that are closest to our factory's MLflow is an open source platform for managing the machine learning lifecycle,",
        "projected output (our \"optimal sales trend\"), we can filter them down to those that including experimentation, reproducibility and deployment. Databricks notebooks have small DTW distances as well as similar absolute sales numbers. One good offer a fully integrated MLflow environment, allowing you to create experiments,",
        "candidate would be Product #202, which has a DTW distance of 6.86 versus the log parameters and metrics, and save results. For more information about population median distance of 7.89 and tracks our optimal trend very closely. getting started with MLflow, take a look at the excellent documentation.",
        "# Review P202 weekly sales MLflow's design is centered around the ability to log all of the inputs and y_p202 - sales_pdf Isales_pdf 'Product_ Code'] P202'1.values [0] [1:53] outputs of each experiment we do in a systematic, reproducible way. On every",
        "pass through the data, known as a \"run,\" we're able to log our experiment's: Parameters: The inputs to our model Metrics: The output of our model, or measures of our model's success Comparing Optimal Sales Trends With Weekly Sales for P2 60 P716 Optimal Sales Trend",
        "Artifacts: Any files created by our model = for example, PNG plots or 50 CSV data output 40 Models: The model itself, which we can later reload and use to serve predictions 10 20 30 40 50 databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 31",
        "In our case, we can use it to run the dynamic time warping algorithm several times import mlflow over our data while changing the \"stretch factor,\" the maximum amount of warp def run_ DTW (ts_stretch factor) :",
        "that can be applied to our time series data. To initiate an MLflow experiment, and # calculate DTW distance and Z-score for each product <strong>with mlflw.start_run () as run:</strong> allow for easy logging using mllow.log_param (), mllow.log.metrico. mllow.log.artifact (), and mlflow.log.model (), we wrap our main function",
        "# Log Model using Custom Flavor dtw_model = 'stretch_factor\" : float t.stretch.factor), using: 'pattern' : optimal pattern) <strongpmlflow cus om._lavor.log_model (dtw_model, artifact_ path= \"model\") </strong> iwith mllow.start_run () as run: # Log our stretch factor parameter to MLflow trogallow.ogaran Catretch_factor\", ts.stretch._factor) </ strong>",
        "as shown in the abbreviated code at right. # Log the median DTW distance for this run StroNpIegte (\"Median Distance\", distance._median) </ strong> # Log artifacts CSV file and PNG plot to MLflow <strongpmitow.logartifact/ascore.outliers + str (ts_stretch factor) + .csv') mllow.logartifact com.distMistogran png') return run.infoc/strong)",
        "stretch_ factors_ to_test = [0.0, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5] for n in stretch facto S to_test: run_DTW (n) With each run through the data, we've created a log of the \"stretch factor\" parameter being used, and a log of products we classified as being outliers based",
        "upon the Z-score of the DTW distance metric. We were even able to save an artifact (file) of a histogram of the DTW distances. These experimental runs are saved locally on Databricks and remain accessible in the future if you decide to",
        "view the results of your experiment at a later date. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 32 Now that MLflow has saved the logs of each experiment, we can go back through reload models after they are initially constructed, as demonstrated in this",
        "and examine the results. From your Databricks notebook, select the \"Runs\" icon in blog post. For example, when using MLflow with scikit-learn, logging a model is the upper right-hand corner to view and compare the results of each of our runs.",
        "as easy as running the following code from within an experiment: wwwyoutubecom/watchpl-62PAPo-2ZU mlflow. sklearn.log.model (model=sk model, artifact path-sk_mode. ath\" Not surprisingly, as we increase our \"stretch factor,\" our distance metric decreases. MLflow also offers a \"Python function\" flavor, which allows you to save any",
        "Intuitively, this makes sense: as we give the algorithm more flexibility to warp the model from a third-party library (such as XGBoost or spaCy), or even a simple time indices forward or backward, it will find a closer fit for the data. In essence,",
        "Python function itself, as an MLflow model. Models created using the Python we've traded some bias for variance. function flavor live within the same ecosystem and are able to interact with other MLflow tools through the Inference API. Although it's impossible to plan for every",
        "use case, the Python function model flavor was designed to be as universal and Logging models in MLflow flexible as possible. It allows for custom processing and logic evaluation, which can come in handy for ETL applications. Even as more \"official\" model flavors",
        "MLflow has the ability to not only log experiment parameters, metrics and artifacts come online, the generic Python function flavor will still serve as an important (like plots or CSV files), but also to log machine learning models. An MLflow model",
        "\"catchall,\" providing a bridge between Python code of any kind and MLflow's is simply a folder that is structured to conform to a consistent API, ensuring robust tracking toolkit. compatibility with other MLflow tools and features. This interoperability is very",
        "powerful, allowing any Python model to be rapidly deployed to many different Logging a model using the Python function flavor is a straightforward process. types of production environments. Any model or function can be saved as a model, with one requirement: It",
        "must take in a pandas DataFrame as input, and return a DataFrame or NumPy MLflow comes pre-loaded with a number of common model \"flavors\" for many array. Once that requirement is met, saving your function as an MLflow model of the most popular machine learning libraries, including scikit-learn, Spark MLlib,",
        "involves defining a Python class that inherits from PythonModel, and overriding the PyTorch, TensorFlow, and others. These model flavors make it trivial to log and -predict () method with your custom function, as described here. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 33",
        "Loading a logged model from one of our runs Next steps Now that we've run through our data with several different stretch factors, As you can see, our MLflow model is predicting new and unseen values with ease.",
        "the natural next step is to examine our results and look for a model that did And since it conforms to the Inference API, we can deploy our model on any particularly well according to the metrics that we've logged. MLflow makes it easy",
        "serving platform (such as Microsoft Azure ML or Amazon SageMaker), deploy it as to then reload a logged model, and use it to make predictions on new data, using a local REST API end point, or create a user-defined function (UDF) that can easily the following instructions:",
        "be used with Spark SQL. In closing, we demonstrated how we can use dynamic 1. Click on the link for the run you'd like to load our model from time warping to predict sales trends using the Databricks Unified Data Analytics",
        "Platform. Try out the Using Dynamic Time Warping and MLflow to Predict Sales 2. Copy the \"Run ID\" Trends notebook with Databricks Runtime for Machine Learning today. 3. Make note of the name of the folder the model is stored in. In our case, it's simply named \"model\"",
        "4. Enter the model folder name and Run ID as shown below: import custom flavor as mllox.custom_lavor loaded_model mlflow_ custom flavor load model artifact.path-modelr run id-'e26961b25c4d4402a9a5a7a679fc8052\") To show that our model is working as intended, we can now load the model and",
        "use it to measure DTW distances on two new products that we've created within the variable new.sales_units : # use the model to evaluate new products found in new_sales_units output - loadedmodel-predict (new_sales.units) print (output) databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 34 CHAPTER 5:",
        "Detecting Financial Fraud Detecting fraudulent patterns at scale using artificial intelligence is a challenge, no matter the use case. Scale With Decision The massive amounts of historical data to sift through, the complexity of the constantly evolving machine at",
        "learning and deep learning techniques, and the very small number of actual examples of fraudulent Trees and MLflow on behavior are comparable to finding a needle in a haystack while not knowing what the needle looks like. In Databricks",
        "the financial services industry, the added concerns with security and the importance of explaining how fraudulent behavior was identified further increase the complexity of the task. df Rules df.riticolum-labev, to Identify Known Fraud-based .whe (df.oldbalanceorg 56900) & (df.type == \"TRANSFER\") df.newbalanceDest = 105)) I",
        "(df.oldbalanceorg 56900) & (df.neubalanceoris S= 12)) I 0101 Financial Data ), (df.oldbalanceorg 56900) & (df.nevbalanceoris 12) & (df.amount 1160000) ).otherwise(0)) d Rules to Identify Fraud-based By Elena Boiarskaia, Navin Albert E and Denny Lee (df.oldbalanceorg 56900) (df.type \"TRANSFER\") C (df.oldbalanceorg 56900) & (df.newbalanceorig 12)) 0101 Financial Data",
        "(df.oldbalanceorg 56900) & (df.newbalanceoris 12) & (df.amount 1160000) Try this notebook in Databricks * ).otherwise(e)) ), To build these detection patterns, a team of domain experts comes up with a set of rules based on how",
        "fraudsters typically behave. A workflow may include a subject matter expert in the financial fraud detection space putting together a set of requirements for a particular behavior. A data scientist may then take a",
        "subsample of the available data and select a set of deep learning or machine learning algorithms using these requirements and possibly some known fraud cases. To put the pattern in production, a data engineer may convert the resulting model to a set of rules with thresholds, often implemented using SQL.",
        "databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 35 This approach allows the financial institution to present a clear set of characteristics that lead to the identification of a fraudulent transaction that is compliant with the General Data Protection Regulation (GDPR). However, this approach also poses",
        "numerous difficulties. The implementation of a fraud detection system using a hardcoded set of rules is very brittle. Any changes to the fraud patterns would take Data Engineer a very long time to update. This, in turn, makes it difficult to keep up with and adapt",
        "to the shift in fraudulent activities that are happening in the current marketplace. Datal Engineer Data Analyst Data Scientist 0 Data Analyst Data Scientist X Additionally, the systems in the workflow described above are often siloed, with In this blog, we will showcase how to convert several such rule-based detection",
        "the domain experts, data scientists and data engineers all compartmentalized. use cases to machine learning use cases on the Databricks platform, unifying the The data engineer is responsible for maintaining massive amounts of data and key players in fraud detection: domain experts, data scientists and data engineers.",
        "translating the work of the domain experts and data scientists into production level We will learn how to create a machine learning fraud detection data pipeline and",
        "code. Due to a lack of a common platform, the domain experts and data scientists visualize the data in real time, leveraging a framework for building modular features have to rely on sampled down data that fits on a single machine for analysis. This",
        "from large data sets. We will also learn how to detect fraud using decision trees leads to difficulty in communication and ultimately a lack of collaboration. and Apache SparkM MLlib. We will then use MLflow to iterate and refine the model to improve its accuracy. databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 36 Solving with machine learning There is a certain degree of reluctance with regard to machine learning models in This simultaneously mitigates three concerns: the financial world, as they are believed to offer a \"black box\" solution with no way",
        "1. The lack of training labels of justifying the identified fraudulent cases. GDPR requirements, as well as financial 2. The decision of what features to use regulations, make it seemingly impossible to leverage the power of data science. 3. Having an appropriate benchmark for the model",
        "However, several successful use cases have shown that applying machine learning to detect fraud at scale can solve a host of the issues mentioned above. Training a machine learning model to recognize the rule-based fraudulent behavior flags offers a direct comparison with the expected output via a confusion matrix.",
        "The Databricks Lakehouse Platform Provided that the results closely match the rule-based detection pattern, this approach helps gain confidence in machine learning-based fraud prevention with Databricks Notebooks the skeptics. The output of this model is very easy to interpret and may serve as",
        "a baseline discussion of the expected false negatives and false positives when compared to the original detection pattern. Financial Data Data Engineering Data Analytics Machine Learning Furthermore, the concern with machine learning models being difficult to interpret",
        "may be further assuaged if a decision tree model is used as the initial machine bastbonwi With wegeed Data Democratization learning model. Because the model is being trained to a set of rules, the decision tree is likely to outperform any other machine learning model. The additional",
        "Training a supervised machine learning model to detect financial fraud is very benefit is, of course, the utmost transparency of the model, which will essentially difficult due to the low number of actual confirmed examples of fraudulent show the decision-making process for fraud, but without human intervention and",
        "behavior. However, the presence of a known set of rules that identify a particular need the to hard code any rules or thresholds. Of course, it must be understood",
        "type of fraud can help create a set of synthetic labels and an initial set of features. that the future iterations of the model may utilize a different algorithm altogether The output of the detection pattern that has been developed by the domain",
        "to achieve maximum accuracy. The transparency of the model is ultimately experts in the field has likely gone through the appropriate approval process to achieved by understanding the features that went into the algorithm. Having be put in production. It produces the expected fraudulent behavior flags and",
        "interpretable features will yield interpretable and defensible model results. may, therefore, be used as a starting point to train a machine learning model. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 37 The biggest benefit of the machine learning approach is that after the initial",
        "Exploring the data modeling effort, future iterations are modular, and updating the set of labels, Creating the DataFrames: Now that we have uploaded the data to Databricks File",
        "features or model type is very easy and seamless, reducing the time to production. System (DBFS), we can quickly and easily create DataFrames using Spark SQL. This is further facilitated on the Databricks Collaborative Notebooks where the",
        "domain experts, data scientists and data engineers may work off the same data set # Create df DataFrame which contains our simulated financial fraud detection dat taset at scale and collaborate directly in the notebook environment. So let's get started! df = spark.sql (\"select step, type, amount, nameOrig, oldbalanceorg,",
        "newbalanceorig, nameDest, oldbalanceDest, newbalanceDest from sim_fin fraud_detection\"? Ingesting and exploring the data Now that we have created the DataFrame, let's take a look at the schema and the We will use a synthetic data set for this example. To load the data set yourself,",
        "first thousand rows to review the data. please download it to your local machine from Kaggle and then import the data via # Review the schema of your data Import Data = Azure and AWS. df.printSchema () root step: integer (nullable = true)",
        "The PaySim data simulates mobile money transactions based on a sample of real - type: string (nullable = true) transactions extracted from one month of financial logs from a mobile money amount: double (nullable = true) nameOrig: string (nullable = true)",
        "service implemented in an African country. The below table shows the information oldbalanceorg: double (nullable true) newbalanceOrig: double (nullable = true) that the data set provides: nameDest: string (nullable = true) oldbalanceDest: double (nullable = true) newbalanceDest: double (nullable = true) Column Name Description step",
        "maps aunit toftimei int ther real world. Int this case stepi is1 hour oft time.\" Total steps 744 (30 days simulation). type CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER. amount amount ofthet transactioni ink local currency. PAYMENT 707T 0154988899 183195 176087.23 M408069119 nameOrig customer who started thet transaction",
        "PAYMENT 7861.64 C1912850431 176087.23 168225.59 M633326333 oldbalanceOrg initial balance before thet transaction DAMME newbalanceOrig new balance aftert thet transaction nameDest customer whoi ist ther recipient oft the transaction oldbalanceDest initial balance recipient before thet transaction. Note that therei isr noti informationi for customers that starty withMMerchants).",
        "newbalanceDest newb balance recipient aftert thet transaction. Notet thatt thereisnoti informationi for customers thats start with M (Merchants). databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 38 Types of transactions 500G Let's visualize the data to understand the types of transactions the data captures 450G 400G",
        "and their contribution to the overall transaction volume. 350G E 300G 8sql 250G Organize by Type 200G select type, count (1) from financials group by type 150G 100G 50G 1% 0.00 TRANSFER CASH_IN CASH_OUT PAYMENT DEBIT type - TRANSFER 8% CASHLIN CASH_OUT PAYMENT 35% DEBIT Rule-based model 22%",
        "We are not likely to start with a large data set of known fraud cases to train our model. In most practical applications, fraudulent detection patterns are identified by a set of rules established by the domain experts. Here, we create a column called label based on these rules. 34%",
        "# Rules to Identify Known Fraud-based df = df.withColumn (\"label\", F.when( (df.oldbalanceorg 56900) & (dF.newbalanceorig To get an idea of how much money we are talking about, let's also visualize the 56900) & (dF.newbalanceorig > 12) & (df.amount > 1160000)",
        "data based on the types of transactions and on their contribution to the amount of , 1 .otherwise (0)) cash transferred (i.e., sum(amount). 8sql select type, sum (amount) from financials group by type databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 39 Visualizing data flagged by rules",
        "Selecting the appropriate machine learning models These rules often flag quite a large number of fraudulent cases. Let's visualize the number of flagged transactions. We can see that the rules flag about 4% of the In many cases, a black box approach to fraud detection cannot be used. First, the",
        "cases and 11% of the total dollar amount as fraudulent. domain experts need to be able to understand why a transaction was identified as fraudulent. Then, if action is to be taken, the evidence has to be presented 8sql",
        "in court. The decision tree is an easily interpretable model and is a select label, count (1) as \"Transactions', sun (amount) as 'Total Amount' great starting from financials_labeled group by label point for this use case. Transactions Total Amount 4% B: label 4.0 B. X,4.2) (4,21.2) 11% O 3.0",
        ":: (X,<1.9) X,21.9) X,2.4) (X,22.4 X, 2.0 (Kxo.9) X22 * 1.0 9 (X,<0.4) (4,20.4) (X,0.4) (X,20.4) a 96% 89% 0.0 0.5 1.0 1.5 2.0 X,<1.4) (X,21.4) (X,0.9) (X,20.9) X, (a) (b) Creating the training set To build and validate our ML model, we will do an 80/20 split using",
        "randomSplit. This will set aside a randomly chosen 80% of the data for training and the remaining 20% to validate the results. # Split our dataset between training and test datasets (train, test) = df.randomsplitqt0.8, 0.2], seed-12345) databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 40",
        "Creating the ML model pipeline Visualizing the model To prepare the data for the model, we must first convert categorical variables to Calling display () on the last stage of the pipeline, which is the decision numeric using Stringindexer. We then must assemble all of the features",
        "tree model, allows us to view the initial fitted model with the chosen decisions we would like for the model to use. We create a pipeline to contain these feature at each node. This helps us to understand how the algorithm arrived at the",
        "preparation steps in addition to the decision tree model SO that we may repeat resulting predictions. these steps on different data sets. Note that we fit the pipeline to our training data display (dt.model.stages [-1]) first and will then use it to transform our test data in a later step.",
        "from pyspark.ml import Pipeline from Pyspark.ml.feature import StringIndexer from Pyspark.ml.feature import VectorAssembler from Peparkmidasication import Decisionfreclassile: cae44 de # Encodes a string column of labels to a column of label indices indexer = StringIndexer (inputCol = \"type\", outputCol = \"typeIndexed\")",
        "# VectorAssembler is a transformer that combines a given list of columns into a single vector column va - VectorAssembler (inputCols = l'typeIndexed\", \"amount\", \"oldbalanceOrg\", \"newbalanceorig\", \"oldbalanceDest\", \"newbalanceDest\", \"orgDiff\", destDiff\"), outputCol = \"features\") # Using the DecisionTree classifier model dt = Decisionfreclassiler (labelCol = \"label\", featuresCol = 20+",
        "\"features\", seed = 54321, maxDepth = 5) # Create our pipeline stages pipeline = Pipeline (stages= [indexer, va, dt]) 6 # View the Decision Tree model (prior to CrossValidator) dt_model = pipeline.fit (train) Visual representation of the decision tree model databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 41 Model tuning Model performance To ensure we have the best-fitting tree model, we will cross-validate the model We evaluate the model by comparing the Precision-Recal (PR) and area under the",
        "with several parameter variations. Given that our data consists of 96% negative ROC curve (AUC) metrics for the training and test sets. Both PR and AUC appear to and 4% positive cases, we will use the Precision-Recall (PR) evaluation metric to be very high. account for the unbalanced distribution.",
        "<b>from</b> pyspark.ml.tuning <b>importx/b> CrossValidator, # Build the best model (training and test datasets) ParamcridBuilder train_pred = CVModel u .transform (train) test_pred CVModel .transform (test) # Build the grid of different parameters paramGrid = ParamdridBuildero # Evaluate the model on training datasets .addGrid (dt.maxDepth, [5, 10, 15]) I pr_train evaluatorP.evaluate (train_pred)",
        "addGrid (dt.maxBins, [10, 20, 30]) I auc_train = evaluatorAvC.evaluate (train_pred) .build() # Evaluate the model on test datasets # Build out the cross validation pr_test = evaluatorP.evaluate (test_pred) crossval = CrossValidator (estimator dt, auc_test - evaluatorAvC.evaluate (test_pred) estimatorParamlaps = paramGrid, evaluator - evaluatorpR,",
        "# Print out the PR and AUC values numFolds = 3) print (\"PR train:\", pr_train) # Build the CV pipeline print (\"AUC train:\", auc_train) pipelinecv = Pipeline (stages= [indexer, va, crossval]) print (\"PR test:\", pr_test) print (\"AUC test:\", auc_test)",
        "# Train the model using the pipeline, paramet ter grid, and preceding BinaryClassifcationivaluator CVModel_u - pipelinecv.fit (train) # Output: # PR train: 0.953789498452128 AUC train: 0.998647996459481 PR test: 0.9539170535377599 # AUC test: 0.9984378183482442 databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 42",
        "To see how the model misclassified the results, let's use Matplotlib and pandas to # Reset the DataFrames for no fraud (dfn') and fraud (dfy') dfn = train.filter (train.label == 0) visualize our confusion matrix. dfy = train.filter (train.label == 1) # Calculate summary metrics Confusion Matrix (Unbalanced Test)",
        "N = train.count () 1200000 y = dfy.count () 1050000 P = y/N Fraud 50717 58 900000 # Create a more balanced training dataset train_b = dfn.sample (<b>Falsex/b>, P, seed = 92285) union (dfy) 750000 600000 # Print out metrics",
        "print (\"Total count: 8s, Fraud cases count: 8s, Proportion of fraud 450000 cases: 8s\" % (N, Y, p)) print (\"Balanced training dataset count: 8s\" % train_b.count ()) No Fraud 2421 1219030 300000 150000 # Output: # Total count: 5090394, Fraud cases count: 204865, Proportion of fraud Fraud No Fraud",
        "cases: 0.040245411258932016 Predicted label # Balanced training dataset count: 401898 Balancing the classes # Display our more balanced training dataset displaytraia..groupe, (\"label\") .count ()) We see that the model is identifying 2,421 more cases than the original rules identified. This is not as alarming, as detecting more potential fraudulent cases",
        "label could be a good thing. However, there are 58 cases that were not detected by a a the algorithm but were originally identified. We are going to attempt to improve our prediction further by balancing our classes using undersampling. That is,",
        "we will keep all the fraud cases and then downsample the non-fraud cases to match that number to get a balanced data set. When we visualize our new 49% 51% data set, we see that the yes and no cases are 50/50. databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 43 Updating the pipeline Review the results Now let's update the ML pipeline and create a new cross validator. Because we are Now let's look at the results of our new confusion matrix. The model misidentified",
        "using ML pipelines, we only need to update it with the new data set and we can only one fraudulent case. Balancing the classes seems to have improved the model. quickly repeat the same pipeline steps. # Re-run the same ML pipeline (including parameters grid) Confusion Matrix (Balanced Test) 1200000",
        "crossval_b CrossValidator (estimator = dt, estimatorParamaps - paramGrid, 1050000 evaluator - evaluatorAUC, numFolds = 3) Fraud 50774 1 900000 pipelinecvb = Pipeline (stages-lindexer, va, crossval_b]) 750000 # Train the model using the pipeline, parameter grid, and BinaryClassifcationivaluator using the train_b dataset 600000 CVModel_b - pipelinecv_b.fit (train_b) 450000",
        "# Build the best model (balanced training and full test datasets) NoF Fraud 488 1220963 300000 train_predb CvModel b.transform (train_b) test_predb cwModel_D.transform (test) 150000 # Evaluate the model on the balanced training datasets pr_train_b evaluatorP.evaluate (train_pred.b) Fraud No Fraud auc_train_b evaluatorAvC.evaluate (train_predb) Predicted label",
        "# Evaluate the model on full test datasets pr_test_b evaluatorP.evaluate (test_pred_b) Model feedback and using MLflow auc_test_b = evaluatorAUC.evaluate (test_pred_b) Once a model is chosen for production, we want to continuously collect feedback # Print out the PR and AUC values to ensure that the model is still",
        "the behavior of interest. Since we are print (\"PR train:\", pr_train_b) identifying print (\"AUC train: auc_train_b) starting with a rule-based label, we want to supply future models with verified true print (\"PR test:\", pr_test_b) print (\"AUC test:\", auc_test_b) labels based on human feedback. This stage is crucial for maintaining confidence",
        "and trust in the machine learning process. Since analysts are not able to review # Output: every single case, we want to ensure we are presenting them with carefully chosen # PR train: 0.999629161563572 # AUC train: 0.998071389056655",
        "cases to validate the model output. For example, predictions, where the model has # PR test: 0.990470917789063 low are # AUC test: 0.9997903902201509 certainty, good candidates for analysts to review. The addition of this type of feedback will ensure the models will continue to improve and evolve with the",
        "changing landscape. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 44 4 MLflow helps us throughout this cycle as we train different model versions. Conclusion We can keep track of our experiments, comparing the results of different model",
        "configurations and parameters. For example here, we can compare the PR and AUC We have reviewed an example of how to use a rule-based fraud detection of the models trained on balanced and unbalanced data sets using the MLflow UI.",
        "label and convert it to a machine learning model using Databricks with MLflow. Data scientists can use MLflow to keep track of the various model metrics and any This approach allows us to build a scalable, modular solution that will help us",
        "additional visualizations and artifacts to help make the decision of which model keep up with ever-changing fraudulent behavior patterns. Building a machine should be deployed in production. The data engineers will then be able to easily learning model to identify fraud allows us to create a feedback loop that helps",
        "retrieve the chosen model along with the library versions used for training as a jar the model to evolve and identify new potential fraudulent patterns. We have seen file to be deployed on new data in production. Thus, the collaboration between",
        "how a decision tree model, in particular, is a great starting point to introduce the domain experts who review the model results, the data scientists who update machine learning to a fraud detection program due to its interpretability and",
        "the models, and the data engineers who deploy the models in production will be excellent accuracy. strengthened throughout this iterative process. A major benefit of using the Databricks platform for this effort is that it allows for data scientists, engineers and business users to seamlessly work together wwwyoutupecom/watchveK.459-ki8",
        "throughout the process. Preparing the data, building models, sharing the results wwwyputubecom/atchveBVSyPymiw and putting the models into production can now happen on the same platform, allowing for unprecedented collaboration. This approach builds trust across the previously siloed teams, leading to an effective and dynamic fraud detection program.",
        "Try this notebook by signing up for a free trial in just a few minutes and get started creating your own models. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 45 CHAPTER 6: Fine-Grained Time Series",
        "Advances in time series forecasting are enabling retailers to generate more reliable demand forecasts. The challenge now is to produce these forecasts in a timely manner and at a level of granularity that allows Forecasting at Scale With",
        "the business to make precise adjustments to product inventories. Leveraging Apache Spark and Facebook Prophet and Apache Spark\" Prophet, more and more enterprises facing these challenges are finding they can overcome the scalability and accuracy limits of past solutions.",
        "In this chapter, we'll discuss the importance of time series forecasting, visualize some sample time series data, and then build a simple model to show the use of Facebook Prophet. Once you're comfortable building",
        "a single model, we'll combine Facebook Prophet with the magic of Spark to show you how to train hundreds of models at once, allowing you to create precise forecasts for each individual product-store combination at a level of granularity rarely achieved until now.",
        "Accurate and timely forecasting is now more important than ever Improving the speed and accuracy of time series analyses in order to better forecast demand for products and services is critical to retailers' success. If too much product is placed in a store, shelf and storeroom",
        "space can be strained, products can expire, and retailers may find their financial resources are tied up in By Bilal Obeidat, Bryan Smith inventory, leaving them unable to take advantage of new opportunities generated by manufacturers or shifts and Brenner Heintz",
        "in consumer patterns. If too little product is placed in a store, customers may not be able to purchase the products they need. Not only do these forecast errors result in an immediate loss of revenue to the retailer, Try this time series forecasting",
        "but over time consumer frustration may drive customers toward competitors. notebook in Databricks * New expectations require more precise time series forecasting methods and models For some time, enterprise resource planning (ERP) systems and third-party solutions have provided",
        "retailers with demand forecasting capabilities based upon simple time series models. But with advances in technology and increased pressure in the sector, many retailers are looking to move beyond the linear models and more traditional algorithms historically available to them.",
        "New capabilities, such as those provided by Facebook Prophet, are emerging PRGPHET from the data science community, and companies are seeking the flexibility to apply these machine learning models to their time series forecasting needs. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 46",
        "This movement away from traditional forecasting solutions requires retailers and Next, by viewing the same data on a monthly basis, we can see that the year- the like to develop in-house expertise not only in the complexities of demand",
        "over-year upward trend doesn't progress steadily each month. Instead, we see a forecasting but also in the efficient distribution of the work required to generate clear seasonal pattern of peaks in the summer months and troughs in the winter",
        "hundreds of thousands or even millions of ML models in a timely manner. Luckily, months. Using the built-in data visualization feature of Databricks Collaborative we can use Spark to distribute the training of these models, making it possible to",
        "Notebooks, we can see the value of our data during each month by mousing predict both demand for products and services and the unique demand for each over the chart. product in each location. sale Dec 1,2017 695,170 Visualizing demand seasonality in time series data 1.0M 800k",
        "To demonstrate the use of Prophet to generate fine-grained demand forecasts 600k for individual stores and products, we will use a publicly available data set from Kaggle. It consists of 5 years of daily sales data for 50 individual items across 10 2013 2015 MONTH 2017 different stores.",
        "To get started, let's look at the overall yearly sales trend for all products and stores. At the weekday level, sales peak on Sundays (weekday 0), followed by a hard drop",
        "As you can see, total product sales are increasing year over year with no clear sign on Mondays (weekday 1), then steadily recover throughout the rest of the week. of convergence around a plateau. year 30k 2013 11M 20k 2014 10M 2015 e 9.5M 10k 2016 2017 9.0M 8.5M 0.00",
        "8.OM WEEKDAY 3 5 2013 2014 2015 2016 2017 year databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 47 Getting started with a simple time series forecasting model on Facebook Prophet As illustrated above, our data shows a clear year-over-year upward trend in sales,",
        "Now that we have fit our model to the data, let's use it to build a 90-day forecast. along with both annual and weekly seasonal patterns. It's these overlapping In the code below, we define a data set that includes both historical dates and 90",
        "patterns in the data that Facebook Prophet is designed to address. days beyond, using Prophet's make.ature.Aatafrane method: Facebook Prophet follows the scikit-learn API, SO it should be easy to pick up for future_pd = model.make_future_dataframe ( periods=90, anyone with experience with sklearn. We need to pass in a two-column pandas",
        "freq-'d', DataFrame as input: the first column is the date, and the second is the value to includeMistory-Trve predict (in our case, sales). Once our data is in the proper format, building a model # predict over the dataset is easy: forecast_pd model.predict (future_pd) import pandas as pd",
        "That's it! We can now visualize how our actual and predicted data line up, as well as from fbprophet import Prophet a forecast for the future using Prophet's built-in -plot method. As you can see, the # instantiate the model and set parameters",
        "weekly and seasonal demand patterns we illustrated earlier are in fact reflected in model = Prophet interval width=0.95, the forecasted results. growth=' linear', alyeaoaltytal, predict_fig - model.plot (forecast_pd, xlabel-'date', ylabel-'sales') elysesaltyt, display (fig) ayeaoalityrte, seasonalitymode-'multiplicative # fit the model to historical data model.fit (history_pd) databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 48 This visualization is a bit busy. Bartosz Mikulski provides an excellent breakdown of it that is 50 well worth checking out. In a nutshell, the black HISTORICAL FORECASTED DATA DATA dots represent our actuals, with the darker 40",
        "blue line representing our predictions and the lighter blue band representing our (95%) uncertainty interval. 30 a 20 10 0 2017-02 2017-04 2017-06 2017-08 2017-10 2017-12 2018-02 2018-04 date databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 49 Training hundreds of time series forecasting models",
        "Of course, training models on a cluster of worker nodes (computers) requires more in parallel with Prophet and Spark cloud infrastructure, and this comes at a price. But with the easy availability of on-demand cloud resources, companies can quickly provision the resources they",
        "Now that we've demonstrated how to build a single model, we can use the power need, train their models and release those resources just as quickly, allowing them of Spark to multiply our efforts. Our goal is to generate not one forecast for the",
        "to achieve massive scalability without long-term commitments to physical assets. entire data set, but hundreds of models and forecasts for each product-store combination, something that would be incredibly time-consuming to perform as a The key mechanism for achieving distributed data processing in Spark is the sequential operation.",
        "DataFrame. By loading the data into a Spark DataFrame, the data is distributed across the workers in the cluster. This allows these workers to process subsets Building models in this way could allow a grocery store chain, for example, to",
        "of the data in a parallel manner, reducing the overall amount of time required to create a precise forecast for the amount of milk they should order for their perform our work. Sandusky store that differs from the amount needed in their Cleveland store,",
        "based upon the differing demand at those locations. Of course, each worker needs to have access to the subset of data it requires to do its work. By grouping the data on key values, in this case on combinations of",
        "store and item, we bring together all the time series data for those key values onto How to use Spark DataFrames to distribute the processing a specific worker node. of time series data store_item history Data scientists frequently tackle the challenge of training large numbers of models groupBy ('store', 'item')",
        "using a distributed data processing engine such as Spark. By leveraging a Spark cluster, individual worker nodes in the cluster can train a subset of models in We share the groupBy code here to underscore how it enables us to train",
        "parallel with other worker nodes, greatly reducing the overall time required to train many the entire collection of time series models. models in parallel efficiently, although it will not actually come into play until we set up and apply a custom pandas function to our data in the next section.",
        "databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 50 Leveraging the power of pandas user-defined functions With our time series data properly grouped by store and item, we now need to def forecaststore.iten (history_pd: pd.DataFrame) -> pd.DataFrame:",
        "train a single model for each group. To accomplish this, we can use a pandas # instantiate the model, configure the parameters function, which allows us to apply a custom function to each group of data in our model = Prophet ( DataFrame. interval.width-0.S, growth=' linear', aly,easoalty-ral,",
        "This function will not only train a model for each group, but also generate a result ely.sescmaltyt, aryacatyrte, set representing the predictions from that model. But while the function will train seasonalltymode-\"'ntiplicative and predict on each group in the DataFrame independent of the others, the results",
        "returned from each group will be conveniently collected into a single resulting # fit the model DataFrame. This will allow us to generate store-item level forecasts but present our model.fit (history_pd) results to analysts and managers as a single output data set. # configure predictions",
        "future_pd = model.make future_ dataframe ( periods=90, As you can see in the abbreviated code below, building our function is relatively freq=' d', straightforward. Unlike in previous versions of Spark, we can declare our functions include.history-Trve in a fairly streamlined manner, specifying the type of pandas object we expect to",
        "receive and return, ie., Python type hints. # make predictions results_pd = model.predict (future_pd) Within the function definition, we instantiate our model, configure it and fit it to the data it has received. The model makes a prediction, and that data is returned as # return predictions",
        "the output of the function. return results_pd databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 51 Now, to bring it all together, we use the groupBy command we discussed earlier Next steps to ensure our data set is properly partitioned into groups representing specific",
        "store and item combinations. We then simply add the applylnPandas function We have now constructed a forecast for each store-item combination. Using a",
        "to our DataFrame, allowing it to fit a model and make predictions on each grouping SQL query, analysts can view the tailored forecasts for each product. In the chart of data. below, we've plotted the projected demand for product #1 across 10 stores. As you",
        "can see, the demand forecasts vary from store to store, but the general pattern is The data set returned by the application of the function to each group is updated consistent across all of the stores, as we would expect.",
        "to reflect the date on which we generated our predictions. This will help us keep track of data generated during different model runs as we eventually take our Mar28,2018 23.0 functionality into production. 35 from Pypark.sgl.functions import current_date results = ( store_item.history 0 groupBy ('store', 'item') Vane DATE Mar01 apply(forecast_store_item)",
        "withColumn training.datel, current date ()) As new sales data arrives, we can efficiently generate new forecasts and append these to our existing table structures, allowing analysts to update the business's expectations as conditions evolve. To generate these forecasts in your Databricks environment, please import",
        "the following notebook: Fine-Grained Demand Forecasting With Spark 3. To access the prior version of this notebook, built for Spark 2.0, please click this link. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 52 CHAPTER 7: Applying Image Introduction Classification With PyTorch",
        "PyTorch Lightning is a great way to simplify your PyTorch code and bootstrap your deep learning workloads. Lightning on Databricks Scaling your workloads to achieve timely results with all the data in your lakehouse brings its own",
        "challenges, however. This article will explain how this can be achieved and how to efficiently scale your code with Horovod. Increasingly, companies are turning to deep learning in order to accelerate their advanced machine learning applications. For example, computer vision techniques are used nowadays to improve defect inspection for",
        "manufacturing: natural language processing is utilized to augment business processes with chatbots and neural network based recommender systems are used to improve customer outcomes. Training deep learning models, even with well-optimized code, is a slow process, which limits the ability of",
        "data science teams to quickly iterate through experiments and deliver results. As such, it is important to know how to best harness compute capacity in order to scale this up. In this article we will illustrate how to first structure your codebase for maximum code reuse, then show how",
        "to scale this from a small single node instance across to a full GPU cluster. We will also integrate it all with MLflow to provide full experiment tracking and model logging. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 53 Part 1 - Data Loading and Adopting",
        "PyTorch Lightning First, let's start with a target architecture. Cluster setup When scaling deep learning, it is important to start small and gradually scale up To follow through the notebooks, an instance type with at least 64GB RAM is",
        "the experiment in order to efficiently utilize expensive GPU resources. Scale up required. The modeling process is memory intensive and it is possible to run out your code to run on multiple GPUS within a single node before looking to scale",
        "of RAM with smaller instances, which can result in the following error. across multiple nodes to reduce code complexity. Databricks supports single-node clusters to support this very usage pattern. Fatal error: The Python kernel is unresponsive. See: Azure Single Node Clusters, AWS Single Node Clusters, GCP Single Node code",
        "The was built and tested on Databricks Runtime 10.4 LTS for Machine Learning Clusters. In terms of instance selection, NVIDIA T4 GPUS provide a cost-effective and also 11.1 ML. On DBR 10.4 LTS ML only Pytorch-ightning up to 1.6.5 is supported.",
        "instance type to start with. On AWS these are available in G4 instances. On On DBR 11.1 ML, Pytorch-ightning 1.7.2 has been tested. We have installed our Azure these are available in NCasT4_v3 instances. On GCP these are available libraries",
        "as workspace level libraries. Unlike using %pip, which installs libraries only as A2 instances. for the active notebook on the driver node, workspace libraries are installed on all nodes, which we will need later for distributed training. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 54",
        "Target Architecture DBR 10.4 LTS ML Configuration Install library Library Source PyTorch Lightaing Upload DBFS/S3 PyPI Maven CRAN Workspace DELTA LAKE Pétastorm DataLoader mlflow Package Pytorch-ighting:-165 Lightning Model land Training Loop with MLflow experiment Repository 0 logging Optional Cancel Install Figure 2: Key components DBR 11.1 ML Configuration Install library",
        "x The goal of this article is to build up a codebase structured as above. We will store our data using the open source Linux Foundation project Delta Lake. Library Source Under the hood, Delta Lake stores the raw data in Parquet format. Petastorm Upload DBFS/S3 PyPI Maven CRAN Workspace",
        "takes on the data loading duties and provides the interface between the Package lakehouse and our deep learning model. MLflow will provide experiment Pytorch-ighing:-172 tracking tools and allow for saving out the model to our model registry. Repository 0",
        "With this setup, we can avoid unnecessary data duplication costs as well as Optional Cancel Install govern and manage the models that we are training. Figure 1:Library configuration databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 55 Part 2 = Example Use Case and",
        "Leveraging your data lake for deep learning with Petastorm Library Overview Historically, data management systems like lakehouses and data warehouses have developed in parallel with, rather than in integration with, machine learning Example use case frameworks. As such, PyTorch DataLoader modules do not support the Parquet",
        "format out of the box. They also do not integrate with lakehouse metadata For this use case example, we will use the TensorFlow flowers data set. This data set structures like the hive metastore. will be used for a classification type problem where we are trying to identify which",
        "class of flower is which. The Petastorm project provides the interface between your lakehouse tables and PyTorch. It also handles data sharding across training nodes and provides a caching layer. Petastorm comes prepackaged in the Databricks Runtime for Machine Learning.",
        "Let's first become familiar with the data set and how to work with it. Of tulips is that (2) sunflowers (3) sunfiowers (3) note all we need to do to transform a Spark DataFrame into a Petastorm object is the code: peta_conv_df = make.spark.converte: (preprocessed.dr)",
        "Once we have the spark_converter object, we can convert that into a PyTorch roses (4) sunflowers (3) dandelion (0) DataLoader using: with peta_ conv df.make_ toro taloader (transfomm.spec-transfomm._fune) as converted dataset This then provides a converted.dataset DataLoader that we can use in our dandelion (0) PyTorch code as per normal.",
        "dandelion (0) dandelion (0) Open and follow the notebook titled Exploring the flowers dataset. A standard ML runtime cluster will be sufficient; there is no need to run this on a GPU cluster. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 56",
        "Simplify and structure your model = enter PyTorch Lightning By default, PyTorch code can get quite verbose. There is the model definition, This will help to make our code more portable and also improve organization.",
        "the training loop and the setup of the dataloaders. By default all this code is mixed These classes and functions will all be pulled into the main execution notebook, together, making it hard to swap data sets and models in and out, which can be",
        "via Srun, where the training hyperparameters will be defined and the code key for fast experimentation. actually executed. PyTorch Lightning helps to make this simpler by greatly reducing the boilerplate Model Class required to set up the experimental model and the main training loop. It is an",
        "Model Type (ie Resnet/ViT) opinionated approach to structuring PyTorch code, which allows for more readable Execution Notebook Metrics Optimiser (ie (ie CrossEntropyl Adam / SGD) maintainable code. Accuracy) Imports other modules For our project, we will break up the code into three main modules (ie Sets batch Hyperparameters size)",
        "Dataloaders Data Module (ie train/val/t Class test) Loads Spark dataframes Transformations (ie random PyTorch Model Creates Petastorm crop) DataLoaders and Transformations loader Train Loop Main Training Loop pl.Trainer Callbacks Figure 3: Code layout databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 57 Model definition: Main training loop:",
        "This module contains the code for the model architecture itself in a model class, This is the main training function. It takes the lightningPataModule and LightningModule. This is where the model architecture lives. For reference, this the LightningModule defining the model before feeding it into the Trainer",
        "is the module that needs updating to leverage popular model frameworks like class. We will instantiate the PyTorch Lightning Trainer and define all necessary timm, HuggingFace and the like. This module will also contain the definitions for callbacks here.",
        "optimizers. In this case, we just use SGD, but it can be parameterized to test out other types of optimizers. As we scale up the training process later on, we do not need some processes",
        "like MLflow logging to be run on all the processing nodes. As such, we will restrict DataLoader class: these to run on the first GPU only. Unlike with native PyTorch, where DataLoader code is intermixed with the model code, PyTorch Lightning allows us to split it out into a separate",
        "if device_id == 0: ughtningDataModule class. This allows for easier management of data sets and # we only need this on node 0 the ability to quickly test different interactions of your data sets. mltow.pyorch.autolog () When building a LightningDataModule with a Petastorm DataLoader, we feed",
        "in the spark_converter object rather than the raw spark dataframes. The Checkpointing our model during training is important for preserving progress, but Spark DataFrame is managed by the underlying Spark cluster, which is already will default PyTorch Lighting by",
        "handle this for us and we do not need to add code. distributed, whereas the PyTorch DataLoader will be distributed through other means later. Follow along in the Building the PyTorch Lightning Modules notebook. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 58 def train_hvd) :",
        "Part 3 = Scaling the Training Job hvd.init () # MLflow setup for the worker processes While single-GPU training is much faster than CPU training, it is often not enough. mlflow. set_tracking.uri (\"databricks\") oS. environ L DATABRICKS_HOST = db_host",
        "Proper production models can be large and the data sets required to train these os.environ! L DATABRICKS.TOMEN = db_token properly will be large too. Hence, we need to look into how we can scale our training across multiple GPUS. hvd_model = Latclasifcationkadel (class_count-5, learning_rate-le- 5*hvd.size (), device_id-hvd.rank (), device,count-hvd.sise ())",
        "The main approach to distributing deep learning models is via data parallelism, hvd_datamodule = FlowersDataModule (train_converter, val_converter, device_id-hvd.rank (), device.count-hvd.size ()) where we send a copy of the model to each GPU and feed in different shards of # gpus' paramet ter here should be 1 because the parallelism is",
        "data to each. This lets us increase the batch size and leverage higher learning rates controlled by Horovod to improve training times as discussed in this article. return train (hvd model, hvd_datamodule, gpus=1, strategy--horovoa\", device.ld-hvd.ranko, device.count-hd.asise ())",
        "To assist us in distributing the training job across GPUS, we can leverage Horovod. Horovod is another Linux Foundation project that offers us an alternative to This function will start Horovod hvd.init () and ensure that our DataModule and manually triggering distributed PyTorch processes across multiple nodes.",
        "train function are triggered with the correct node number hvd.rank () and total Databricks Runtime for Machine Learning includes by default the HorovodRunner number of devices hvd.size (). As discussed in this Horovod article we scale up class, which helps us scale on both single-node and multi-node training.",
        "the learning rate with the number of GPUS. In order to leverage Horovod, we need to create a new \"super\" train loop. hvd_model = LAtClasifcationkodal (class_ count=5, learning_rate-le- 5*hvd.size (), device.id-hvd.rank 0, device_ int-hvd.size ())",
        "Then we return the normal train loop with the GPU count set to 1as Horovod is handling the parallelism. Follow along in the Main Execution notebook and we will go through the ways to go from single- to multi-GPU. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION",
        "59 Step 1- Scaling on one node In our code, we set the default_dir parameter to a DBFS location in the train function. This is where PyTorch Lightning will save out the checkpoints. If we set a",
        "ckpt_ restore path to point to ckpt, the train function will resume training from that checkpoint. Driver Node FullBatch - V def train (model, dataloader, gpus:int-0, strategy :str-None, device_ id:int=0, Batch Batch Batch Shard1 Shard2 Shardn device_ count:int-1, logging evel-loging.INPO, default dir: CI dbfs, CI trainei logs ckpt restoreistr-None,",
        "GPU1 GPU1 (usuallyr GPUn mllow.experiment_id:str-lone) out at Kore To scale out our train function to multiple GPUS on one node, we will use HorovodRunner: Figure 4: Single-node scaling from sparkdl import HorovodRunner hr = HorovodRunner (np=-4, driver_log.verbosity-'all)",
        "Scaling on one node is the easiest way to scale. It is also very performant, as it hvd_model = hr.run (train_hvd) avoids the network traffic required for multi-node training. Unlike Spark-native ML Libraries, most deep learning training processes do not automatically recover",
        "Setting np to negative will make it run on the single driver node with 4 GPUS. from node failures. PyTorch Lightning, however, does automatically save out A positive np value will spread the training across other worker nodes. checkpoints for recovering training epochs. databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 60 Step 2 - Scaling across nodes Databricks runtime version 0 Runtime: 10.4 LTS ML (GPU, Scala 2.12, Spark 3.2.1) NVIDIA EULA 0 Use Photon. Acceleration 0 Preview Worker Node1 Worker Node 2 Worker Node n",
        "Use your own Docker container 0 Full Batch Autopilot options - Enable autoscaling 0 Batch Batch Batch Batch Batch Batch Enable autoscaling local storage 0 Split1 Split 2 Split3 Split4 Splitn Split n+l V V M Terminate after 50 minutes ofi inactivity 0 Worker type 0 Workers GPU1 GPU1",
        "GPU3 GPU4 GPUn GPUn+1 g4dn.4xlarge 64GB Memory, 1 GPU V B Driver type g4dn.4xlarge 64 GB Memory, 1 GPU V Figure 5: Multi-node scaling DBU /hour: 25.65 0 g4dn.4xlarge We have already wrapped our training function with a Horovod wrapper and we Advanced options",
        "have already successfully leveraged HorovodRunner for single-node multi-GPU processing. The final step is to go to a mutt-node/muti-GPU setup. If you have Figure 6: Multi-node cluster setup been following along with a single-node cluster, this is the point where we will",
        "move to a multi-node cluster. For the code that follows, we will use the cluster When running distributed training on Databricks, autoscaling is not currently configuration shown at right: supported, SO we will set our workers to a fixed number ahead of time. hr = HorovodRunner (np=8, driver_log.verbosity-'all)",
        "hvd_model = hr.run (train_hvd) databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 61 A common problem that will occur as you scale up your distributed deep learning Analysis job is that the Petastorm table has not been partitioned well enough to ensure that",
        "all the GPUS get a batch split. We need to make sure that we have at least as many When training deep neural networks, it is important to make sure we do not data partitions as we have GPUS.",
        "overfit the network. The standard way to manage this is to leverage early stopping. We address this in our code by setting the number of GPUS in the prepare data This process checks to make sure that with each epoch, we are still seeing function with the num devices variable.",
        "improvements to the metric that we set it to monitor. In this case, val_loss. For our experiments, we set min_ delta to 0.01, SO we expect to see at least 0.01 flowers_df, train_converter, val_converter = prepare_data (data_dir-Data Directory, .derierUKDEVICE)",
        "improvement to val_ loss each epoch. We set patience to be 10 SO the train loop will continue to run up to 10 epochs of no improvement before the training datamodule = FlowersDataModule (train_ C train converter, val_converter val converter)",
        "stops. We set this to make sure that we can eke out the last drop of performance. To keep the experimentation shorter, we also set a stopping.threshold of 0.55 SO we will stop the training process once our val_ loss drops below this level.",
        "This simply calls a standard Spark repartition command. We set the number of partitions to be a multiple of the num devices, the number of GPUS, to make sure With those parameters in mind, the results of our scaling experiments are as",
        "that the data set has sufficient partitions for all the GPUS we have allocated for the follows: training process. Insufficient partitions is a common cause of idling GPUS. Running Time VS Cluster Setup flowers_dataset = flowers_dataset.repartition (num_devices*2) 40 30 20 I 10 0",
        "1GPU Single Node 2GPU Single Node 4GPU Single Node 8GPU Multi Node databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 62 Get started Val Loss (Lower is Better) VS Cluster Setup 0.8 We have shown how you can leverage PyTorch Lightning within Databricks and",
        "wrap it with the HorovodRunner to scale across multiple nodes, as well as 0.6 provided some guidance on how to leverage EarlyStopping. Now it's your turn to try. 3 0.4 a - 0.2 Notebooks: - Exploring the flowers dataset * 0.0",
        "1 GPU Single Node 2GPU Single Node 4 GPU Single Node 8GPU Multi Node Building the PyTorch Lightning Modules * Main Execution Notebook * As we can see, in the Running Time VS Cluster Setup chart, we nearly halved",
        "the training time as we increased the system resources. The scaling is not quite linear, which is due to the overhead of coordinating the training process across See Also: different GPUS. When scaling deep learning, it is common to see diminishing HorovodRunner *",
        "returns and hence it is important to make sure that the train loop is efficient prior to adding GPUS. Petastorm * Deep Learning Best Practices * That is not the full picture, however, as per the best practices advised in our How (Not) to Scale Deep Learning *",
        "previous blog article, How (Not) To Scale Deep Learning in 6 Easy Steps, we used Leveling the Playing Field: HorovodRunner for Distributed EarlyStopping hence it is important to check the final validation loss achieved Deep Learning Training *",
        "by the various training runs as well. In this case, we set the stopping_threshold of 0.55. Interestingly, the single-GPU setup stopped at a worse validation loss than the multi-GPU setups. The single-GPU training ran till there were no more improvements in the val loss. databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 63 CHAPTER 8: Processing Geospatial The evolution and convergence of technology has fueled a vibrant marketplace for timely and accurate geospatial data. Every day, billions of handheld and loT devices along with thousands of airborne and Data at Scale With",
        "satellite remote sensing platforms generate hundreds of exabytes of location-aware data. This boom of Databricks geospatial big data combined with advancements in machine learning is enabling organizations across industries to build new products and capabilities. FRAUD AND ABUSE RETAIL FINANCIAL SERVICES HEALTHCARE",
        "Detect patterns of fraud and Site selection, urban planning, Economic distribution, loan risk Identifying disease epicenters, collusion (e.g. claims fraud, foot traffic analysis analysis, predicting sales at environmental impact on credit card fraud) retail, investments health, planning care DISASTER RECOVERY DEFENSE AND INTEL INFRASTRUCTURE ENERGY",
        "By Nima Razavi and Michael Johns Flood surveys, earthquake Reconnaissance, threat Transportation planning, Climate change analysis, energy mapping, response planning detection, damage assessment agriculture management, asset inspection, oil discovery housing development Maps leveraging geospatial data For example, numerous companies provide localized drone-based services such as mapping and site",
        "are used widely across industries, inspection (reference Developing for the Intelligent Cloud and Intelligent Edge). Another rapidly growing spanning multiple use cases, including disaster recovery, defense and intel, industry for geospatial data is autonomous vehicles. Startups and established companies alike are amassing",
        "infrastructure and health services. large corpuses of highly contextualized geodata from vehicle sensors to deliver the next innovation in self- driving cars (reference Databricks fuels wejo's ambition to create a mobility data ecosystem). Retailers and government agencies are also looking to make use of their geospatial data. For example, foot-traffic",
        "analysis (reference Building Foot-Traffic Insights Data Set) can help determine the best location to open a new store or, in the public sector, improve urban planning. Despite all these investments in geospatial data, a number of challenges exist. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 64",
        "Challenges analyzing geospatial at scale The first challenge involves dealing with scale in streaming and batch applications. Compatibility with various spatial formats poses the second challenge. There are The sheer proliferation of geospatial data and the SLAS required by applications many different specialized geospatial formats established over many decades as",
        "overwhelms traditional storage and processing systems. Customer data has been well as incidental data sources in which location information may be harvested: spilling out of existing vertically scaled geodatabases into data lakes for many Vector formats such as GeoJSON, KML, shapefile and WKT",
        "years now due to pressures such as data volume, velocity, storage cost and strict schema-on-write enforcement. While enterprises have invested in geospatial data, Raster formats such as ESRI Grid, GeOTIFF, JPEG 2000 and NITF few have the proper technology architecture to prepare these large, complex data",
        "Navigational standards such as used by AIS and GPS devices sets for downstream analytics. Further, given that scaled data is often required for Geodatabases accessible via JDBC/ODBC connections such as advanced use cases, the majority of Al-driven initiatives are failing to make it from PostgresQU/Postais pilot to production.",
        "Remote sensor formats from hyperspectral, multispectral, lidar and radar platforms OGC web standards such as WCS, WFS, WMS and WMTS Geotagged logs, pictures, videos and social media Unstructured data with location references In this blog post, we give an overview of general approaches to deal with the two",
        "main challenges listed above using the Databricks Unified Data Analytics Platform. This is the first part of a series of blog posts on working with large volumes of geospatial data. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 65 Scaling geospatial workloads with Databricks",
        "Databricks offers a unified data analytics platform for big data analytics and 2. Wrapping single-node libraries such as GeoPandas, Geospatial Data machine learning used by thousands of customers worldwide. It is powered by Abstraction Library (GDAL) or Java Topology Suite (JTS) in ad hoc user-",
        "Apache Spark, Delta Lake and MLflow with a wide ecosystem of third-party and defined functions (UDFS) for processing in a distributed fashion with Spark available library integrations. Databricks UDAP delivers enterprise-grade security, DataFrames. This is the simplest approach for scaling existing workloads",
        "support, reliability and performance at scale for production workloads. Geospatial without much code rewrite; however, it can introduce performance workloads are typically complex, and there is no one library fitting all use cases. drawbacks as it is more lift-and-shift in nature.",
        "While Apache Spark does not offer geospatial Data Types natively, the open source community as well as enterprises have directed much effort to develop 3. Indexing the data with grid systems and leveraging the generated index to spatial libraries, resulting in a sea of options from which to choose.",
        "perform spatial operations is a common approach for dealing with very large-scale or computationally restricted workloads. S2, GeoHex and Uber's There are generally three patterns for scaling geospatial operations such as spatial H3 are examples of such grid systems. Grids approximate geo features such joins or nearest neighbors:",
        "as polygons or points with a fixed set of identifiable cells, thus avoiding expensive geospatial operations altogether, and thus offer much better 1. Using purpose-built libraries that extend Apache Spark for geospatial scaling behavior. Implementers can decide between grids fixed to a single",
        "analytics. GeoSpark, GeoMesa, GeoTrellis and RasterFrames are a few of accuracy that can be somewhat lossy yet more performant or grids with such libraries used by our customers. These frameworks often offer multiple accuracies that can be less performant but mitigate against lossines.",
        "multiple language bindings and have much better scaling and performance than non-formalized approaches, but can also come with a learning curve. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 66 The following examples are generally oriented around a New York City taxi Geospatial operations using geospatial",
        "pickup/drop-off data set found here. NYC Taxi Zone data with geometries will libraries for Apache Spark also be used as the set of polygons. This data contains polygons for the five boroughs of NYC as well the neighborhoods. This notebook will walk you through",
        "Over the last few years, several libraries have been developed to extend the preparations and cleanings done to convert the initial CSV files into Delta Lake capabilities of Apache Spark for geospatial analysis. These frameworks bear the tables as a reliable and performant data source.",
        "brunt of registering commonly applied user-defined types (UDT) and functions Our base DataFrame is the taxi pickup/drop-off data read from a Delta Lake Table (UDF) in a consistent manner, lifting the burden otherwise placed on users and using Databricks.",
        "teams to write ad hoc spatial logic. Please note that in this blog post, we use several different spatial frameworks chosen to highlight various capabilities. We éscala understand - that other frameworks exist those which val dfRaw spark.read.format: (\"delta\").load\"/ml/blogs/geospatial/ beyond highlighted, you might delta/nyc-green\"?",
        "also want to use with Databricks to process your spatial workloads. display (dfRaw) // showing first 10 columns Earlier, we loaded our base data into a DataFrame. Now we need to turn the latitude/longitude attributes into point geometries. To accomplish this, we will pickup datetim dropolf latetime mes 2017-09-30235743",
        "use UDFS to perform operations on DataFrames in a distributed fashion. Please mmas 2017-09-3023.5530 N refer to the provided notebooks at the end of the blog for details on adding these mes 2017-09-30233729 N frameworks to a cluster and the initialization calls to UDFS and N register UDTS. For",
        "mes 2017-09-3023.54.59 2017-09-30 2017-09-302331.49 N 89 2.35 starters, we have added GeoMesa to our cluster, a framework especially adept thef first 1000rows. at handling vector data. For ingestion, we are mainly leveraging its integration of JTS with Spark SQL, which allows us to easily convert to and use registered JTS",
        "Figure 1: Geospatial data read from al Delta Lake table using Databricks geometry classes. We will be using the function st_makePoint that, given a latitude and longitude, create a Point geometry object. Since the function is a UDF, we can apply it to columns directly. Sscala val df = dfRaw",
        ".withColumn Cpickup.point\", st_makePoint (col Cpichup.longitude?, col (\"pickup latitude\"))) withColumn (\"dropoff_point\" st makePoint (col (\"dropoff longitude\"),col (\"dropof: atitude\"))) display (df.select (\"dropoff_point\" dropotf.datetime?) databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 67 (2) SparkJobs Wrapping single-node libraries in UDFS Figure 2: Using UDFS to perform dropoff.point dropoft. datetime",
        "operations on DataFrames in POINT 73.98411560058594 40.695980072021484) 2016-04-010 00:05:53 a distributed fashion to turn In addition to purpose-built distributed POINT (73.8504409790039. 40.724143981933594 2016-04-01 00:05:55 using spatial frameworks, existing single- geospatial data latitudellongitude Showingi thefi first 10001 rows. attributes into point geometries.",
        "node libraries can also be wrapped in ad hoc UDFS for performing geospatial operations on DataFrames in a distributed fashion. This pattern is available to all Spark language bindings - Scala, Java, Python, R and SQL = and is a can also simple We",
        "perform distributed spatial joins, in this case using GeoMesa's approach for leveraging existing workloads with minimal code changes. To provided st_contains UDF to produce the resulting join of all polygons against demonstrate a single-node example, let's load NYC borough data and define pickup points.",
        "UDF find_boroughC-) for point-in-polygon operation to assign each GPS location éscala to a borough using geopandas. This could also have been accomplished with a val joinedDF = wktDF.join (df, st_contains (S\"the_geom\", \"pickup.point\") display GoinedDF.select: (\"zone\" \"borough\" Pichp.poinePichp vectorized UDF for even better performance datetime\")) Spython",
        "# read the boroughs polygons with geopandas (2) Spark. Jobs gdf = gdp.read_file C/dbfs/ml/blogs/geospatial/nyc_boroughs. geojson\") zone borough pickup.point pickup.datetime Fort Greene Brooklyn POINT 73.98098486084453- 40.689029693603516 2016-06-09 10:35:08 b_gdf = sc.broadcast (gdf) # broadcast the geopandas dataframe to all Crown Heights North Brooklyn POINT 73.95674896240234 40.67413330078125) 2016-06-09 10:42:15",
        "nodes of the cluster Brooklyn Heights Brooklyn POINT (73.9929428100586 40.69749069213867 2016-06-09 10:47:38 def find_borough (latitude, longitude): Brooklyn Heights Brooklyn POINT 73.991172790527344 40.6959114074707 2016-06-09 10:46:09 mgdf b.gdf.value-apply (lambda x: x[\"boro_name\") if xI'geometry\"l. Williamsburg (South Side) Brooklyn POINT (73.98204376220703- 40.70991516113281), 2016-06-09 10:06:12 intersects (Point (longitude, latitude)) East HarlemNorth Manhattan POINT (73.93933868408203- 40.80525207519531)",
        "2016-06-09 10:58:19 idx = mgdf .first_valid index () Steinway Queens POINT (73.9175796508789 40.7569954681398484) 2016-06-091 10:45:41 return mgdf.loclidx) if idx is not None else None Morningsidel Heights Manhattan POINT 73.96385192871094 40,80808639526367 2016-06-09 10:36:34 Showing the first 1000r rows. Doi n01e findborough_udf = udf (find_borough, StringType ())",
        "Figure 3: Using GeoMesa's provided st_contains UDF, for example, to produce the resulting join of all polygons against pickup points databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 68",
        "Now we can apply the UDF to add a column to our Spark DataFrame, which assigns Grid systems for spatial indexing a borough name to each pickup point. Geospatial operations are inherently computationally expensive. Point-in-polygon, Spython # read the coordinates from delta",
        "spatial joins, nearest neighbor or snapping to routes all involve complex operations. df = spark.read.format: rdelta.loadcmi/Bloga/geospatial/delta/nyc- By indexing with grid systems, the aim is to avoid geospatial operations altogether. green\") df with_boroughs = df.withColumn Cpickup.borough\", find_borough This approach leads to the most scalable implementations with the caveat of",
        "udf (col CPichp.atita-7eat (pickup.longitude)) approximate operations. Here is a brief example with H3. display (df with boroughs.select: \"pickup_ datetime \"pickup_ latitude\" pickup.longitude\" \"pickup_ borough\")) Scaling spatial operations with H3 is essentially a two-step process. The first step",
        "is to compute an H3 index for each feature (points, polygons, ) defined as UDF geoTOH3(.). The second step is to use these indices for spatial operations such (2)Spark.Jobs as spatial join (point-in-polygon, k-nearest neighbors, etc.), in this case defined as pickup. datetime pickup.Jatitude pickup.Jongitude pickup. borough UDF mutiPolygonfoH34..",
        "2016-04-01 00:06:39 40.718135833740234 -73.9595108032266 Manhattan 2016-04-01 00:06:28 40.86066818237305 -73.889864080810547 Manhattan 2016-04-01 00:07:25 40.73863983154297 -73.88591766357422 Manhattan 2016-04-01 00:09:44 40.69947814941406 -73.92366790771484 Manhattan 2016-04-01 00:16:02 40.691192626953125 -73.9872055053711 Manhattan 2016-04-01 00:14:52 40.761085510253906 -73.92341613769531 Manhattan 2016-04-01 00:11:00 40.686092376708984 -73.97399139404297 Manhattan 2016-04-01 00:17:17 40.79181671142578 -73.944580078125 Manhattan Emgnam rows.",
        "Figure 4: The result of a single-node example, where GeoPandas is used to assign each GPS location to an NYC borough databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 69 éscala val multiPolygonToH3 = udf! (geometry: Geometry, resolution: Int) => import com.uber.h3core.H3Core",
        "var points: List[GeoCoord) = List () import com.uber.h3core.util.GeoCoord var holes: List [java.util.List [GeoCoord]] = List () import scala.collection..Javaconversions. if (geometry-getceometryType == \"MultiPolygon\") ( import scala. collection.JavaConverters. val numGeometries = ESADNCtIN () if (numGeometries > 0) ( object H3 extends Serializable ( points = List ( val instance = H3Core.newInstance ()",
        "geometry getGeometryN (0) getCoordinates () val geoTOH3 = udf! (latitude: Double, longitude: Double, resolution: toList Int) => .map (coord => new GeoCoordicoord.y, coord.x)): H3.instance.georon3 (latitude, longitude, resolution) ) if (numGeometries > 1) ( holes = (1 to (numGeometries - 1)).toList.map (n => (",
        "val polygonTOH3 = udf! (geometry: Geometry, resolution: Int) => List( var points: List[GeoCoord) = List() geometry var holes: Listljava.util.List/Geocoordl) = List() -getGeometryN (n) if (geometry-getceometryType == \"Polygon\") ( getCoordinates () points = List ( .toList geometry .map (coord => new GeoCoord (coord.y, coord.x)): *).asJava getCoordinates () 1) toList )",
        ".map (coord => new GeoCoord (coord.y, coord.x)): *) ) H3.instance.polylll (points, holes.asJava, resolution) .toList H3.instance.polynll (points, holes.asJava, resolution).tolist databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 70 We can now apply these two UDFS to the NYC taxi data as well as the set of éscala",
        "val dfwithBorougha3 = dfH3.join WAtDFB3,\"A3index? borough polygons to generate the H3 index. display (df_with_borough._h3.select (\"zone\", \"borough' \", \"pickup_ éscala point\", \"pickup.datetime\", \"h3index\")) val res = 7 //the resolution of the H3 index, 1.2km val dfH3 = df.withColumn ( \"h3index\", geoTOH3 (col (\"pickup latitude\"), col (\"pickup_ longitude\"), lit (res)) )",
        "(1) SparkJobs val wktDFH3 wktDF zone borough pickup point pickup. datetime h3index withColumn (\"h3index\", multiPolygonToH3 (col (\"the_geom\"), lit (res))) Morningsidel Heights Manhattan POINT 73.95296478271484 40.80758285522461) 2016-06-09 10:14:34 61322962300085247 withColumn (\"h3index\", explode (S\"h3index\")) Central Harlem Manhattan POINT (73.94908905029297 40.80293655395508) 2016-06-09 10:04:08 613229523028148223 Brooklyn Heights Brooklyn POINT (73.99422454833984 40.69488525390625) 2016-06-09 10:52:24 61322955141100391",
        "Van Nest/Moris Park Bronx POINT 73.84475708007812 40.847774505615234) 2016-06-091 10:23:52 61329520937287679 Astoria Queens POINT (73.9139633178711 40.76524353027344) 2016-06-09 10:25:38 613229624726841343 Morningsidel Heights Manhattan POINT 673.95944213867188 40.8091239291992) 2016-06-09 10:42:56 61322962300085247 Park Slope Brooklyn POINT 73.98164367675781 40.6669464113281) 2016-06-09 10:29:28 613229852660905983 Park Slope Brooklyn POINT (7397588348388672 40.67397689819336) 2016-06-09 10:53:01 61329562669294591",
        "Given a set of lat/lon points and a set of polygon geometries, it is now possible Ln Mlanl n. Showingt thet first 1000rows. to perform the spatial join using h3index field as the join condition. These assignments can be used to aggregate the number of points that fall within each",
        "Figure 5: DataFrame table representing the spatialj join of a set of lat/lon points and polygon, for instance. There are usually millions or billions of points that have to polygon geometries, using a specific field as the join condition",
        "be matched to thousands or millions of polygons, which necessitates a scalable approach. There are other techniques not covered in this blog that can be used for indexing in support of spatial operations when an approximation is insufficient. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 71",
        "Here is a visualization of taxi drop-off locations, with latitude and longitude Handling spatial formats with Databricks binned at a resolution of 7 (1.22km edge length) and colored by aggregated counts within each bin. Geospatial data involves reference points, such as latitude and longitude, to physical",
        "locations or extents on the Earth along with features described by attributes. Nev While there are many file formats to choose from, we have picked out a handful of Canaan uffern Spring Valley Norwalk representative vector and raster formats to demonstrate reading with Databricks. Darien STAMFORD Vector data Oakland Wyckoff",
        "Vector data is a representation of the world stored in X (longitude), y (latitude) coordinates in degrees, and also Z (altitude in meters) if elevation is considered. Wayne PATERSON The three basic symbol types for vector data are points, lines and polygons.",
        "Well-known-text (WKT), GeoJSON and shapefile are some popular formats for Clifton en Cove Oyster Bay HUNTINGTON storing vector data we highlight below. Let's read NYC Taxi Zone data with geometries stored as WKT. The data structure",
        "we want to get back is a DataFrame that will allow us to standardize with other APIs Hempstead es and available data sources, such as those used elsewhere in the blog. We are able Union Lin to easily convert the WKT content found in field the_geom into its ELIZAE corresponding",
        "Freeport JTS Geometry class through the st_geomFromXT) UDF call. ea éscala val wktDFText = gContest.read.forms t (\"csv\") .option (\"header\", \"true\") option (\"inferSchema \", \"true\") loadc/ai/Bloga/goepatial/nyesax, zones.wkt.csv\") Figure 6: Geospatial visualization of taxi drop-off locations, with latitude and longitude binned",
        "at a resolution of 7( (1.22km edgel length) and colored by aggregated counts within each bin val wktDF = WAEDFTeXtE.MithCOlumA (\"the_geom\", st_geomFi romWKT (col (\"the_ geom\")).cache databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 72",
        "GeoJSON is used by many open source GIS packages for encoding a variety of From there, we could choose to hoist any of the fields up to top level columns using",
        "geographic data structures, including their features, properties and spatial extents. Spark's built-in explode function. For example, we might want to bring up geometry, For this example, we will read NYC Borough Boundaries with the approach taken properties and type and then convert geometry to its corresponding JTS class, as",
        "depending on the workflow. Since the data is conforming to JSON, we could use was shown with the WKT example. the Databricks built-in JSON reader with optioncmutlineuue) to load the data Spython with the nested schema. from pyspark.sql import functions as F json_explode_df = ( json_df.select Spython \"features\",",
        "json_df = spark.read.option (\"multiline\", \"true\")-json (\"nyc_boroughs. \"type\", geojson\") F.explode (F.col CFeatures.properties?).alias (\"properties\") ). select ( F.explode (F. col : features.geometry\"7).alias (\"geometry\")). drop (\"features\")) D json_df: PpdtmDaNfens asplydan.-ploa.an features: array V element: struct geometry: struct coordinates: array type properties geometry element: array FeatureCollection wobject wobject element: array boro_code:2 coordinates I-73.89680885 357),",
        "element: array boro name: Bronx [-73.8972 038 843939 994), element: double shape_area: 1186612476.97 173.89857332865558, 73.8919434249981, ,40 960691 40 79634). type: string shape_Jeng: 462958.186921 73.897882532 properties: struct -73.896 boro_code: string boro_name: string 3.8 shape_area: string -73.8883725137 shape_leng: string type: string type: string",
        "Figure 8: Using the Spark's built-in explode function to raise a field to the top level, displayed within al DataFrame table Figure 7: Using the Databricks built-in JSON reader opioncmutlinetue), to load the data with the nested schema databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 73",
        "We can also visualize the NYC Taxi Zone data within a notebook using an existing DataFrame or directly rendering the data with a library such as Folium, a Python library for rendering spatial data. Databricks File System (DBFS) runs over a",
        "distributed storage layer, which allows code to work with data formats using familiar file system standards. DBFS has a FUSE Mount to allow local API calls that perform file read and write operations, which makes it very easy to load data with",
        "non-distributed APIs for interactive rendering. In the Python open(..) command Figure 9: We can also visualize the NYC Taxi Zone data, for example, within a below, the \"/dbfs/.\" prefix enables the use of FUSE Mount. notebook using an existing DataFrame or directly rendering the data with al library",
        "such as Folium, al Python library for rendering geospatial data Spython import folium import json Shapefile is a popular vector format developed by ESRI that stores the geometric with open /dbfs/ml/blogs/geospatial/nyc_boroughs. .geojson\", \"r\") as location and attribute information of geographic features. The format consists myfile:",
        "of a collection of files with a common filename prefix (*.shp, *shx and *.dbf are bore.data-ylle.reado # read GeoJSON from DBFS using FuseMount mandatory) stored in the same directory. An alternative to shapefile is KML, also m = folium.Map (",
        "used by our customers but not shown for brevity. For this example, let's use NYC location-[40.7128, -74.00601, tiles-'Stamen Terrain', Building shapefiles. While there are many ways to demonstrate reading shapefiles, zoom_start-12 we will give an example using GeoSpark. The built-in ShapefileReader is used to ) folium.GeoJson (json.loads (boro_data)) .add_to (m)",
        "generate the rawSpatialDf DataFrame. m # to display, also could use displayHTML (.. .) variants éscala var spatialRDD = new SpatialRDD [Geometryl spatialRDD = ShapenleReader.readregeometryRDD (sc, \"/ml/blogs/ PAEAANPeeT var rawSpatialDf = Adapter.toDf (spatialRDD, spark) rawdpatialDf.createoneplacerempviex C\"rawSpatialDE\"? //DataFrame now available to SQL, Python, and R databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 74 By registering rawSpatialDf as a temp view, we can easily drop into pure Spark Raster data SQL syntax to work with the DataFrame, to include applying a UDF to convert the",
        "Raster data stores information of features in a matrix of cells (or pixels) organized into shapefile WKT into Geometry. rows and columns (either discrete or continuous). Satellite images, photogrammetry and scanned maps are all types of raster-based Earth Observation (EO) data. 8sql SELECT *,",
        "ST_ GeomFromwKT (geometry) AS geometry GeoSpark UDF to convert WKT to The following Python example uses RasterFrames, a Dataframe-centric spatial Geometry FROM rawspatialdf analytics framework, to read two bands of GeoTIFF Landsat-8 imagery (red and near-infrared) and combine them into Normalized Difference Vegetation Index.",
        "We can use this data to assess plant health around NYC. The rf_ipython module Additionally, we can use Databricks' built-in visualization for in-line analytics, such is used to manipulate RasterFrame contents into a variety of visually useful forms, as charting the tallest buildings in NYC.",
        "such as below where the red, NIR and NDVI tile columns are rendered with color ramps, using the Databricks built-in displayHTMLC) command to show the results 8sql within the notebook. SELECT name, round (Cast (num floors AS DOUBLE), 0) AS num_floors --String to Number FROM rawspatialdf Spython WHERE name",
        "# construct a CSV \"catalog\" for RasterFrames 'raster' reader ORDER BY num_floors DESC LIMIT 5 # catalogs can also be Spark or orid TradeCtr WTC Figure 10: AI Databricks built-in visualization for in-line analytics charting, for example, the tallest buildings in NYC databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 75 Through its custom Spark DataSource, RasterFrames can read various raster * LE results: PpMagatmaDifiem: [ongitude.Jatitude: udt, rf.tile(red): udt. -2moret fields] Showing only top 51 rows formats, including GeOTIFF, JP2000, MRF and HDF, from an array of services. longitude_Jatitude",
        "rf_tile(red) rf_tile(NIR) rf_tile(NDVI) It also supports reading the vector formats GeoJSON and WKT/WKB. RasterFrame contents can be filtered, transformed, summarized, resampled and rasterized POINT (75.6431054921628. 41.35507991091221) through over 200 raster and vector functions, such as st_reproject() and st_centroid.) used in the example above. It provides APIs for Python, SQL and",
        "Scala as well as interoperability with Spark ML. POINT (75.55129747458508 41.3555632722104) POINT (75.64242580157753 41.285904858936576) POINT (75.55071479581207 41.28638012434911) POINT (7545900176161878: 41.286782331725604) Screenshot Figure 11: RasterFrame contents can bei filtered, transformed, summarized, resampled and rasterized through over 200 raster and vector functions databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 76 Geodatabases Getting started with geospatial analysis on Databricks Geodatabases can be file based for smaller scale data or accessible via JDBC/ODBC connections for medium scale data. You can use Databricks to",
        "Businesses and government agencies seek to use spatially referenced data in query many SQL databases with the built-in JDBC/ODBC Data Source. conjunction with enterprise data sources to draw actionable insights and deliver Connecting to PostgreSQL is shown below and is commonly used for smaller",
        "on a broad range of innovative use cases. In this blog we demonstrated how the scale workloads by applying PostGIS extensions. This pattern of connectivity Databricks Unified Data Analytics Platform can easily scale geospatial workloads, allows customers to maintain as-is access to existing databases.",
        "enabling our customers to harness the power of the cloud to capture, store and analyze data of massive size. éscala display ( Context.read.fomat (\"jdbc\") In an upcoming blog, we will take a deep dive into more advanced topics for .option (\"url\", jdbcUrl) geospatial",
        "at-scale with Databricks. You will find additional details option (\"driver\", or.poatyreglpriver? processing .option (\"dbtable\", about the spatial formats and highlighted frameworks reviewing Data MIn * by (SELECT FROM ellocsripata.taging OFFSET 5 LIMIT 10) AS t\"\") //predicate pushdown Prep Notebook, GeoMesa + H3 Notebook, GeoSpark Notebook, GeoPandas option (\"user\", jdbcUsername)",
        "Notebook, and RasterFrames Notebook. Also, stay tuned for a new section in our option 'jdbcPassword\", jdbcPassword) .load) documentation specifically for geospatial topics of interest. pep. pickup. 2019-01-0 2019-0 2019 1:27 2019 2019 2019 2019 2019-0 019-01 57:03 databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 77",
        "CHAPTER 9: Exploring Twitter Introduction Sentiment and Crypto The market capitalization of cryptocurrencies increased from $17 billion in 2017 to $2.25 trillion in 2021. Price Correlation Using That's over a 13,000% ROI in a short span of 5 years! Even with this growth, cryptocurrencies remain Databricks",
        "incredibly volatile, with their value being impacted by a multitude of factors: market trends, politics, technology : and Twitter. Yes, that's right. There have been instances where their prices were impacted on account of tweets by famous personalities.",
        "As part of a data engineering and analytics course at the Harvard Extension School, our group worked on a project to create a cryptocurrency data lake for different data personas - including data engineers, ML",
        "practitioners and BI analysts = to analyze trends over time, particularly the impact of social media on the price volatility of a crypto asset, such as Bitcoin (BTC). We leveraged the Databricks Lakehouse Platform to ingest unstructured data from Twitter using the Tweepy library and traditional structured pricing data",
        "from Yahoo Finance to create a machine learning prediction model that analyzes the impact of investor sentiment on crypto asset valuation. The aggregated trends and actionable insights are presented on a Databricks SQL dashboard, allowing for easy consumption to relevant stakeholders. By Monica Lin, Christoph Meier,",
        "Matthew Parker and Kiran Ravella This blog walks through how we built this ML model in just a few weeks by leveraging Databricks and its collaborative notebooks. We would like to thank the Databricks University Alliance program and the extended team for all the support. databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 78 Overview One advantage of cryptocurrency for investors is that it is traded 24/7 and The full orchestration workflow runs a sequence of Databricks notebooks that",
        "the market data is available round the clock. This makes it easier to analyze the perform the following tasks: correlation between the Tweets and crypto prices. A high-level architecture 1. Data ingestion pipeline of the data and ML pipeline is presented in figure 11 below.",
        "Imports the raw data into the Cryptocurrency Delta Lake Bronze tables 2. Data science Cleans data and applies the Twitter sentiment machine learning Data Sources Chryptocurrency Delta Lake model into Silver tables Cryp Aggregates the refined Twitter and Yahoo Finance data into an aggregated Gold table Rawtweets (Bronze) Refinedrweets (Silver)",
        "Computes the correlation ML model between price and sentiment A 3. Data Yfina analysis Rawticker data Refinedticker data Runs updated SQL BI queries on the Gold table (Bronze) (Silver) DataQualiy AWSS3 Cloud Native Storage The lakehouse paradigm combines key capabilities of data lakes and data",
        "warehouses to enable all kinds of BI and Al use cases. The use of the lakehouse architecture enabled rapid acceleration of the pipeline creation to one 1: week. Figure Crypto Lake using Delta just As a team, we played specific roles to mimic different data personas, and this",
        "paradigm facilitated the seamless handoffs between data engineering, machine learning and business intelligence roles without requiring data to be moved across systems. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 79 Data/ML pipeline Data science Ingestion using a medallion architecture",
        "The data science portion of our project consists of three major parts: exploratory The two primary data sources were Twitter and Yahoo Finance. A lookup table data analysis, sentiment model and correlation model. The objective is to build",
        "was used to hold the crypto tickers and their Twitter hashtags to facilitate the a sentiment model and use the output of the model to evaluate the correlation subsequent search for associated tweets. between sentiment and the prices of different cryptocurrencies, such as Bitcoin,",
        "Ethereum, Coinbase and Binance. In our case, the sentiment model follows a We used yfinance python library to download historical crypto exchange market supervised, multi-class classification approach, while the correlation model",
        "data from Yahoo Finance's API in 15-minute intervals. The raw data was stored in a uses a linear regression model. Lastly, we used MLflow for both models' lifecycle Bronze table containing information such as ticker symbol, datetime, open, close, management, including experimentation, reproducibility, deployment and a central",
        "high, low and volume. We then created a Delta Lake Silver table with additional model registry. MLflow Model Registry collaboratively manages the full lifecycle of data, such as the relative change in price of the ticker in that interval. Using Delta",
        "an MLflow model by offering a centralized model store, set of APIs and UI. Some of Lake made it easy to reprocess the data, as it guarantees atomicity with every its most useful features include model lineage (which MLflow experiment and run",
        "operation. It also ensures that schema is enforced and prevents bad data from produced the model), model versioning, stage transitions (such as from staging to creeping into the lake. production or archiving), and annotations. We used Tweepy Python library to download Twitter data. We stored the raw",
        "tweets in a Delta Lake Bronze table. We removed unnecessary data from the Bronze table and also filtered out non-ASCII characters like emojis. This refined data was stored in a Delta Lake Silver table. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 80 Exploratory data analysis",
        "amp Anonymous fransactions Private 4s 30 ttT :0 using Smashcash The EDA section provides insightful visualizations on the data set. For example, we 25 ETH Blockchain looked at the distribution of tweet lengths for each sentiment category using violin 20 SmashcashPlatforn SHIBAINU want",
        "plots from Seaborn. Word clouds (using Matplotlib and wordcloud libraries) for 6 15 INU Transaction: make SHIBA 5 positive and negative tweets were also used to show the most common words for 10 EICMI-XRP Anywhere Close the two sentiment types. Lastly, an interactive topic modeling dashboard was built, 5",
        "yer su E E CO using Gensim, to provide insights on the top most common topics in the data set positive neutral negative BTCGasparino and the most frequently used words in each topic, as well as how similar the topics sentiment XRP BTX ETH are to each other.",
        "Figure 2: Violin plots for text length Figure 3: Word cloud for positive and negative tweets Topic Previous Topi Nerl Topi Cles Topi Slide tric2) A-1 Intertopic Distance Map (van mutidimensional: scaling) Top-30 Most Salient Terms\" Figure 4: Interactive topic modeling dashboard databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 81 Sentiment analysis model Developing a proper sentiment analysis model has been one of the core tasks In this project, we focused on the latter two approaches since they are supposed",
        "within the project. In our case, the goal of this model was to classify the polarities to be the most promising. Thereby, we used SparkNLP as the NLP library of choice that are expressed in raw tweets as input using a mere polar view of sentiment",
        "due to its extensive functionality, its scalability (fully supported by Apache (i.e., tweets were categorized as \"positive,\" \"neutral\" or \"negative\"). Since sentiment SparkM) and accuracy (e.g. it contains multiple state-of-the-art embeddings and",
        "analysis is a problem of great practical relevance, it is no surprise that multiple ML allows users to make use of transfer learning). First, we built a sentiment analysis strategies related to it can be found in literature: pipeline using the aforementioned classical ML algorithms. The following figure",
        "shows its high-level architecture consisting of three parts: preprocessing, feature Sentiment lexicons algorithms Off-the-shelf sentiment analysis systems vectorization and finally training including hyperparameter tuning. Compare each word in a tweet to a database Exemplary systems: Amazon Comprehend,",
        "of words that arel labeled as having positive or Google Cloud Services, Stanford Core NLP negative sentiment Pros: do not require great preprocessing Preprocessing stages At tweet with more positive words than negative of the data and allow the user to directly",
        "would be scored as a positive and vice versa start a prediction \"out of the box\" Pros: straightforward approach Cons: limited fine-tuning for the underlying Document Tokenizer Normalizer Stopwords Lemmatizer Stemmer Finisher use-case (retraining might be needed to Assembler Cleaner",
        "Cons: performs poorly in general and greatly adjust the model performance) depends ont the quality of the database of words Classical ML algorithms Deep Learning (DL) algorithms Feature Vectorization Training & Tuning Application of traditional supervised classifiers Application of NLP related neural network",
        "like logistic regression, random forest, support architectures like BERT, GPT-2/ GPT-3 mainly vector machine or Naive Bayes via transfer learning HashingTF IDF Classifier (LogReg, NB, RF, SVC) Pros: well known, often financially and Pros: many pretrained neural networks for computationally cheap, easy toi interpret word embeddings and sentiment prediction",
        "already exist (particularly helpful for transfer Figure 5: Machine learning model pipeline Cons: in general, performance on unstructured learning), DL models scale effectively with data data like text is expected to be worse compared to structured data and necessary Cons: difficult and computationally expensive preprocessing can be extensive",
        "to tune architecture and hyperparameters databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 82 We run this pipeline for every classifier and compare their corresponding Correlation model accuracies on the test set. As a result, the Support Vector Classifier achieved the",
        "highest accuracy with 75.7%, closely followed by Logistic Regression (75.6%), Naïve The project requirement included a correlation model on sentiment and price; Bayes (74%) and finally Random Forest (71.9%). To improve the performance, other therefore, we built a linear regression model using scikit-learn and mlflow.sklearn",
        "supervised classifiers like XGBoost or Gradientoostedtres could be tested. for this task. Besides, the individual algorithms could be combined to an ensemble, which is We quantified the sentiment by assigning negative tweets a score of -1, neutral then used for prediction (e.g., majority voting, stacking).",
        "tweets a score of O, and positive tweets a score of 1. The total sentiment score In addition to this first pipeline, we developed a second Spark pipeline with a for each cryptocurrency is then calculated by adding up the scores for each",
        "similar architecture making use of the rich Spark NLP functionalities regarding cryptocurrency in 15-minute intervals. The linear regression model is built using pretrained word embeddings and DL models. Starting with the standard Document the total sentiment score in each window for all companies to predict the",
        "Assembler annotator, we only used a Normalizer annotator to remove Twitter percentage change in cryptocurrency prices. However, the model shows no clear handles, alphanumeric characters, hyperlinks, html tags and timestamps but no linear relationship between sentiment and change in price. A possible future",
        "further preprocessing related annotators. In terms of the training stage, we used improvement for the correlation model is using sentiment polarity to predict the a pretrained (on the well-known IMDb data set) sentiment DL model provided change in price instead.",
        "by Spark NLP. Using the default hyperparameter settings, we already achieved a test set accuracy of 83%, which could potentially be even enhanced using other pretrained word embeddings or sentiment DL models. Thus, the DL strategy Pre-processing stages I Training MAE: 1.6",
        "clearly outperformed the pipeline in figure 5 with the Support Vector Classifier by MSE: 29.4 around 7.4 percent points. RMSE: 5.42 R2: 0.01 Finance & Transformation I Linear Regression Sentiment Figure 6: Correlation model pipeline databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 83 Business intelligence",
        "Understanding stock correlation models was a key component of generating buy/ The use of the SQL Editor in Databricks was key to making the process fast and sell predictions, but communicating results and interacting with the information is",
        "simple. For each query, the editor GUI enables the selection of different views of equally critical to make well-informed decisions. The market is sO dynamic, SO a the data including tables, charts and summary statistics to immediately see the real-time visualization is required to aggregate and organize trending information.",
        "output. From there, views could be imported directly into the dashboards. This Databricks Lakehouse enabled all of the BI analyst tasks to be coordinated in one eliminated redundancy by utilizing the same query for different visualizations. place with streamlined access to the lakehouse data tables. First, a set of SQL",
        "queries were generated to extract and aggregate information from the lakehouse. Visualization Then the data tables were easily imported with a GUI tool to rapidly create For the topic of Twitter sentiment analysis, there are three key views to help users",
        "dashboard views. In addition to the dashboards, alert triggers were created to interact with the data on a deeper level. notify users of critical activities like stock movement up/down by > X%, increases",
        "in Twitter activity about a particular crypto hashtag or changes in overall positivel View 1: Overview Page, taking a high-level view of Twitter influencers, stock negative sentiment about each cryptocurrency. movement and frequency of tweets related to particular cryptos. 12,658 @ethereum Tweet Count Dashboard generation binance TheWizardo:Doge",
        "The business intelligence dashboards were created using Databricks SQL. This Top Influencer system provides a full ecosystem to generate SQL queries, create data views XRP-USD DOTI-USD and charts, and ultimately organizes all of the information using Databricks Dashboards. eerd @dogecoin @ethereum Cripple",
        "Figure 7: Overview Dashboard\" View with top level statistics databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 84 View 2: Sentiment Analysis, to understand whether each tweet is positive, negative Summary or neutral. Here you can easily visualize which cryptocurrencies are receiving the",
        "Our team of data engineers, data scientists and BI analysts was able to most attention in a given time window. leverage the Databricks tools to investigate the complex issue of Twitter usage Analysis and cryptocurrency stock movement. The lakehouse design created a robust BarChart rypto",
        "data environment with smooth ingestion, processing and retrieval by the whole team. The data collection and cleaning pipelines deployed using Delta tables were easily managed even at high update frequencies. The data was analyzed by a natural language sentiment model and a stock correlation model using MLflow,",
        "which made the organization of various model versions simple. Powerful analytics dashboards were created to view and interpret the results using built-in SQL and Dashboard features. The functionality of Databricks' end-to-end product tools Figure 8: Sentiment Analysis Dashboard removed significant technical barriers, which enabled the entire project to be",
        "View 3: Stock Volatility to provide the user with more specific information about completed in less than 4 weeks with minimal challenges. This approach could the price for each cryptocurrency with trends over time. easily be applied to other technologies where streamlined data pipelines,",
        "machine learning and BI analytics can be the catalyst for a deeper understanding of your data. I - Figure 9: Stock Ticker Dashboard databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 85 Our findings",
        "These are additional conclusions from the data analysis to highlight the extent of Overall, the use of Databricks to coordinate the pipeline from data ingestions, the Twitter users' influence on the price of cryptocurrencies. lakehouse data structure and the BI reporting dashboards was hugely beneficial",
        "to completing this project efficiently. In a short period of time, the team was able Volume of tweets correlated with volatility in cryptocurrency price to build the data pipeline, complete machine learning models and produce high-",
        "There is a clear correlation in periods of high tweet frequency to the movement of quality visualizations to communicate results. The infrastructure provided by the a cryptocurrency. Note that this happens before and after a stock price change, Databricks platform removed many of the technical challenges and enabled the",
        "indicating some tweet frenzies precede price change and are likely influencing project to be successful. value, and others are in response to big shifts in price. While this tool will not enable you to outwit the cryptocurrency markets, we Twitter users with more followers don't actually have more influence",
        "strongly believe it will predict periods of increased volatility, which can be on crypto stock price advantageous for specific investing conditions. This is often discussed in media events, particularly with lesser-known currencies. Disclaimer: This article takes no responsibility for financial investment decisions.",
        "Some extreme influencers like Elon Musk gained a reputation for being able to drive Nothing contained in this website should be construed as investment advice. enormous market swings with a small number of targeted tweets. While it is true",
        "that a single tweet can impact cryptocurrency price, there is not an underlying correlation between number of followers to movement of the currency price. There Try notebooks is also a slightly negative correlation to number of retweets VS. price movement, Please try out the referenced Databricks notebooks",
        "indicating the Twitter activity by influencers might have broader reach as it moves into other mediums like new articles rather than reaching directly to investors. Data Science * Merge to Gold * The Databricks platform was incredibly useful for solving complex problems like Orchestrator * Inference *",
        "merging Twitter and stock data. Tweepy * Y_Finance * databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 86 CHAPTER 10: CUSTOMER CASE STUDIES Comcast delivers the As a global technology and media company that connects millions of customers to personalized experiences, future of entertainment",
        "Comcast struggled with massive data, fragile data pipelines and poor data science colaboration. By using Databricks - including Delta Lake and MLflow = they were able to build performant data pipelines for petabytes of data and easily manage the lifecycle of hundreds of models, creating a highly innovative, unique",
        "and award-winning viewer experience that leverages voice recognition and machine learning. Use case: In the intensely competitive entertainment industry, there's no time to press the Pause button. Comcast realized they needed to modernize their entire approach to analytics, from data ingest to the",
        "deployment of machine learning models that deliver new features to delight their customers. Solution and benefits: Armed with a unified approach to analytics, Comcast can now fast-forward into the future of Al-powered entertainment = keeping viewers engaged and delighted with competition-beating customer experiences.",
        "Emmy-winning viewer experience: Databricks helps Comcast to create a highly innovative and award- COMCAST winning viewer experience with intelligent voice commands that boost engagement Reduced compute costs by 10x: Delta Lake has enabled Comcast to optimize data ingestion, replacing",
        "640 machines with 64 = while improving performance. Teams can spend more time on analytics and \"With Databricks, we can now be more informed about the decisions we make, less time on infrastructure management. and we can make them faster.\"",
        "Higher data science productivity: The upgrades and use of Delta Lake fostered global collaboration among data scientists by enabling different programming languages through a single interactive Jim Forsythe Senior Director, Product Analytics and Behavioral Sciences",
        "workspace. Delta Lake also enabled the data team to use data at any point within the data pipeline, Comcast allowing them to act much quicker in building and training new models. Faster model deployment: By modernizing, Comcast reduced deployment times from weeks to",
        "minutes as operations teams deployed models on disparate platforms Learn more databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 87 CHAPTER 10: CUSTOMER CASE STUDIES Regeneron accelerates Regeneron's mission is to tap into the power of genomic data to bring new medicines to patients in need. with",
        "Yet, transforming this data into life-changing discovery and targeted treatments has never been more drug discovery challenging. With poor processing performance and scalability limitations, their data teams lacked what genomic sequencing they needed to analyze petabytes of genomic and clinical data. Databricks now empowers them to quickly",
        "analyze entire genomic data sets quickly to accelerate the discovery of new therapeutics. Use case: More than 95% of all experimental medicines that are currently in the drug development pipeline are expected to fail. To improve these efforts, the Regeneron Genetics Center built one of the most",
        "comprehensive genetics databases by pairing the sequenced exomes and electronic health records of more than 400,000 people. However, they faced numerous challenges analyzing this massive set of data: Genomic and clinical data is highly decentralized, making it very difficult to analyze and train models against their entire 10TB data set",
        "Difficult and costly to scale their legacy architecture to support analytics on over 80 billion data points Data teams were spending days just trying to ETL the data SO that it could be used for analytics REGENERON",
        "Solution and benefits: Databricks provides Regeneron with a Unified Data Analytics Platform running on Amazon Web Services that simplifies operations and accelerates drug discovery through improved data science productivity. This is empowering them to analyze the data in new ways that were previously impossible. \"The Databricks Unified Data Analytics Platform",
        "is enabling everyone in our integrated drug Accelerated drug target identification: Reduced the time it takes data scientists and development process = from physician-scientists to computational biologists to run queries on their entire data set from 30 minutes down to 3 seconds - computational biologists = to easily access, analyze",
        "a 600x improvement! and extract insights from all of our data.\" Increased productivity: Improved collaboration, automated DevOps and accelerated pipelines Jeffrey Reid, Ph.D. (ETL in 2 days VS. 3 weeks) have enabled their teams to support a broader range of studies Head of Genome Informatics Regeneron Learn more databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 88 CHAPTER 10: CUSTOMER CASE STUDIES Nationwide reinvents The explosive growth in data availability and increasing market competition are challenging insurance providers to provide better pricing to their customers. With hundreds of millions of insurance records insurance with actuarial",
        "to analyze for downstream ML, Nationwide realized their legacy batch analysis process was slow and modeling inaccurate, providing limited insight to predict the frequency and severity of claims. With Databricks, they have been able to employ deep learning models at scale to provide more accurate pricing predictions,",
        "resulting in more revenue from claims. Use case: The key to providing accurate insurance pricing lies in leveraging information from insurance claims. However, data challenges were difficult, as they had to analyze insurance records that were volatile because claims were infrequent and unpredictable - resulting in inaccurate pricing.",
        "Solution and benefits: Nationwide leverages the Databricks Unified Data Analytics Platform to manage the entire analytics process from data ingestion to the deployment of deep learning models. The fully managed platform has simplified IT operations and unlocked new data-driven opportunities for their data science teams.",
        "Data processing at scale: Improved runtime of their entire data pipeline from 34 hours to less than Nationwide 4 hours, a 9x performance gain Faster featurization: Data engineering is able to identify features 15x faster = from 5 hours to around \"With Databricks, we are able to train models",
        "20 minutes against all our data more quickly, resulting in more accurate pricing predictions that have Faster model training: Reduced training times by 50%, enabling faster time-to-market of new models had a material impact on revenue.\"",
        "Improved model scoring: Accelerated model scoring from 3 hours to less than 5 minutes, a 60x Bryn Clark improvement Data Scientist Nationwide databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 89 CHAPTER 10: CUSTOMER CASE STUDIES Condé Nast boosts",
        "Condé Nast is one of the world's leading media companies, counting some of the most iconic magazine titles in its portfolio, including The New Yorker, Wired and Vogue. The company uses data to reach over 1 billion reader engagement with people in print, online, video and social media.",
        "experiences driven by data and Al Use case: As a leading media publisher, Condé Nast manages over 20 brands in their portfolio. On a monthly basis, their web properties garner 100 million-plus visits and 800 million-plus page views, producing a",
        "tremendous amount of data. The data team is focused on improving user engagement by using machine learning to provide personalized content recommendations and targeted ads. Solution and benefits: Databricks provides Condé Nast with a fully managed cloud platform that simplifies operations, delivers superior performance and enables data science innovation.",
        "Improved customer engagement: With an improved data pipeline, Condé Nast can make better, faster and more accurate content recommendations, improving the user experience Built for scale: Data sets can no longer outgrow Condé Nast's capacity to process and glean insights",
        "More models in production: With MLflow, Condé Nast's data science teams can innovate their products CONDÉ faster. They have deployed over 1,200 models in production. NAST Learn more \"Databricks has been an incredibly powerful end-to- end solution for us. It's allowed a variety of different",
        "team members from different backgrounds to quickly get in and utilize large volumes of data to make actionable business decisions.\" - Paul Fryzel Principal Engineer of AI Infrastructure Condé Nast databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 90 CHAPTER 10: CUSTOMER CASE STUDIES Showtime leverages ML",
        "SHOWTIME is a premium television network and streaming service, featuring award-winning original series to deliver data-driven and original limited series like \"Shameless,\" \"Homeland,\" \"Billions,\" \"The Chi,\" \"Ray Donovan,\" \"SMILF,\" \"The Affair,\" \"Patrick Melrose,\" \"Our Cartoon President,\" \"Twin Peaks\" and more. content programming",
        "Use case: The Data Strategy team at Showtime is focused on democratizing data and analytics across the organization. They collect huge volumes of subscriber data (e.g., shows watched, time of day, devices used, subscription history, etc.) and use machine learning to predict subscriber behavior and improve scheduling and programming.",
        "Solution and benefits: Databricks has helped Showtime democratize data and machine learning across the organization, creating a more data-driven culture. 6x faster pipelines: Data pipelines that took over 24 hours are now run in less than 4 hours, enabling teams to make decisions faster",
        "Removing infrastructure complexity: Fully managed platform in the cloud with automated cluster management allows the data science team to focus on machine learning rather than hardware SHOWTIME configurations, provisioning clusters, debugging, etc. Innovating the subscriber experience: Improved data science collaboration and productivity has",
        "reduced time-to-market for new models and features. Teams can experiment faster, leading to a better, \"Being on the Databricks platform has allowed a team of exclusively data scientists to make huge more personalized experience for subscribers. strides in setting aside all those configuration headaches that we were faced with. It's",
        "Learn more dramatically improved our productivity.\" - Josh McNutt Senior Vice President of Data Strategy and Consumer Analytics Showtime databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 91 CHAPTER 10: CUSTOMER CASE STUDIES Shell innovates with",
        "Shell is a recognized pioneer in oil and gas exploration and production technology and is one of the world's solutions for leading oil and natural gas producers, gasoline and natural gas marketers and petrochemical manufacturers. energy a cleaner world",
        "Use case: To maintain production, Shell stocks over 3,000 different spare parts across their global facilities. It's crucial the right parts are available at the right time to avoid outages, but equally important is not overstocking, which can be cost-prohibitive.",
        "Solution and benefits: Databricks provides Shell with a cloud-native unified analytics platform that helps with improved inventory and supply chain management. Predictive modeling: Scalable predictive model is developed and deployed across more than 3,000 types of materials at 50-plus locations",
        "Historical analyses: Each material model involves simulating 10,000 Markov Chain Monte Carlo iterations to capture historical distribution of issues Massive performance gains: With a focus on improving performance, the data science team reduced",
        "the inventory analysis and prediction time to 45 minutes from 48 hours on a 50 node Apache SparkIM cluster on Databricks = a 32x performance gain Reduced expenditures: Cost savings equivalent to millions of dollars per year \"Databricks has produced an enormous amount",
        "of value for Shell. The inventory optimization tool Learn more [built on Databricks] was the first scaled up digital product that came out of my organization and the fact that it's deployed globally means we're now delivering millions of dollars of savings every year.\" - Daniel Jeavons",
        "General Manager Advanced Analytics COE Shell databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 92 CHAPTER 10: CUSTOMER CASE STUDIES Riot Games leverages Al Riot Games' goal is to be the world's most player-focused gaming company. Founded in 2006 and based in and",
        "LA, Riot Games is best known for the League of Legends game. Over 100 million gamers play every month. to engage gamers reduce churn Use case: Improving gaming experience through network performance monitoring and combating in-game abusive language.",
        "Solution and benefits: Databricks allows Riot Games to improve the gaming experience of their players by providing scalable, fast analytics. Improved in-game purchase experience: Able to rapidly build and productionize a recommendation engine that provides unique offers based on over 500B data points. Gamers can now more easily find",
        "the content they want. Reduced game lag: Built ML model that detects network issues in real time, enabling Riot Games to avoid outages before they adversely impact players Faster analytics: Increased processing performance of data preparation and exploration by 50% compared to EMR, significantly speeding up analyses m RIDT",
        "Learn more \"We wanted to free data scientists from managing clusters. Having an easy-to-use, managed Spark solution in Databricks allows us to do this. Now our teams can focus on improving the gaming experience.\" - Colin Borys Data Scientist Riot Games databricks",
        "EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 93 CHAPTER 10: CUSTOMER CASE STUDIES Eneco uses ML to reduce Eneco is the technology company behind Toon, the smart energy management device that gives people and",
        "control over their energy usage, their comfort, the security of their homes and much more. Eneco's smart energy consumption devices are in hundreds of thousands of homes across Europe. As such, they maintain Europe's largest operating costs",
        "energy data set, consisting of petabytes of loT data, collected from sensors on appliances throughout the home. With this data, they are on a mission to help their customers live more comfortable lives while reducing energy consumption through personalized energy usage recommendations.",
        "Use case: Personalized energy use recommendations: Leverage machine learning and loT data to power their Waste Checker app, which provides personalized recommendations to reduce in-home energy consumption. Solution and benefits: Databricks provides Eneco with a unified data analytics platform that has fostered a",
        "scalable and collaborative environment across data science and engineering, allowing data teams to more quickly innovate and deliver ML-powered services to Eneco's customers. Lowered costs: Cost-saving features provided by Databricks (such as auto-scaling clusters and Spot Eneco instances) have helped Eneco significantly reduce the operational costs of managing infrastructure,",
        "while still being able to process large amounts of data Faster innovation: With their legacy architecture, moving from proof of concept to production took \"Databricks, through the power of Delta Lake and over 12 months. Now with Databricks, the same process takes less than eight weeks. This enables",
        "structured streaming, allows us to deliver alerts Eneco's data teams to develop new ML-powered features for their customers much faster. and recommendations to our customers with a very limited latency, sO they're able to react Reduced energy consumption: Through their Waste Checker app, Eneco has identified over 67 million",
        "to problems or make adjustments within their kilowatt hours of energy that can be saved by leveraging their personalized recommendations home before it affects their comfort levels.\" - Stephen Galsworthy Learn more Head of Data Science Eneco databricks About Databricks Databricks is the lakehouse company. More than",
        "7,000 organizations worldwide = including Comcast, Condé Nast, H&M and over 50% of the Fortune 500 - rely on the Databricks Lakehouse Platform to unify their data, analytics and Al. Databricks is headquartered in San Francisco, with offices around the globe. Founded by the original creators of",
        "Apache Spark, Delta Lake and MLflow, Databricks is on a mission to help data teams solve the world's toughest problems. To learn more, follow Databricks on Twitter, Linkedin and Facebook. - Schedule a personalized demo Sign up for a free trial databricks",
        "C Databricks 2022. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation. Privacy Policyl Terms of Use"
    ],
    "summaries": [
        {
            "chunk_id": 1,
            "summary": "A collection of machine learning blogs, including code samples and data notebooks, from leading experts in the field."
        },
        {
            "chunk_id": 2,
            "summary": "The following papers have been published in the journal Management Science."
        },
        {
            "chunk_id": 3,
            "summary": "In Case You Missed It: A round-up of interesting technology-related links shared over the weekend."
        },
        {
            "chunk_id": 4,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the most"
        },
        {
            "chunk_id": 5,
            "summary": "This collection of case studies shows how machine learning and artificial intelligence (AI) can be used to"
        },
        {
            "chunk_id": 6,
            "summary": "BBC News takes a look at some of the key stories of the week."
        },
        {
            "chunk_id": 7,
            "summary": "Major League Baseball and the players' union have reached an agreement on a new"
        },
        {
            "chunk_id": 8,
            "summary": "In 1998, the Oakland A's became the first team in Major League Baseball"
        },
        {
            "chunk_id": 9,
            "summary": "The National Football League (NFL) and National Basketball Association (NBA) are two of the world's"
        },
        {
            "chunk_id": 10,
            "summary": "Major League Baseball (MLB) is the world's most popular sport, with more than 300 million fans tuning in"
        },
        {
            "chunk_id": 11,
            "summary": "Major League Baseball has announced it will introduce a pitch-tracking system called Statcast for the"
        },
        {
            "chunk_id": 12,
            "summary": "Major League Baseball is collecting more data than ever before."
        },
        {
            "chunk_id": 13,
            "summary": "Artificial intelligence (AI) and machine learning (ML) are being used in football to improve the performance of"
        },
        {
            "chunk_id": 14,
            "summary": "The following table shows the number of people living in Norway at the end of last year."
        },
        {
            "chunk_id": 15,
            "summary": "BBC Sport takes a look at how data teams are using artificial intelligence (AI) to make"
        },
        {
            "chunk_id": 16,
            "summary": "Statcast is a cloud-based football analytics platform that allows teams to monitor"
        },
        {
            "chunk_id": 17,
            "summary": "Major League Baseball teams have an advantage in the regular season when it comes to scoring runs."
        },
        {
            "chunk_id": 18,
            "summary": "Scientists at the Massachusetts Institute of Technology (MIT) are using artificial intelligence (AI)"
        },
        {
            "chunk_id": 19,
            "summary": "Tampa Bay Rays manager Kevin Cash spoke to the media for the first time since his team'"
        },
        {
            "chunk_id": 20,
            "summary": "The BBC Sport team looks at how data is changing the way cricket is played."
        },
        {
            "chunk_id": 21,
            "summary": "What do you need to know to get the most out of your analytics platform?"
        },
        {
            "chunk_id": 22,
            "summary": "Is there a way to make it easier for players to steal bases?"
        },
        {
            "chunk_id": 23,
            "summary": "Major League Baseball (MLB) teams are looking for ways to improve the performance of their players."
        },
        {
            "chunk_id": 24,
            "summary": "I've been speaking with a number of Major League Baseball teams over the past few months."
        },
        {
            "chunk_id": 25,
            "summary": "In the world of Major League Baseball, there are only a handful of people who can claim"
        },
        {
            "chunk_id": 26,
            "summary": "Big data analytics is the art and science of gathering and analysing vast amounts of data."
        },
        {
            "chunk_id": 27,
            "summary": "The University of Iowa's baseball team uses the latest technology to keep track of player performance."
        },
        {
            "chunk_id": 28,
            "summary": "We have been working with the Major League Baseball (MLB) and its data teams to create a way to"
        },
        {
            "chunk_id": 29,
            "summary": "The following code snippet shows how to access data from an API."
        },
        {
            "chunk_id": 30,
            "summary": "The kitchen is the place where the food is prepared and consumed."
        },
        {
            "chunk_id": 31,
            "summary": "A waiter is a person who serves food to a customer."
        },
        {
            "chunk_id": 32,
            "summary": "Figure 4: Example of how an API works, using a restaurant analogy"
        },
        {
            "chunk_id": 33,
            "summary": "In this article, I'm going to show you how to retrieve and save data using"
        },
        {
            "chunk_id": 34,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at"
        },
        {
            "chunk_id": 35,
            "summary": "The data from Statcast is constantly changing and needs to be updated."
        },
        {
            "chunk_id": 36,
            "summary": "The book is stored on the hard drive in the cloud."
        },
        {
            "chunk_id": 37,
            "summary": "Databricks is a cloud-based data management platform that allows organisations to ingest, store and analyse large amounts of"
        },
        {
            "chunk_id": 38,
            "summary": "This video shows how to use the Auto Loader to ingest large volumes of data."
        },
        {
            "chunk_id": 39,
            "summary": "Here are some examples of how to use Auto Loader in Python."
        },
        {
            "chunk_id": 40,
            "summary": "BBC Hindi's Shilpa Kannan answers your questions about the Indian Premier League."
        },
        {
            "chunk_id": 41,
            "summary": "Figure 7 shows the differences between the two data formats trigger and Auto Loader."
        },
        {
            "chunk_id": 42,
            "summary": "In this tutorial, we are going to look at how to transform the data from the BBC's"
        },
        {
            "chunk_id": 43,
            "summary": "In this article we are going to look at how Databricks works with nested data."
        },
        {
            "chunk_id": 44,
            "summary": "A new data format, called semi- Loader, has been developed."
        },
        {
            "chunk_id": 45,
            "summary": "I've been working with structured data support for a while now and I've come to the conclusion that"
        },
        {
            "chunk_id": 46,
            "summary": "The following papers have been published in the journal Computers in Education."
        },
        {
            "chunk_id": 47,
            "summary": "Is there a way to get the name of a page to be read by the browser?"
        },
        {
            "chunk_id": 48,
            "summary": "The following code shows how to stream data from a checkpoint to a Delta table."
        },
        {
            "chunk_id": 49,
            "summary": "Delta Lake is an open format storage layer that brings reliability, security and .option (\"multiline"
        },
        {
            "chunk_id": 50,
            "summary": "The .format ('delta') I syntax allows flexibility to maintain nested data"
        },
        {
            "chunk_id": 51,
            "summary": "Baseball analytics teams .start use Delta to version Statcast data and enforce specific needs to"
        },
        {
            "chunk_id": 52,
            "summary": "Databricks has announced the release of Delta Lake, an analytics platform for Major League Baseball (MLB"
        },
        {
            "chunk_id": 53,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of"
        },
        {
            "chunk_id": 54,
            "summary": "BBC Sport and BBC Sport Wales have joined forces to bring you BBC Sport and BBC Sport Wales Now, a new"
        },
        {
            "chunk_id": 55,
            "summary": "Tampa Bay Rays left-hander Blake Snell struck out a career-high 12 batters in a win over"
        },
        {
            "chunk_id": 56,
            "summary": "Major League Baseball is experimenting with artificial intelligence (AI) and machine learning (ML) as part of its efforts to"
        },
        {
            "chunk_id": 57,
            "summary": "The Major League Baseball Players Association (MLBPA) has announced that it has reached an agreement"
        },
        {
            "chunk_id": 58,
            "summary": "MLflow Tracking MLflow Projects MLflow Models MLflow Models Registry Record and query Package data science code"
        },
        {
            "chunk_id": 59,
            "summary": "Machine learning (ML) is a branch of computer science that uses computers to solve problems."
        },
        {
            "chunk_id": 60,
            "summary": "To implement machine learning algorithms and models to real-time use cases, The outputs a machine learning"
        },
        {
            "chunk_id": 61,
            "summary": "Major League Baseball is working with Databricks, a London-based start-up"
        },
        {
            "chunk_id": 62,
            "summary": "Here's an example of how to use a machine learning model while data is being used during"
        },
        {
            "chunk_id": 63,
            "summary": "Watch the video above to see how mlflow works."
        },
        {
            "chunk_id": 64,
            "summary": "I .table Chaseatre7 #pass stream through model model_transform model."
        },
        {
            "chunk_id": 65,
            "summary": "A look at some of the rules that have been introduced in Major League Baseball over the past few years."
        },
        {
            "chunk_id": 66,
            "summary": "Major League Baseball and the players' union have reached an agreement on a rule to prevent pitchers from using"
        },
        {
            "chunk_id": 67,
            "summary": "Umpires will be able to check for performance-enhancing drugs during baseball games for the first time in"
        },
        {
            "chunk_id": 68,
            "summary": "Major League Baseball is experimenting with using artificial intelligence to help pitchers."
        },
        {
            "chunk_id": 69,
            "summary": "Here's a look at some of the technology being used by Major League Baseball."
        },
        {
            "chunk_id": 70,
            "summary": "Major League Baseball is using artificial intelligence to predict the performance of pitchers."
        },
        {
            "chunk_id": 71,
            "summary": "Big data and artificial intelligence (AI) are changing the way we work."
        },
        {
            "chunk_id": 72,
            "summary": "The world of sports analytics has changed dramatically in the last few years."
        },
        {
            "chunk_id": 73,
            "summary": "Real-time analytics will be critical to staying one step ahead of the competition."
        },
        {
            "chunk_id": 74,
            "summary": "Databricks has a solution for every sports team."
        },
        {
            "chunk_id": 75,
            "summary": "Retailers are missing out on more than $1 trillion in sales because they don't have enough inventory, according to"
        },
        {
            "chunk_id": 76,
            "summary": "Artificial intelligence (AI) has the power to transform the way we shop."
        },
        {
            "chunk_id": 77,
            "summary": "More than half of UK shoppers say they do not spend enough time looking for bargains, according to a survey by"
        },
        {
            "chunk_id": 78,
            "summary": "One out of 13 products in the UK is not purchasable at the exact moment"
        },
        {
            "chunk_id": 79,
            "summary": "On-shelf availability is one of the biggest challenges facing retailers today."
        },
        {
            "chunk_id": 80,
            "summary": "In this week's Tech Tent, we look at the issue of out-of-stock computers."
        },
        {
            "chunk_id": 81,
            "summary": "Online retail sales in the UK rose by 5.2% in the year to March, according to the Office for"
        },
        {
            "chunk_id": 82,
            "summary": "Online shopping has changed the way people shop."
        },
        {
            "chunk_id": 83,
            "summary": "US shoppers are more likely to switch retailers if they can't find the product they are"
        },
        {
            "chunk_id": 84,
            "summary": "The rise of online shopping has led to a rise in the number of people trying to buy products in-store,"
        },
        {
            "chunk_id": 85,
            "summary": "Over-stocks (OOS) are a growing problem in the US retail industry."
        },
        {
            "chunk_id": 86,
            "summary": "The number of people working in retail has more than doubled"
        },
        {
            "chunk_id": 87,
            "summary": "Some of the world's biggest retailers are experimenting with artificial intelligence (AI) and machine"
        },
        {
            "chunk_id": 88,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at"
        },
        {
            "chunk_id": 89,
            "summary": "How do you solve the problem of out-of-stock goods?"
        },
        {
            "chunk_id": 90,
            "summary": "Have you ever wondered what it's like to run a retail supply chain?"
        },
        {
            "chunk_id": 91,
            "summary": "Databricks is a leading provider of omni-channel analytics and business intelligence solutions for the retail industry."
        },
        {
            "chunk_id": 92,
            "summary": "The OSA and Retail Supply Chain Control Tower solutions are designed to meet the needs of a broad range"
        },
        {
            "chunk_id": 93,
            "summary": "This course will teach you how to solve two of the most common problems"
        },
        {
            "chunk_id": 94,
            "summary": "A few weeks ago, we were contacted by a company offering to help us with our day-to"
        },
        {
            "chunk_id": 95,
            "summary": "In our series of letters from African-American journalists, film-maker and columnist Richard Roeper looks at the"
        },
        {
            "chunk_id": 96,
            "summary": "In our series of letters from African journalists, journalist and columnist Nkosazana Dlamini-Z"
        },
        {
            "chunk_id": 97,
            "summary": "In our series of letters from African journalists, novelist and"
        },
        {
            "chunk_id": 98,
            "summary": "As part of our series of letters from African journalists,"
        },
        {
            "chunk_id": 99,
            "summary": "This paper describes the challenges of building a reliable system for stockout frequently observed OSA challenges manifested in the data."
        },
        {
            "chunk_id": 100,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at the"
        },
        {
            "chunk_id": 101,
            "summary": "The following table shows the expected lead times for each of the following:"
        },
        {
            "chunk_id": 102,
            "summary": "Here are the latest sales figures for May from the Hong Kong Stock Exchange."
        },
        {
            "chunk_id": 103,
            "summary": "As part of our series of letters from UK business leaders, we look at some of the challenges faced by"
        },
        {
            "chunk_id": 104,
            "summary": "Retailers face a number of challenges when it comes to inventory management."
        },
        {
            "chunk_id": 105,
            "summary": "A phantom inventory is an inventory that is not actually on hand."
        },
        {
            "chunk_id": 106,
            "summary": "Warehouse management software (WMS) is an essential part of any warehouse operation."
        },
        {
            "chunk_id": 107,
            "summary": "Out-of-stock events can be a major problem for retailers."
        },
        {
            "chunk_id": 108,
            "summary": "A zero-sales event is when a retailer does not sell any products in a given week."
        },
        {
            "chunk_id": 109,
            "summary": "It's easy to get caught up in the daily ebbs and flows of the"
        },
        {
            "chunk_id": 110,
            "summary": "The following is a daily guide to the key stories, newspaper headlines and quotes from the news"
        },
        {
            "chunk_id": 111,
            "summary": "Figure 2: Examining the cumulative probability of consecutive: zero-sales events to identify potential OSAC alerts Figure 3:"
        },
        {
            "chunk_id": 112,
            "summary": "Zero-sales events are a common problem in the retail industry."
        },
        {
            "chunk_id": 113,
            "summary": "This chart shows the cumulative probability of back-to-back zero-sales events over the course of"
        },
        {
            "chunk_id": 114,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the"
        },
        {
            "chunk_id": 115,
            "summary": "This paper describes the challenges faced by data scientists working in the oil and gas industry and how"
        },
        {
            "chunk_id": 116,
            "summary": "Power BI is a powerful, yet easy-to-use, analytics platform."
        },
        {
            "chunk_id": 117,
            "summary": "The UK's offshore oil and gas industry has a huge amount of data to analyse"
        },
        {
            "chunk_id": 118,
            "summary": "In our series of articles on how to get the most out of your supply chain, we take a look at"
        },
        {
            "chunk_id": 119,
            "summary": "An out-of-stock or supply chain problem can be solved with a few simple steps"
        },
        {
            "chunk_id": 120,
            "summary": "The Tredence Data Platform is designed to meet the needs of large-scale data processing."
        },
        {
            "chunk_id": 121,
            "summary": "Tredence has added a number of new features to its Databricks platform, including the ability to"
        },
        {
            "chunk_id": 122,
            "summary": "In our series of letters from African journalists, filmmaker and columnist Ahmedou Ould-Abdallah looks at"
        },
        {
            "chunk_id": 123,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the most"
        },
        {
            "chunk_id": 124,
            "summary": "This is the second part of our series on how to use artificial intelligence (AI"
        },
        {
            "chunk_id": 125,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid"
        },
        {
            "chunk_id": 126,
            "summary": "Google's artificial intelligence (AI) assistant, Assistant, has been given the task of"
        },
        {
            "chunk_id": 127,
            "summary": "Dynamic time warping is a powerful technique that can be applied across many different domains."
        },
        {
            "chunk_id": 128,
            "summary": "A daily guide to the key stories, newspaper headlines and quotes from the news."
        },
        {
            "chunk_id": 129,
            "summary": "Wearable technology is changing the way we get around - here are some of the"
        },
        {
            "chunk_id": 130,
            "summary": "How much do you know about the drivers of the world's fastest cars?"
        },
        {
            "chunk_id": 131,
            "summary": "In our series of blogs on big data, we will be looking at some of the techniques used in big data"
        },
        {
            "chunk_id": 132,
            "summary": "Dynamic time warping on sample sales data using MLflow"
        },
        {
            "chunk_id": 133,
            "summary": "Dynamic time warping is a technique used to compare time series between points on a map."
        },
        {
            "chunk_id": 134,
            "summary": "Time warping is a technique used to find patterns in time series."
        },
        {
            "chunk_id": 135,
            "summary": "When looking at time series, it is useful to note how the time series has changed over time."
        },
        {
            "chunk_id": 136,
            "summary": "dynamic time warping has been proposed as an alternative to traditional time series matching."
        },
        {
            "chunk_id": 137,
            "summary": "f(x) maps to f(x) when i  =j to think of"
        },
        {
            "chunk_id": 138,
            "summary": "In this paper, we present a new approach to the analysis of audio clips."
        },
        {
            "chunk_id": 139,
            "summary": "In this series, we will be looking at some of the most famous quotes from famous people."
        },
        {
            "chunk_id": 140,
            "summary": "This is a new time series based on clip 1 where the intonation and them are extremely exaggerated"
        },
        {
            "chunk_id": 141,
            "summary": "Here's another time series that's based on the quote \"Doors and corners, kid.\""
        },
        {
            "chunk_id": 142,
            "summary": "This is a new time series based on clip 1 where the intonation and speech pattern is similar to clip 1"
        },
        {
            "chunk_id": 143,
            "summary": "Doors and corners, kid, that's where they get you."
        },
        {
            "chunk_id": 144,
            "summary": "You walk into a room too fast, Clip 3 Doors and corners, kid."
        },
        {
            "chunk_id": 145,
            "summary": "BBC News takes a look at some of the top stories of the week."
        },
        {
            "chunk_id": 146,
            "summary": "The following code snippet shows how to create a subplot in scipy.io."
        },
        {
            "chunk_id": 147,
            "summary": "The Big Book of learning CASES 2ND contains the following code:"
        },
        {
            "chunk_id": 148,
            "summary": "In this article, I will be looking at how dynamic time warping can be used to compare time series between two"
        },
        {
            "chunk_id": 149,
            "summary": "Match of the Day presenter Gary Lineker and his wife Rochelle are celebrating their 50th wedding anniversary"
        },
        {
            "chunk_id": 150,
            "summary": "In this article, we are going to compare the time series of fastdtw with the time series of PyPi"
        },
        {
            "chunk_id": 151,
            "summary": "The code base can be found in the notebook Dynamic Time Warping."
        },
        {
            "chunk_id": 152,
            "summary": "In our series of letters from African journalists, filmmaker and columnist Ahmedou Ould-Abdallah looks at some"
        },
        {
            "chunk_id": 153,
            "summary": "This clip is longer than the others because of the exaggerated intonation and speed of the words."
        },
        {
            "chunk_id": 154,
            "summary": "In our series of letters from African journalists, film-maker and columnist Farai Sevenzo looks at some of the"
        },
        {
            "chunk_id": 155,
            "summary": "How do you predict the demand for your products?"
        },
        {
            "chunk_id": 156,
            "summary": "It's that time of year again."
        },
        {
            "chunk_id": 157,
            "summary": "If you're running a small business and you're struggling to make ends meet"
        },
        {
            "chunk_id": 158,
            "summary": "How do you know when your production capacity is high or low?"
        },
        {
            "chunk_id": 159,
            "summary": "According to the US Bureau of Labor Statistics, US production capacity..."
        },
        {
            "chunk_id": 160,
            "summary": "The latest weekly sales figures from the UK's Department for Work and Pensions (DWP)"
        },
        {
            "chunk_id": 161,
            "summary": "How do you choose the right product for the right time of year?"
        },
        {
            "chunk_id": 162,
            "summary": "Choosing the right product for your business can be tricky."
        },
        {
            "chunk_id": 163,
            "summary": "If you run a commercial printing business, you'll know how important it is to"
        },
        {
            "chunk_id": 164,
            "summary": "If you'd like to know when your next album will be released, here's"
        },
        {
            "chunk_id": 165,
            "summary": "If you want to know what your customers are buying, you might want to"
        },
        {
            "chunk_id": 166,
            "summary": "In this series, we'll look at the best-selling products of the year."
        },
        {
            "chunk_id": 167,
            "summary": "We will use the weekly sales transaction data set found in the UCI Data Set # Calculate distance via dynamic time warping"
        },
        {
            "chunk_id": 168,
            "summary": "import numpy as np James Tan, emssussedusgestan Singapore University of"
        },
        {
            "chunk_id": 169,
            "summary": "# Review data display spark.createatarame (sales_pdf))"
        },
        {
            "chunk_id": 170,
            "summary": "calibrated by the BBC's science and technology team."
        },
        {
            "chunk_id": 171,
            "summary": "The P5 8 5 13 11 14 11 18 distribution of DTW distances in a P6 3 3 2 7"
        },
        {
            "chunk_id": 172,
            "summary": "The following table shows the weekly sales figures for the top 10 best-selling children's toys in the UK in"
        },
        {
            "chunk_id": 173,
            "summary": "The following table lists the most popular products sold in the UK in the past year."
        },
        {
            "chunk_id": 174,
            "summary": "We wanted to see if there were any product codes that were furthest from the optimal sales trend"
        },
        {
            "chunk_id": 175,
            "summary": "Let's select the products that are closest to the optimal sales trend."
        },
        {
            "chunk_id": 176,
            "summary": "Optimal Sales Trend by cast (dtw_dist as float) limit 30"
        },
        {
            "chunk_id": 177,
            "summary": "The chart below shows the best-selling products in the UK over the past 12 months."
        },
        {
            "chunk_id": 178,
            "summary": "We're comparing the time it takes a product to reach its destination with the time it"
        },
        {
            "chunk_id": 179,
            "summary": "The following table shows the best and worst match-ups for the Bigbricks CASES 2ND product"
        },
        {
            "chunk_id": 180,
            "summary": "We've been using machine learning to find the best products for our production line."
        },
        {
            "chunk_id": 181,
            "summary": "We've put together a list of the best machine learning notebooks on the market."
        },
        {
            "chunk_id": 182,
            "summary": "In this post I want to show you how to use MLflow to find the most suitable product for a given"
        },
        {
            "chunk_id": 183,
            "summary": "We've been running a series of experiments to see if it's possible to measure"
        },
        {
            "chunk_id": 184,
            "summary": "We're running a model experiment to see if we can find a way to predict the future performance of"
        },
        {
            "chunk_id": 185,
            "summary": "A selection of some of the best news photographs from the past 24 hours."
        },
        {
            "chunk_id": 186,
            "summary": "In this tutorial, I'm going to show you how to use mlflow"
        },
        {
            "chunk_id": 187,
            "summary": "In this article, we are going to look at how to implement an MLflow experiment."
        },
        {
            "chunk_id": 188,
            "summary": "# Log Model using Custom Flavor dtw_model = 'stretch_factor' : float"
        },
        {
            "chunk_id": 189,
            "summary": "The following is a partial list of all the runners who took part in this year's London Marathon"
        },
        {
            "chunk_id": 190,
            "summary": "We're running a series of experiments to see if there's a relationship between the amount of time it"
        },
        {
            "chunk_id": 191,
            "summary": "In this paper we show how Databricks can be used to store experimental runs of the Distance"
        },
        {
            "chunk_id": 192,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the most"
        },
        {
            "chunk_id": 193,
            "summary": "In this blog post, we are going to look at how to use Databricks to"
        },
        {
            "chunk_id": 194,
            "summary": "MLflow is an easy-to-use Python program that allows you to measure the distance between two"
        },
        {
            "chunk_id": 195,
            "summary": "We've just published a new version of our time-series model, which takes into account the"
        },
        {
            "chunk_id": 196,
            "summary": "In this post I'm going to show you how to use the Python Inference API to create"
        },
        {
            "chunk_id": 197,
            "summary": "This paper describes the use of a Python function model in MLflow."
        },
        {
            "chunk_id": 198,
            "summary": "Machine learning (ML) is one of the fastest growing areas of computer science."
        },
        {
            "chunk_id": 199,
            "summary": "Python and MLflow have come together to create an easy-to-use wrapper around the"
        },
        {
            "chunk_id": 200,
            "summary": "In this article we are going to look at how to use Python's Logging function to create"
        },
        {
            "chunk_id": 201,
            "summary": "pandas MLflow is a wrapper around the pandas DataFrame and NumPy MLflow libraries."
        },
        {
            "chunk_id": 202,
            "summary": "Python is an open-source framework for building software."
        },
        {
            "chunk_id": 203,
            "summary": "Here's how we've done it:"
        },
        {
            "chunk_id": 204,
            "summary": "We've been using MLflow to build a machine learning model for a problem we've been"
        },
        {
            "chunk_id": 205,
            "summary": "In our series of articles on how to use artificial intelligence (AI) and machine learning (ML) in"
        },
        {
            "chunk_id": 206,
            "summary": "In this session, we discussed how to use Spark to query large amounts of data."
        },
        {
            "chunk_id": 207,
            "summary": "In our series of tutorials on how to use Databricks for machine learning, we'll be looking at"
        },
        {
            "chunk_id": 208,
            "summary": "In this tutorial we will show you how to load a custom model into mllox."
        },
        {
            "chunk_id": 209,
            "summary": "This is an example of how to use the predicted distance model."
        },
        {
            "chunk_id": 210,
            "summary": "In our series of letters from African journalists, filmmaker and columnist Farai Sevenzo looks"
        },
        {
            "chunk_id": 211,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks"
        },
        {
            "chunk_id": 212,
            "summary": "The Financial Conduct Authority (FCA) has published new rules to help banks and other financial institutions identify known"
        },
        {
            "chunk_id": 213,
            "summary": "The European Central Bank (ECB) has published a set of rules on how to identify fraud-based"
        },
        {
            "chunk_id": 214,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid"
        },
        {
            "chunk_id": 215,
            "summary": "A fraud detection workflow is a set of rules and procedures that are used to"
        },
        {
            "chunk_id": 216,
            "summary": "In this tutorial, we will look at how to build a fraud detection model from a sample of data."
        },
        {
            "chunk_id": 217,
            "summary": "Big data is the collection, analysis and use of vast amounts of personal data."
        },
        {
            "chunk_id": 218,
            "summary": "This paper describes the challenges faced by the Data Engineer team in implementing a fraud detection system."
        },
        {
            "chunk_id": 219,
            "summary": "Rule-based fraud detection is becoming an increasingly important part of the fight against cybercrime."
        },
        {
            "chunk_id": 220,
            "summary": "The fraud detection team at Databricks is made up of three distinct teams."
        },
        {
            "chunk_id": 221,
            "summary": "In this course you will learn how to create a machine"
        },
        {
            "chunk_id": 222,
            "summary": "In our series of letters from African journalists, filmmaker and columnist Ahmed Rashid looks at the challenges faced by"
        },
        {
            "chunk_id": 223,
            "summary": "In this talk, we will learn how to detect fraud using decision trees using Apache SparkM MLlib."
        },
        {
            "chunk_id": 224,
            "summary": "In our series of letters from African journalists, filmmaker and columnist Ahmedou Ould-Abdallah looks at the"
        },
        {
            "chunk_id": 225,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the"
        },
        {
            "chunk_id": 226,
            "summary": "Fraud detection at scale is a difficult problem to solve because of the complexity of the problem, the size of the"
        },
        {
            "chunk_id": 227,
            "summary": "A new approach to fraud detection has been developed by researchers at the University of California, Los Angeles"
        },
        {
            "chunk_id": 228,
            "summary": "This paper aims to provide an overview of the state-of-the-art"
        },
        {
            "chunk_id": 229,
            "summary": "A decision tree is a machine learning model that is trained to a set of rules."
        },
        {
            "chunk_id": 230,
            "summary": "In our series of letters from African journalists, film-maker and columnist Farai Sevenzo looks at some of"
        },
        {
            "chunk_id": 231,
            "summary": "In this article, I will show you how to hard code"
        },
        {
            "chunk_id": 232,
            "summary": "Researchers at the Massachusetts Institute of Technology (MIT) have developed a machine learning model that can detect"
        },
        {
            "chunk_id": 233,
            "summary": "Researchers at the University of California, Los Angeles (UCLA) have developed a machine learning algorithm that can detect"
        },
        {
            "chunk_id": 234,
            "summary": "The aim of this study is to develop a machine learning approach for the analysis of large amounts of data."
        },
        {
            "chunk_id": 235,
            "summary": "This is the first in a series of posts on how to"
        },
        {
            "chunk_id": 236,
            "summary": "Databricks is an open-source software platform that allows us to build and"
        },
        {
            "chunk_id": 237,
            "summary": "In our series of articles on how to work with big data, we're going to take a look at"
        },
        {
            "chunk_id": 238,
            "summary": "What do you want to know about the data?"
        },
        {
            "chunk_id": 239,
            "summary": "How do I import data from Kaggle into Azure and AWS?"
        },
        {
            "chunk_id": 240,
            "summary": "The Bank of England has released the latest version of its PaySim mobile money simulation tool."
        },
        {
            "chunk_id": 241,
            "summary": "The following table shows the status of the oldbalance.org service in an African country."
        },
        {
            "chunk_id": 242,
            "summary": "The following table shows the amount of time it takes for a transaction to be completed:"
        },
        {
            "chunk_id": 243,
            "summary": "Details of all transactions carried out by the Bank on behalf of its customers."
        },
        {
            "chunk_id": 244,
            "summary": "Let's take a look at some of the key statistics for the retail sector in the UK."
        },
        {
            "chunk_id": 245,
            "summary": "The table below lists the key financials for the six months to 30 June 2014."
        },
        {
            "chunk_id": 246,
            "summary": "In this paper, we train a fraud detection model based on a set of rules established by the domain experts."
        },
        {
            "chunk_id": 247,
            "summary": "# Rules to identify Known Fraud-based df = df.withColumn (\"label"
        },
        {
            "chunk_id": 248,
            "summary": "The Bank of England's latest inflation figures show that the value of banknotes issued in England and Wales rose by "
        },
        {
            "chunk_id": 249,
            "summary": "In the world of fraud detection, there are a number of rules that need to be followed."
        },
        {
            "chunk_id": 250,
            "summary": "The World Anti-Doping Agency (Wada) has announced a new set of standards"
        },
        {
            "chunk_id": 251,
            "summary": "This decision tree shows the relationship between the number of transactions and the amount of money involved."
        },
        {
            "chunk_id": 252,
            "summary": "We will build and validate our ML model, we will do an 80/20 split using"
        },
        {
            "chunk_id": 253,
            "summary": "This experiment is designed to test the hypotheses that:"
        },
        {
            "chunk_id": 254,
            "summary": "In this tutorial, we will learn how to create a pipeline for a machine learning model."
        },
        {
            "chunk_id": 255,
            "summary": "In this paper, we use a tree model to train a machine learning algorithm."
        },
        {
            "chunk_id": 256,
            "summary": "In this tutorial, we will build a decision tree model and train it to make predictions on different data sets."
        },
        {
            "chunk_id": 257,
            "summary": "The following code is derived from the Pyspark.ml.feature and Pyspark.ml"
        },
        {
            "chunk_id": 258,
            "summary": "# VectorAssembler is a transformer that combines a given list of columns into a single vector column va - Vector"
        },
        {
            "chunk_id": 259,
            "summary": "Here's how to build a decision tree for a drug development project:"
        },
        {
            "chunk_id": 260,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at"
        },
        {
            "chunk_id": 261,
            "summary": "In this study, we investigate the effect of training and test sets on the likelihood of recall."
        },
        {
            "chunk_id": 262,
            "summary": "ParamcridBuilder train_pred = CVModel u .transform (train) test_pred CV"
        },
        {
            "chunk_id": 263,
            "summary": "I auc_train = evaluatorC.evaluate (train_pred) .build"
        },
        {
            "chunk_id": 264,
            "summary": "Print out the results of the PR and AUC tests."
        },
        {
            "chunk_id": 265,
            "summary": "Train the model using the pipeline, paramet ter grid, and preceding BinaryClassifcationivalu"
        },
        {
            "chunk_id": 266,
            "summary": "Let's take a look at the results of a survey on whether people would be willing to travel by train"
        },
        {
            "chunk_id": 267,
            "summary": "How do you measure fraud in the UK?"
        },
        {
            "chunk_id": 268,
            "summary": "print (\"Total count: 8s, Fraud cases count: 8s\" % (N, Y, p"
        },
        {
            "chunk_id": 269,
            "summary": "This graph shows the changes in the number of cases we have identified in our training dataset."
        },
        {
            "chunk_id": 270,
            "summary": "We have developed an algorithm that can predict the likelihood of a child being born with a rare genetic"
        },
        {
            "chunk_id": 271,
            "summary": "We are going to divide our data set into 51% fraud and 49% non-fraud cases."
        },
        {
            "chunk_id": 272,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the"
        },
        {
            "chunk_id": 273,
            "summary": "We are trying to reduce the number of fraudulent cases by balancing the classes in our ML pipeline."
        },
        {
            "chunk_id": 274,
            "summary": "Train the model using the pipeline grid, and BinaryClassifcationivaluator using the train_b"
        },
        {
            "chunk_id": 275,
            "summary": "Fraud, fraud, fraud, fraud, fraud, fraud, fraud, fraud, fraud, fraud,"
        },
        {
            "chunk_id": 276,
            "summary": "We want to collect feedback on the model selected for production."
        },
        {
            "chunk_id": 277,
            "summary": "In this paper, we present a new model for label-based 3D printing."
        },
        {
            "chunk_id": 278,
            "summary": "We want to make it as easy as possible for you to see how our machine learning"
        },
        {
            "chunk_id": 279,
            "summary": "The US Food and Drug Administration (FDA) has announced that it is adding a new type of feedback to its"
        },
        {
            "chunk_id": 280,
            "summary": "We use artificial intelligence (AI) to train a machine learning model."
        },
        {
            "chunk_id": 281,
            "summary": "In this talk, we are going to look at how to use the MLflow UI to"
        },
        {
            "chunk_id": 282,
            "summary": "In this talk, we will look at how you can use Databricks and MLflow to"
        },
        {
            "chunk_id": 283,
            "summary": "This project aims to create a machine learning model to identify fraud."
        },
        {
            "chunk_id": 284,
            "summary": "In this project we have been working with a team of researchers to develop a new way to"
        },
        {
            "chunk_id": 285,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks"
        },
        {
            "chunk_id": 286,
            "summary": "A team of researchers at the University of California, Los Angeles (UCLA) is using the Databricks platform"
        },
        {
            "chunk_id": 287,
            "summary": "A fraud detection team at one of the world's largest banks is using artificial intelligence (AI) to"
        },
        {
            "chunk_id": 288,
            "summary": "In this series, BBC News takes a look at some of the key stories of the"
        },
        {
            "chunk_id": 289,
            "summary": "In our series of articles on forecasting, we look at how retailers are using time series"
        },
        {
            "chunk_id": 290,
            "summary": "In today's hyper-connected world, it's more important than ever to"
        },
        {
            "chunk_id": 291,
            "summary": "In this chapter, we'll discuss the importance of time series forecasting,"
        },
        {
            "chunk_id": 292,
            "summary": "We'll show you how to train hundreds of models at once, allowing you to create precise"
        },
        {
            "chunk_id": 293,
            "summary": "Time series analysis has never been more important to retailers."
        },
        {
            "chunk_id": 294,
            "summary": "As the US economy continues to recover from the Great Recession, retailers and manufacturers are facing a"
        },
        {
            "chunk_id": 295,
            "summary": "Retailers face a number of challenges when it comes to product placement."
        },
        {
            "chunk_id": 296,
            "summary": "Some of the key findings from this year's World Economic Forum (WEF)"
        },
        {
            "chunk_id": 297,
            "summary": "Retailers are increasingly turning to artificial intelligence (AI) and machine learning (ML) to help"
        },
        {
            "chunk_id": 298,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the"
        },
        {
            "chunk_id": 299,
            "summary": "The rise of the internet of things (IoT) has led to a shift in the"
        },
        {
            "chunk_id": 300,
            "summary": "This chart shows the annual rate of change in the annual rate of change in the average"
        },
        {
            "chunk_id": 301,
            "summary": "In this talk we will be looking at how we can use Databricks Collaborative to"
        },
        {
            "chunk_id": 302,
            "summary": "Visualizing demand in time series data 1.0M 800k"
        },
        {
            "chunk_id": 303,
            "summary": "In this talk we will demonstrate the use of Prophet to generate demand forecasts for individual stores and products."
        },
        {
            "chunk_id": 304,
            "summary": "In our series of letters from African journalists, film-maker and"
        },
        {
            "chunk_id": 305,
            "summary": "This chart shows the year-on-year change in retail sales in the UK between 2013 and 2017."
        },
        {
            "chunk_id": 306,
            "summary": "BBC News takes a look back at some of the key stories over the past five years."
        },
        {
            "chunk_id": 307,
            "summary": "In last week's article, we looked at how to build a weather model that can predict the"
        },
        {
            "chunk_id": 308,
            "summary": "In our series of posts on artificial intelligence and machine learning, we're looking at how to use sklearn to"
        },
        {
            "chunk_id": 309,
            "summary": "The following code shows how to build a model # predict over a dataset and import pandas as pd."
        },
        {
            "chunk_id": 310,
            "summary": "Here's how it's done:"
        },
        {
            "chunk_id": 311,
            "summary": "This chart shows the annualised growth rates of the UK's retail sales over the past five years"
        },
        {
            "chunk_id": 312,
            "summary": "In our series of letters from African journalists, film-maker and columnist Yolande Bartosz looks at the"
        },
        {
            "chunk_id": 313,
            "summary": "Predicting the outcome of the UK general election."
        },
        {
            "chunk_id": 314,
            "summary": "Cloud-based training services such as Prophet and Spark allow companies to train their employees on-demand"
        },
        {
            "chunk_id": 315,
            "summary": "We've been working with the National Oceanic and Atmospheric Administration (NOAA) and the National Weather Service"
        },
        {
            "chunk_id": 316,
            "summary": "In this article, we will look at how Spark can be used to ingest and store large amounts of data"
        },
        {
            "chunk_id": 317,
            "summary": "A new approach to building models has been developed by researchers at the Massachusetts Institute of Technology ("
        },
        {
            "chunk_id": 318,
            "summary": "The amount of milk that is needed to make a cup of coffee is determined by the amount of milk"
        },
        {
            "chunk_id": 319,
            "summary": "In this case, we're looking at the number of people working in different parts of"
        },
        {
            "chunk_id": 320,
            "summary": "How to use Spark DataFrames to distribute the processing of a specific worker node"
        },
        {
            "chunk_id": 321,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at"
        },
        {
            "chunk_id": 322,
            "summary": "In our next section, we will look at how to train time series models using pandas."
        },
        {
            "chunk_id": 323,
            "summary": "The BBC News website has a new look."
        },
        {
            "chunk_id": 324,
            "summary": "pandas # instantiate the model, configure the parameters function, and allow us to apply a custom function to"
        },
        {
            "chunk_id": 325,
            "summary": "This function will train a model for each group in the DataFrame, and predict on each group in the"
        },
        {
            "chunk_id": 326,
            "summary": "We will be generating store-item level forecasts and presenting them as a single output data set."
        },
        {
            "chunk_id": 327,
            "summary": "In this post, I'm going to show you how to build a pandas model using the latest version of Spark"
        },
        {
            "chunk_id": 328,
            "summary": "# make predictions results_pd = model.predict (future_pd) Within the function definition, we"
        },
        {
            "chunk_id": 329,
            "summary": "In our series of letters from readers, we look at some of the ways you can use the internet to"
        },
        {
            "chunk_id": 330,
            "summary": "We have constructed a forecast for each store-item"
        },
        {
            "chunk_id": 331,
            "summary": "As part of our series on the retail sector, we're looking at how the fashion industry is"
        },
        {
            "chunk_id": 332,
            "summary": "This chart shows the seasonal demand forecasts for the UK's major department stores."
        },
        {
            "chunk_id": 333,
            "summary": "To help you keep track of the changes we've made to Pypark over the past year, we've"
        },
        {
            "chunk_id": 334,
            "summary": "The following forecasts are based on the most up-to-date sales data available."
        },
        {
            "chunk_id": 335,
            "summary": "In our series of letters from African journalists, film-maker and columnist Farai Sevenzo looks at"
        },
        {
            "chunk_id": 336,
            "summary": "In this talk, I'll show you how to use PyTorch Lightning and"
        },
        {
            "chunk_id": 337,
            "summary": "Horovod is a machine learning platform designed to accelerate the development and deployment of deep learning applications."
        },
        {
            "chunk_id": 338,
            "summary": "Artificial intelligence (AI) is a branch of computer science that uses computer algorithms to learn and"
        },
        {
            "chunk_id": 339,
            "summary": "In today's data-driven world, it is more important than ever that your code is reusable."
        },
        {
            "chunk_id": 340,
            "summary": "In this paper we are going to show how to scale up a big data analytics experiment using databricks, a"
        },
        {
            "chunk_id": 341,
            "summary": "In this article, I'm going to show you how to build a deep learning"
        },
        {
            "chunk_id": 342,
            "summary": "In this tutorial, I will show you how to run a large-scale experiment on a"
        },
        {
            "chunk_id": 343,
            "summary": "Databricks supports single-node clusters to support this very usage pattern."
        },
        {
            "chunk_id": 344,
            "summary": "T4 GPUS is a GPU-accelerated, low-cost, high-"
        },
        {
            "chunk_id": 345,
            "summary": "Pytorch-ightning 1.7.2 has been tested on AWS and Azure."
        },
        {
            "chunk_id": 346,
            "summary": "workspace libraries are used to create workspaces on the driver nodes."
        },
        {
            "chunk_id": 347,
            "summary": "Figure 2: Key components DBR 11.1 ML install Figure 2: Key components DBR 10.4 ML install Figure 3: Key components D"
        },
        {
            "chunk_id": 348,
            "summary": "In this article, we will learn how to store raw data using the open source Linux Foundation project Delta Lake."
        },
        {
            "chunk_id": 349,
            "summary": "MLflow is an open-source tool for building deep learning models."
        },
        {
            "chunk_id": 350,
            "summary": "In this article, we are going to look at how to set up a case-based learning environment that"
        },
        {
            "chunk_id": 351,
            "summary": "Petastorm is a library for building data lakes for deep learning."
        },
        {
            "chunk_id": 352,
            "summary": "In this talk, we will look at some of the techniques used to classify data sets."
        },
        {
            "chunk_id": 353,
            "summary": "Petastorm is a distributed machine learning platform."
        },
        {
            "chunk_id": 354,
            "summary": "This week I'm going to show you how to transform a Spark DataFrame into a Peta"
        },
        {
            "chunk_id": 355,
            "summary": "In our dandelion (0) PyTorch code, we use the spark_converter object to convert a dataset into a"
        },
        {
            "chunk_id": 356,
            "summary": "The following is a list of the most common flower species in the world."
        },
        {
            "chunk_id": 357,
            "summary": "In this article, I'm going to show you how to structure your"
        },
        {
            "chunk_id": 358,
            "summary": "In this tutorial I'll show you how to create a Mathematica notebook with all the classes and functions"
        },
        {
            "chunk_id": 359,
            "summary": "In this article I will be using PyTorch Lightning to set up an experimental model and the main training loop"
        },
        {
            "chunk_id": 360,
            "summary": "Here are some of the key features of our project:"
        },
        {
            "chunk_id": 361,
            "summary": "A quick look at some of the techniques used in the development of Spark."
        },
        {
            "chunk_id": 362,
            "summary": "This module contains the code for the model architecture itself in a model class, This is the main training function"
        },
        {
            "chunk_id": 363,
            "summary": "This module will be used to update the PyTorch Lightning Trainer."
        },
        {
            "chunk_id": 364,
            "summary": "In this tutorial I will show you how to set up a training process using"
        },
        {
            "chunk_id": 365,
            "summary": "In this tutorial, I will show you how to use PyTorch Lightning on GPUs."
        },
        {
            "chunk_id": 366,
            "summary": "Petastorm's DataLoader makes it easy to load and manage data sets."
        },
        {
            "chunk_id": 367,
            "summary": "We want to make it easier to train PyTorch on Spark."
        },
        {
            "chunk_id": 368,
            "summary": "PyTorch DataLoader is an add-on to PyTorch that allows us to load data into theTorch"
        },
        {
            "chunk_id": 369,
            "summary": "In the second part of this series, I'm going to show you how to"
        },
        {
            "chunk_id": 370,
            "summary": "In our next release we are going to be introducing a new class called os.environ, which will train"
        },
        {
            "chunk_id": 371,
            "summary": "In this article we are going to look at how to distribute deep learning models between GPUs."
        },
        {
            "chunk_id": 372,
            "summary": "In this paper, we show how to use the Hordovo training system to"
        },
        {
            "chunk_id": 373,
            "summary": "We have a training job running on our GPUS and it needs to be distributed across our nodes."
        },
        {
            "chunk_id": 374,
            "summary": "In this article I'm going to show you how to scale up a single-node HorovodRunner"
        },
        {
            "chunk_id": 375,
            "summary": "Horovod is a machine learning algorithm that learns faster than any other."
        },
        {
            "chunk_id": 376,
            "summary": "In this tutorial we will go through the steps of setting up a multi-GPU system."
        },
        {
            "chunk_id": 377,
            "summary": "In this tutorial, I will show you how to use PyTorch Lightning to scale"
        },
        {
            "chunk_id": 378,
            "summary": "ckpt_restore path to point to ckpt, the train function will resume training from checkpoint that"
        },
        {
            "chunk_id": 379,
            "summary": "We are going to scale out our train function to multiple GPUS on one node"
        },
        {
            "chunk_id": 380,
            "summary": "hvd_model = hr.run (train_hvd) is a Spark-"
        },
        {
            "chunk_id": 381,
            "summary": "PyTorch Lightning runs on a single driver node with 4 GPUS."
        },
        {
            "chunk_id": 382,
            "summary": "Preview of the latest release of the Spark artificial intelligence platform."
        },
        {
            "chunk_id": 383,
            "summary": "BBC Sport takes a look at some of the key statistics behind this year's Rugby World Cup."
        },
        {
            "chunk_id": 384,
            "summary": "Figure 5: Multi-node scaling DBU /hour: 25.65 0 g4dn.4"
        },
        {
            "chunk_id": 385,
            "summary": "In this article, I will show you how to set up a multi-node cluster using"
        },
        {
            "chunk_id": 386,
            "summary": "In this post we are going to look at how to run distributed training on Databricks."
        },
        {
            "chunk_id": 387,
            "summary": "The following table lists the most common problems faced by students at the University of Bath."
        },
        {
            "chunk_id": 388,
            "summary": "We use GPUS to train deep neural networks."
        },
        {
            "chunk_id": 389,
            "summary": "We want to make sure that we are still seeing function with the num devices variable at the end of each epoch."
        },
        {
            "chunk_id": 390,
            "summary": "For our experiments, we set min_ delta to 0.01, so we expect to see"
        },
        {
            "chunk_id": 391,
            "summary": "The train loop will continue to run up to 10 epochs of no improvement before the training"
        },
        {
            "chunk_id": 392,
            "summary": "val_threshold is the level at which we stop the training process."
        },
        {
            "chunk_id": 393,
            "summary": "In this post we are going to show you how to repartition a Spark machine."
        },
        {
            "chunk_id": 394,
            "summary": "How do I ensure that all the GPUS in my cluster are running smoothly?"
        },
        {
            "chunk_id": 395,
            "summary": "How do you improve the performance of your computer?"
        },
        {
            "chunk_id": 396,
            "summary": "It's been a busy week in the world of artificial intelligence, with the release of"
        },
        {
            "chunk_id": 397,
            "summary": "Here's a breakdown of how much time we spent building and running the PyTorch"
        },
        {
            "chunk_id": 398,
            "summary": "This graph shows the HorovodRunner training time as we increased the system resources."
        },
        {
            "chunk_id": 399,
            "summary": "Petastorm has put together a list of the best practices for adding GPUS to deep learning systems."
        },
        {
            "chunk_id": 400,
            "summary": "In this article, I will show you how to scale Deep Learning in six easy steps"
        },
        {
            "chunk_id": 401,
            "summary": "In this paper, we show how to set the stopping_threshold of a single-GPU training run"
        },
        {
            "chunk_id": 402,
            "summary": "Geospatial technology is changing the way we live, work and play."
        },
        {
            "chunk_id": 403,
            "summary": "Big data and geospatial technologies are changing the way we work and live."
        },
        {
            "chunk_id": 404,
            "summary": "BBC News NI takes a look at some of the key stories of the week."
        },
        {
            "chunk_id": 405,
            "summary": "Geospatial data can be used for a variety of purposes."
        },
        {
            "chunk_id": 406,
            "summary": "In our series of letters from African journalists, filmmaker and columnist Ahmed Rashid looks at some of the"
        },
        {
            "chunk_id": 407,
            "summary": "In our series of letters from African journalists, filmmaker and columnist Ahmedou Ould-Abdallah looks at the"
        },
        {
            "chunk_id": 408,
            "summary": "The use of spatial data has become an increasingly important part of our daily lives."
        },
        {
            "chunk_id": 409,
            "summary": "In our series of letters from African journalists, filmmaker and columnist Ahmedou Ould-Abdallah looks at the"
        },
        {
            "chunk_id": 410,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at the"
        },
        {
            "chunk_id": 411,
            "summary": "This paper looks at the challenges faced by enterprises in managing large volumes of geospatial data."
        },
        {
            "chunk_id": 412,
            "summary": "Geodatabases have been used in the oil and gas industry for many years."
        },
        {
            "chunk_id": 413,
            "summary": "In our series of letters from African journalists, filmmaker and columnist Ahmedou Ould-Abdallah looks at"
        },
        {
            "chunk_id": 414,
            "summary": "In our series of blog posts on big data, we look at some of the challenges faced by big data"
        },
        {
            "chunk_id": 415,
            "summary": "Databricks is a Java-based data analytics platform for big data and machine learning."
        },
        {
            "chunk_id": 416,
            "summary": "Databricks has announced Databricks UDAP, a next-generation distributed analytics platform for"
        },
        {
            "chunk_id": 417,
            "summary": "This paper looks at the pros and cons of Geospatial."
        },
        {
            "chunk_id": 418,
            "summary": "In our series of articles on Apache Spark, we take a look at 3."
        },
        {
            "chunk_id": 419,
            "summary": "Spatial operations can be carried out in a number of ways."
        },
        {
            "chunk_id": 420,
            "summary": "Apache Spark has been extended to support fixed-point geospatial grids."
        },
        {
            "chunk_id": 421,
            "summary": "Some of the frameworks we use for our analytics are quite accurate."
        },
        {
            "chunk_id": 422,
            "summary": "Big data analytics can be used in a number of ways."
        },
        {
            "chunk_id": 423,
            "summary": "New York City (NYC) Taxi Zone data with geometries will libraries for Apache Spark also be used as"
        },
        {
            "chunk_id": 424,
            "summary": "Apache Spark is an open-source framework for building and deploying web services."
        },
        {
            "chunk_id": 425,
            "summary": "DataFrame is a declarative framework for building web applications."
        },
        {
            "chunk_id": 426,
            "summary": "In this blog post, I'm going to show you some of the techniques we use at delta"
        },
        {
            "chunk_id": 427,
            "summary": "In this post, I will show you how to use the Databricks API to query your data."
        },
        {
            "chunk_id": 428,
            "summary": "The following documentation describes how to use the Universal DataFrames Service (UDFS) in a distributed fashion."
        },
        {
            "chunk_id": 429,
            "summary": "In this paper, we are going to look at how we can use GeoMesa to ingest data from Spark Spark"
        },
        {
            "chunk_id": 430,
            "summary": "In this tutorial, we will be using Databricks to read Geospatial data from a table."
        },
        {
            "chunk_id": 431,
            "summary": "Figure 2: Using UDFS to perform dropoff of single-node libraries."
        },
        {
            "chunk_id": 432,
            "summary": "DataFrames are used to store and manage large amounts of data."
        },
        {
            "chunk_id": 433,
            "summary": "node libraries can be wrapped in ad hoc UDFS for performing geospatial operations on DataFrames in a"
        },
        {
            "chunk_id": 434,
            "summary": "In this tutorial, we'll learn how to perform a distributed join of a single polygon against a"
        },
        {
            "chunk_id": 435,
            "summary": "This example shows how to display a map of a borough using a vector-based UDF."
        },
        {
            "chunk_id": 436,
            "summary": "New York City's Department of Sanitation (DPS) has released a map showing how the five boroughs"
        },
        {
            "chunk_id": 437,
            "summary": "longitude and latitude: Brooklyn Heights Brooklyn 73.991172790527 40.6959114074707"
        },
        {
            "chunk_id": 438,
            "summary": "All photographs courtesy of Getty Images and Reuters."
        },
        {
            "chunk_id": 439,
            "summary": "Figure 3: Using GeoMesa's provided st_bricks,"
        },
        {
            "chunk_id": 440,
            "summary": "We've been using the Open Geospatial Consortium's Unified Data Format (UDF) to"
        },
        {
            "chunk_id": 441,
            "summary": "In our series of papers on web services, we look at some of the challenges faced by developers when trying to"
        },
        {
            "chunk_id": 442,
            "summary": "The following examples show how to scale spatial operations with H3."
        },
        {
            "chunk_id": 443,
            "summary": "The first step in this paper is to define the UDF mutiPolygonfoH34."
        },
        {
            "chunk_id": 444,
            "summary": "New York City Mayor Bill de Blasio has been sworn in for a second term in office."
        },
        {
            "chunk_id": 445,
            "summary": "New York City's Department of Environmental Protection (DepEd) is using Geo-Pandas to"
        },
        {
            "chunk_id": 446,
            "summary": "var points: List[GeoCoord] = List () import com.uber.h3core."
        },
        {
            "chunk_id": 447,
            "summary": "geometry getGeometryN (0) getCoordinates () val geoTOH3 = udf"
        },
        {
            "chunk_id": 448,
            "summary": "The following is a list of all the points on a map of the UK, as compiled by the University of"
        },
        {
            "chunk_id": 449,
            "summary": "New York City's Taxi and Limousine Commission (TLC) has released the results of a two-year study"
        },
        {
            "chunk_id": 450,
            "summary": "display (df_with_borough._h3.select (\"borough\", \"pickup\", \""
        },
        {
            "chunk_id": 451,
            "summary": "The following jobs have been listed on the New York City Department of Labor's website."
        },
        {
            "chunk_id": 452,
            "summary": "All photographs courtesy of the City of New York."
        },
        {
            "chunk_id": 453,
            "summary": "A new method for performing spatial join has been developed."
        },
        {
            "chunk_id": 454,
            "summary": "There are many ways to join points and polygons."
        },
        {
            "chunk_id": 455,
            "summary": "In this blog, we will look at some of the techniques used to solve spatial problems."
        },
        {
            "chunk_id": 456,
            "summary": "Taxi drop-off locations are a great way to find a taxi to take you to your destination."
        },
        {
            "chunk_id": 457,
            "summary": "Vector data is a great way to explore the world around you."
        },
        {
            "chunk_id": 458,
            "summary": "In our series of letters, we look at some of the most common symbols for vector data."
        },
        {
            "chunk_id": 459,
            "summary": "Vector data can be stored in a variety of formats."
        },
        {
            "chunk_id": 460,
            "summary": "We have removed the field \"geom\" from the Union Lin website."
        },
        {
            "chunk_id": 461,
            "summary": "Figure 6: Geospatial of taxi drop-off locations, with latitude and longitude binned."
        },
        {
            "chunk_id": 462,
            "summary": "The following table lists all of the boreholes drilled by the Geological Survey of Scotland between"
        },
        {
            "chunk_id": 463,
            "summary": "In this tutorial we will be looking at how to use GeoJ"
        },
        {
            "chunk_id": 464,
            "summary": "In this class, we will learn how to use Spark's built-in explode function to read and write"
        },
        {
            "chunk_id": 465,
            "summary": "In this post we are going to show how to load data from the Databricks built-in reader with"
        },
        {
            "chunk_id": 466,
            "summary": "The following is a list of all the elements that make up the spark.read class."
        },
        {
            "chunk_id": 467,
            "summary": "string boro_name: string 3.8 shape_area: string 1186612476."
        },
        {
            "chunk_id": 468,
            "summary": "In our series of tutorials on how to use Apache Spark, we show how to use the software's built-"
        },
        {
            "chunk_id": 469,
            "summary": "In this tutorial, we will learn how to use the Databricks file system to view"
        },
        {
            "chunk_id": 470,
            "summary": "DBFS is a distributed storage layer, which allows code to work with data formats using familiar file"
        },
        {
            "chunk_id": 471,
            "summary": "In this paper, we are going to show you how to use the Python open(..) command to"
        },
        {
            "chunk_id": 472,
            "summary": "Here are some examples of python libraries that import Shapefile data."
        },
        {
            "chunk_id": 473,
            "summary": "shapefile is a collection of files with a common filename prefix (*.shp, *"
        },
        {
            "chunk_id": 474,
            "summary": "In this video we will show how to read shapefiles using GeoSpark."
        },
        {
            "chunk_id": 475,
            "summary": "Python, and R databricks now available."
        },
        {
            "chunk_id": 476,
            "summary": "The Spark DataFrame is a new way to store and retrieve Spark data."
        },
        {
            "chunk_id": 477,
            "summary": "Raster data is a type of Earth observation data."
        },
        {
            "chunk_id": 478,
            "summary": "The following Python example uses RasterFrames to read two bands of Landsat-8 imagery and combine"
        },
        {
            "chunk_id": 479,
            "summary": "In this project, we are collecting plant data from around New York City."
        },
        {
            "chunk_id": 480,
            "summary": "The rawdf command is used to display the results of a series of polygonal data sets."
        },
        {
            "chunk_id": 481,
            "summary": "# build an \"AI-brick\" of data for analysis."
        },
        {
            "chunk_id": 482,
            "summary": "Find out more at: www.bigbook.org"
        },
        {
            "chunk_id": 483,
            "summary": "This library supports reading the NIR (Near InfraRed) tiling format."
        },
        {
            "chunk_id": 484,
            "summary": "In this paper, we show how Scala and Spark can be used together to transform data into"
        },
        {
            "chunk_id": 485,
            "summary": "Geobricks is a free, open-source geospatial data management system."
        },
        {
            "chunk_id": 486,
            "summary": "Connecting data sources to relational databases is a growing trend."
        },
        {
            "chunk_id": 487,
            "summary": "Databricks has been working with PostGIS, the world's largest geospatial data repository, to"
        },
        {
            "chunk_id": 488,
            "summary": "Google has announced a new initiative called Google Big Data."
        },
        {
            "chunk_id": 489,
            "summary": "You will find additional details option (\"driver\"), or.poatyreglpriver?"
        },
        {
            "chunk_id": 490,
            "summary": "If you have any questions about this or any of our other publications, please feel free to contact us."
        },
        {
            "chunk_id": 491,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the"
        },
        {
            "chunk_id": 492,
            "summary": "Bitcoin and Ethereum are two of the world's most popular virtual currencies."
        },
        {
            "chunk_id": 493,
            "summary": "As part of a data engineering and analytics course at the Harvard Extension School,"
        },
        {
            "chunk_id": 494,
            "summary": "We leveraged the Databricks Lakehouse Platform to ingest unstructured data from Twitter using the Tweepy library and"
        },
        {
            "chunk_id": 495,
            "summary": "Databricks, a leading artificial intelligence (AI) firm, has announced a partnership with Yahoo Finance to"
        },
        {
            "chunk_id": 496,
            "summary": "We used Databricks to build a machine learning (ML) model for a big data analytics project."
        },
        {
            "chunk_id": 497,
            "summary": "Blockchain technology is changing the way we interact with the world around us."
        },
        {
            "chunk_id": 498,
            "summary": "Twitter has developed a high-level architecture for ingestion and ML of its crypto market data."
        },
        {
            "chunk_id": 499,
            "summary": "The following tables show how Twitter sentiment has changed over the past five years:"
        },
        {
            "chunk_id": 500,
            "summary": "Lakehouse is an artificial intelligence (AI) platform that uses machine learning (ML) and artificial intelligence"
        },
        {
            "chunk_id": 501,
            "summary": "We used Delta's lakehouse architecture to build a data pipeline for Crypto Lake."
        },
        {
            "chunk_id": 502,
            "summary": "The Big Data revolution has transformed the way we work with and use data."
        },
        {
            "chunk_id": 503,
            "summary": "As part of my master's thesis, I have been working on a data science"
        },
        {
            "chunk_id": 504,
            "summary": "Researchers at the University of California, Los Angeles (UCLA) have developed a new way to measure sentiment"
        },
        {
            "chunk_id": 505,
            "summary": "In this paper, we show how sentiment analysis can be used to"
        },
        {
            "chunk_id": 506,
            "summary": "This paper describes how we used machine learning (ML) to predict the performance of the S&P 500"
        },
        {
            "chunk_id": 507,
            "summary": "We used MLflow's Delta model registry to create a table with the highest and lowest"
        },
        {
            "chunk_id": 508,
            "summary": "Lake is an open-source machine learning platform."
        },
        {
            "chunk_id": 509,
            "summary": "This is an example of how Twitter data can be used to create a 3D model of a lake."
        },
        {
            "chunk_id": 510,
            "summary": "The table below shows the percentage of tweets sent by pupils at a secondary school in the UK in the week of"
        },
        {
            "chunk_id": 511,
            "summary": "The sentiment section provides a snapshot of the sentiment on Twitter."
        },
        {
            "chunk_id": 512,
            "summary": "Graphs showing the most common words used in tweets from the INU Transaction: make SHIBA 5 project."
        },
        {
            "chunk_id": 513,
            "summary": "BTCGasparino and BTX are two of the most popular cryptocurrencies on the Ethereum blockchain."
        },
        {
            "chunk_id": 514,
            "summary": "Figure 2: Violin plots for text length Figure 3: Word cloud for positive and negative tweets Topic Previous Top"
        },
        {
            "chunk_id": 515,
            "summary": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at"
        },
        {
            "chunk_id": 516,
            "summary": "In our project, we used SparkNLP as the NLP library of choice to classify the polarities of sentiment"
        },
        {
            "chunk_id": 517,
            "summary": "sentiment SparkM is a sentiment analysis tool for Twitter."
        },
        {
            "chunk_id": 518,
            "summary": "In this paper, we present a novel approach to sentiment analysis using classical ML algorithms."
        },
        {
            "chunk_id": 519,
            "summary": "Twitter has released an artificial intelligence (AI) tool that uses sentiment analysis to learn"
        },
        {
            "chunk_id": 520,
            "summary": "Twitter uses artificial intelligence (AI) to understand what people are saying on Twitter."
        },
        {
            "chunk_id": 521,
            "summary": "What is it and how do I use it?"
        },
        {
            "chunk_id": 522,
            "summary": "Pros: performs well in general and greatly adjust the model performance"
        },
        {
            "chunk_id": 523,
            "summary": "What are the pros and cons of deep learning?"
        },
        {
            "chunk_id": 524,
            "summary": "What are the pros and cons of machine learning?"
        },
        {
            "chunk_id": 525,
            "summary": "In this paper we present the results of a set of experiments to investigate the effects of artificial intelligence on"
        },
        {
            "chunk_id": 526,
            "summary": "In this paper, we present our results for a regression model based on sentiment and price data from the"
        },
        {
            "chunk_id": 527,
            "summary": "The aim of this paper is to find a way to predict the outcome of elections using sentiment analysis."
        },
        {
            "chunk_id": 528,
            "summary": "The sentiment score is calculated by dividing the number of positive tweets by the number of negative tweets."
        },
        {
            "chunk_id": 529,
            "summary": "In this talk we will show how to use sentiment analysis to predict the value of cryptocurrencies."
        },
        {
            "chunk_id": 530,
            "summary": "A new way to measure sentiment on Twitter."
        },
        {
            "chunk_id": 531,
            "summary": "In this study, we have used sentiment data from IMDb to train a model to predict the"
        },
        {
            "chunk_id": 532,
            "summary": "In this paper, we show how to train word embeddings and sentiment Deep Learning (DL)"
        },
        {
            "chunk_id": 533,
            "summary": "Business confidence among small and medium-sized enterprises (SMEs) in the UK has risen to its highest level since"
        },
        {
            "chunk_id": 534,
            "summary": "We used Databricks to generate buy and sell recommendations on the FTSE 100."
        },
        {
            "chunk_id": 535,
            "summary": "The report provides an in-depth analysis of the following:"
        },
        {
            "chunk_id": 536,
            "summary": "Databricks Lakehouse was used to create a single view of the company's financial performance."
        },
        {
            "chunk_id": 537,
            "summary": "How did you use Twitter sentiment analysis to find out what people were saying about you on the"
        },
        {
            "chunk_id": 538,
            "summary": "A new set of dashboards was created for the London Stock Exchange (LSE"
        },
        {
            "chunk_id": 539,
            "summary": "The following table lists the most-read and most-talked-about tweets about cryptocurrencies on Twitter"
        },
        {
            "chunk_id": 540,
            "summary": "Dogecoin and Ethereum have teamed up to create a business intelligence dashboard for Dogecoin"
        },
        {
            "chunk_id": 541,
            "summary": "The following table shows the percentage of positive and negative tweets sent by the hashtag #bitcoin in the past 24"
        },
        {
            "chunk_id": 542,
            "summary": "The team at Databricks worked closely with the BarChart team to create a lakehouse-style"
        },
        {
            "chunk_id": 543,
            "summary": "The data was collected from a variety of sources, including internal and external sources."
        },
        {
            "chunk_id": 544,
            "summary": "Databricks was selected for the project because of its strong track record of delivering high-quality, cost-"
        },
        {
            "chunk_id": 545,
            "summary": "View 2: Price Prediction to provide the user with more specific information about completed in less than 4 weeks with"
        },
        {
            "chunk_id": 546,
            "summary": "How do you use data to improve your business?"
        },
        {
            "chunk_id": 547,
            "summary": "The use of Databricks for the analysis of Twitter users' tweets and their influence on the price"
        },
        {
            "chunk_id": 548,
            "summary": "A team of researchers from the University of California, Los Angeles (UCLA) has"
        },
        {
            "chunk_id": 549,
            "summary": "This chart shows the impact of tweets on the price of cryptocurrencies."
        },
        {
            "chunk_id": 550,
            "summary": "This is a list of the most influential tweets about cryptocurrencies on Twitter."
        },
        {
            "chunk_id": 551,
            "summary": "This article is not intended to be a substitute for professional financial advice."
        },
        {
            "chunk_id": 552,
            "summary": "Twitter has become one of the world's most popular social media platforms."
        },
        {
            "chunk_id": 553,
            "summary": "There Try notebooks is a slightly negative correlation to number of retweets VS."
        },
        {
            "chunk_id": 554,
            "summary": "A look at some of the key technology stories of the week."
        },
        {
            "chunk_id": 555,
            "summary": "BBC News takes a look at some of the key stories of the week."
        },
        {
            "chunk_id": 556,
            "summary": "Comcast is one of the world's largest media and entertainment companies, but it's also one of the"
        },
        {
            "chunk_id": 557,
            "summary": "Comcast is using artificial intelligence (AI) to improve the viewing experience for its customers."
        },
        {
            "chunk_id": 558,
            "summary": "Comcast, one of the world's largest pay-TV providers, needed a new approach to"
        },
        {
            "chunk_id": 559,
            "summary": "Comcast is using Databricks to improve the viewer experience, reduce compute costs, and"
        },
        {
            "chunk_id": 560,
            "summary": "Databricks allows us to scale up and down our infrastructure according to the needs of our"
        },
        {
            "chunk_id": 561,
            "summary": "A look at some of the benefits of using the latest version of Microsoft'"
        },
        {
            "chunk_id": 562,
            "summary": "Comcast used Delta Lake to speed up the development and deployment of new business models."
        },
        {
            "chunk_id": 563,
            "summary": "Big data has the power to transform medicine."
        },
        {
            "chunk_id": 564,
            "summary": "Pharmaceutical companies have vast amounts of clinical and genomic data to sift through."
        },
        {
            "chunk_id": 565,
            "summary": "The goal of this initiative is to speed up the discovery and development of new medicines."
        },
        {
            "chunk_id": 566,
            "summary": "Researchers at the University of California, San Francisco (UCSD) and the University of California, Los Angeles (UCLA"
        },
        {
            "chunk_id": 567,
            "summary": "How did you solve the biggest data challenge of your company?"
        },
        {
            "chunk_id": 568,
            "summary": "Databricks, the leader in data analytics for the pharmaceutical and medical device industries, has been selected by Regeneron Pharmaceuticals"
        },
        {
            "chunk_id": 569,
            "summary": "is enabling everyone in our integrated drug Accelerated drug target identification: Reduced the time it takes data scientists and development process ="
        },
        {
            "chunk_id": 570,
            "summary": "Using databricks, scientists at Regeneron have been able to:"
        },
        {
            "chunk_id": 571,
            "summary": "The insurance market in the UK is set to grow at a faster rate than anywhere else in the world over the"
        },
        {
            "chunk_id": 572,
            "summary": "Nationwide Insurance is using Databricks to train deep learning models to predict the cost of workers' compensation claims"
        },
        {
            "chunk_id": 573,
            "summary": "A leading property and casualty (P&C) insurer has reduced the cost of its insurance policies by up to"
        },
        {
            "chunk_id": 574,
            "summary": "What: Nationwide Insurance uses Databricks to manage the entire analytics process from data ingestion to the deployment"
        },
        {
            "chunk_id": 575,
            "summary": "Nationwide Insurance is using Databricks to speed up the analysis of their customer data."
        },
        {
            "chunk_id": 576,
            "summary": "\"We have reduced the time it takes to train our models from approx."
        },
        {
            "chunk_id": 577,
            "summary": "A look at some of the key findings from this year's World Economic Forum annual meeting in"
        },
        {
            "chunk_id": 578,
            "summary": "Condé Nast is the world's leading media company, counting some of its iconic magazine titles in its portfolio,"
        },
        {
            "chunk_id": 579,
            "summary": "In this case study, we look at how one of the world's largest media companies"
        },
        {
            "chunk_id": 580,
            "summary": "How Condé Nast uses Databricks to ingest, store and manage terabytes of data."
        },
        {
            "chunk_id": 581,
            "summary": "Condé Nast is using artificial intelligence (AI) and machine learning (ML) to better understand"
        },
        {
            "chunk_id": 582,
            "summary": "Condé Nast has been using Databricks for machine learning (ML) and artificial intelligence (AI)"
        },
        {
            "chunk_id": 583,
            "summary": "The BBC's technology team looks at how artificial intelligence (AI) and machine learning (ML) can be used"
        },
        {
            "chunk_id": 584,
            "summary": "Watch full episodes of \"Shameless\" on SHOWTIME now:"
        },
        {
            "chunk_id": 585,
            "summary": "How to use machine learning to predict subscriber behavior and improve scheduling and programming."
        },
        {
            "chunk_id": 586,
            "summary": "Showtime Networks uses Databricks to accelerate data-driven business decisions."
        },
        {
            "chunk_id": 587,
            "summary": "What is it and how is it being used?"
        },
        {
            "chunk_id": 588,
            "summary": "Carmaker Jaguar Land Rover (JLR) has turned to Databricks to help speed up the development of new"
        },
        {
            "chunk_id": 589,
            "summary": "\"If you want to get the most out of your data, then you have to"
        },
        {
            "chunk_id": 590,
            "summary": "The Royal Dutch Shell Group is one of the world's largest independent oil and gas"
        },
        {
            "chunk_id": 591,
            "summary": "BBC News takes a look at the challenges faced by oil and gas companies when it comes to"
        },
        {
            "chunk_id": 592,
            "summary": "Databricks has announced that Royal Dutch Shell has selected the company's analytics platform to"
        },
        {
            "chunk_id": 593,
            "summary": "Here's what you might not have known:"
        },
        {
            "chunk_id": 594,
            "summary": "Databricks and Apache Spark have been working together on a big data analytics project, which has"
        },
        {
            "chunk_id": 595,
            "summary": "Shell's Daniel Jeavons explains how the oil and gas giant is using Databricks to help"
        },
        {
            "chunk_id": 596,
            "summary": "Riot Games is a leading developer and publisher of massively multiplayer online games."
        },
        {
            "chunk_id": 597,
            "summary": "Riot Games is a leading developer and publisher of video games."
        },
        {
            "chunk_id": 598,
            "summary": "What we know: Riot Games is one of the world's largest publishers of video games."
        },
        {
            "chunk_id": 599,
            "summary": "Riot Games, one of the world's largest video game publishers, is using artificial intelligence (AI) and"
        },
        {
            "chunk_id": 600,
            "summary": "Databricks is a leading provider of software for building, managing and analysing big data."
        },
        {
            "chunk_id": 601,
            "summary": "BBC News takes a look at some of the technologies that are changing the way we"
        },
        {
            "chunk_id": 602,
            "summary": "Eneco's smart energy consumption devices are in hundreds of thousands of homes across"
        },
        {
            "chunk_id": 603,
            "summary": "One of the UK's largest energy providers, SSE, is using artificial intelligence (AI)"
        },
        {
            "chunk_id": 604,
            "summary": "What: Eneco, a leading waste management company, is using Databricks to help them"
        },
        {
            "chunk_id": 605,
            "summary": "Databricks has helped Eneco to:"
        },
        {
            "chunk_id": 606,
            "summary": "Databricks, a leading provider of high-performance analytics software, has been selected by the UK's"
        },
        {
            "chunk_id": 607,
            "summary": "Eneco, one of the world's largest waste management companies, is using artificial intelligence (AI) and"
        },
        {
            "chunk_id": 608,
            "summary": "Databricks has developed a new way for homeowners to monitor and control their heating and cooling systems."
        },
        {
            "chunk_id": 609,
            "summary": "Databricks is a leading provider of data management and analytics software."
        },
        {
            "chunk_id": 610,
            "summary": "About Databricks: Databricks is a leading provider of artificial intelligence (AI) and"
        },
        {
            "chunk_id": 611,
            "summary": "Apache, Spark and the Spark logo are trademarks of the"
        }
    ]
}