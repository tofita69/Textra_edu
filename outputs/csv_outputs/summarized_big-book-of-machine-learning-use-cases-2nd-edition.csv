Chunk Number,Original Text Chunk,Summary
1,0 eBook  Big Book of Machine Learning Use Cases,Chunk 1: A selection of the best machine learning and artificial intelligence
2,"A collection of technical blogs, including code samples and notebooks  - ",Chunk 2: BBC News takes a look at some of the best
3,databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 2 Contents CHAPTER 1:,Chunk 3: BBC News NI takes a look at some of the
4,Introduction 3 CHAPTER 2: Moneyball 2.0: Improving Pitch-by-Pitch Decision-Making With MLB's Statcast Data 4,Chunk 4: Moneyball 2.0: Improving Pitch-by-Pitch
5,CHAPTER 3: Improving On-Shelf Availability for Items With Out-of-Stock Modeling 14 CHAPTER 4: Using Dynamic Time Warping and MLflow to Detect Sales Trends,"Chunk 5: In this session, you'll learn how to"
6,Part 1: Understanding Dynamic Time Warping 20 Part 2: Using Dynamic Time Warping and MLflow to Detect Sales Trends 26 CHAPTER 5:,"Chunk 6: In this series, we look at some of the"
7,"Detecting Financial Fraud at Scale With Decision Trees and MLflow on Databricks 34 CHAPTER 6: Fine-Grained Time Series Forecasting at Scale With Prophet and Apache Spark"" 45","Chunk 7: In our series of papers on artificial intelligence, we look"
8,CHAPTER 7: Applying Image Classification With PyTorch Lightning on Databricks 52 CHAPTER 8: Processing Geospatial Data at Scale With Databricks,Chunk 8: Researchers at the Massachusetts Institute of Technology (MIT)
9,63 CHAPTER 9: Exploring Twitter Sentiment and Crypto Price Correlation Using Databricks 77 CHAPTER 10:,Chunk 9: BBC News takes a look back at some of the
10,Customer Case Studies 86 databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 3,Chunk 10: BBC News takes a look at some of the key
11,"CHAPTER 1: Introduction Organizations across many industries are using machine learning to power new customer experiences, optimize business processes and improve employee productivity. From detecting financial fraud to improving the","Chunk 11: In our series of letters from African journalists, film-maker and"
12,"play-by-play decision-making for professional sports teams, this book brings together a multitude of practical use cases to get you started on your machine learning journey. The collection also serves as a guide - including code samples and notebooks - SO you can roll up your sleeves and dive into machine learning on the Databricks Lakehouse.","Chunk 12: If you want to learn how to use machine learning to make better decisions, then this is the book for you."
13,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 4 CHAPTER 2: Moneyball 2.0:,Chunk 13: BBC News NI looks at some of the key stories
14,"Introduction Improving Pitch-by-Pitch The Oakland Athletics baseball team in 2002 used data analysis and quantitative modeling to identify Decision-Making With undervalued players and create a competitive lineup on a limited budget. The book ""Moneyball,"" written",Chunk 14: This lecture is part of a series on the intersection of economics and sport.
15,"MLB's Statcast Data by Michael Lewis, highlighted the A's '02 season and gave an inside glimpse into how unique the team's strategic data modeling was for its time. Fast-forward 20 years - the use of data science and quantitative modeling is now a common practice among all sports franchises and plays a critical role in scouting, roster construction, game-day operations and season planning.","Chunk 15: In 1998, the Oakland Athletics became the first team in Major League Baseball to use data science in a major way."
16,KEY PlayerTacking PachtoPweTading PowerCable 110V,Chunk 16: BBC Sport takes a look back at some of the
17,FbreCable -r dvidual Supply Tecr nov 0 By Max Wittenberg,Chunk 17: BBC News NI takes a look at some of the
18,1oV nev Core S noV 1,Chunk 18: BBC Sport takes a look at some of the key
19,"Figure 1: Position and scope of Hawkeye cameras at ab baseball stadium databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION In 2015, Major League Baseball (MLB) introduced Statcast, a set of cameras and teams are now able to make decisions that influence the outcome of the game,","Chunk 19: Major League Baseball (MLB) is the world's most popular sport, with more than 300 million fans tuning in"
20,"radar systems installed in all 30 MLB stadiums. Statcast generates up to seven pitch by pitch. It's been 20 seasons since the A's first introduced the use of data terabytes of data during a game, capturing every imaginable data point and modeling to baseball. Here's an inside look at how professional baseball teams metric related to pitching, hitting, running and fielding, which the system collects",Chunk 20: The Oakland Athletics are the first team in Major League Baseball to use a pitch-by-pitch tracking system called Statcast.
21,"use technologies like Databricks to create the modern-day ""Moneyball"" and gain and organizes for consumption. This explosion of data has created opportunities competitive advantages that data teams provide to coaches and players on to analyze the game in real time, and with the application of machine learning, the field.",Chunk 21: Major League Baseball (MLB) has become the most data-intensive sport in the world.
22,"2020-09- T00,00.00.000-0000 2020-09- :0 0000 2020-09- BT00:00:00 +0000 2020-09- 0000",Chunk 22: BBC Sport takes a look back at some of the
23,"2020-09- TO0:00:0 2020.09-1 18T00:00:00.000 0-0000 AROAISTDAOAOAONOAN 1.8 2020-09- 18100.0000.000-0000 39.7 SI 2020-09-1 18100,00.00.000-0000 9.8",Chunk 23: Greek Prime Minister Alexis Tsipras says he will seek a
24,578428 35411 FF EARGAHISTDARAOAROAGQON A Figure 3: Sample of data collected by Statcast 6,Chunk 24: Norway's unemployment rate has fallen to its lowest
25,. A3 Figure 2: Numbers represent events during a play captured by Statcast databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION,Chunk 25: BBC Sport looks at the key statistics behind England'
26,"6 Background Data teams need to be faster than ever to provide analytics to coaches and 23 seconds, and this window of time represents a benchmark from which players SO they can make decisions as the game unfolds. The decisions made from",Chunk 26: BBC Sport takes a look at some of the key takeaways from this year's Rugby World Cup
27,Statcast data can be ingested and processed for coaches to use to make real-time analytics can dramatically change the outcome of a game and a team's decisions that can impact the outcome of the game. season. One of the more memorable examples of this was in game six of the 2020 2. Real-Time Analytics: Another competitive advantage for teams is the,Chunk 27: Real-time analytics is a competitive advantage for teams.
28,"World Series. The Tampa Bay Rays were leading the Los Angeles Dodgers 1-0 in creation of insights from their machine learning models in real time. the sixth inning when Rays pitcher Blake Snell was pulled from the mound while An example of this is knowing when to substitute out a pitcher from fatigue, pitching arguably one of the best games of his career, a decision head coach Kevin",Chunk 28: One of the most remarkable moments of this year's World Series came in the sixth inning.
29,"where a model interprets pitcher movement and data points created Cash said was made with the insights from their data analytics. The Rays went on from the pitch itself and is able to forecast deterioration of performance to lose the game and World Series. Hindsight is always 20-20, but it goes to show pitch by pitch.",Chunk 29: Tampa Bay Rays manager Kevin Cash spoke to the media on Monday about the team's use of artificial intelligence.
30,"how impactful data has become to the game. Coaching staff task their data teams with assisting them in making critical decisions - for example, should a pitcher 3. Ease of Use: Analytics teams run into problems ingesting the volumes of throw another inning or make a substitution to avoid a potential injury? Does a data Statcast produces when running data pipelines on their local computers.",Chunk 30: What are the main challenges faced by Major League Baseball's analytics teams?
31,"player have a greater probability of success stealing from first to second base, This gets even more complicated when trying to scale their pipelines to or from second to third? capture minor league data and integrate with other technologies. Teams want a collaborative, scalable analytics platform that automates data ingestion","Chunk 31: When it comes to stealing bases, baseball teams have a lot of data to work with."
32,"I have had the opportunity to work with many MLB franchises and discuss what with performance, creating the ability to impact in-game decision-making. their priorities and challenges are related to data analytics. Typically, I hear three recurring themes their data teams are focused on that have the most value in Baseball teams using Databricks have developed solutions for these priorities",Chunk 32: Major League Baseball is one of the most competitive sports in the world.
33,"helping set their team up for success on the field: and several others. They have shaped what the modern-day version of ""Moneyball"" looks like. What follows is their successful framework explained 1. Speed: Since every MLB team has access to the Statcast data during a game, in an easy-to-understand way.",Chunk 33: The Boston Red Sox are the first team in Major League Baseball to win 100 games in each of the last three seasons
34,one way to create a competitive advantage is to ingest and process the data faster than your opponent. The average length of time between pitches is databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION Getting the data,Chunk 34: Big data analytics is the art and science of gathering and analysing vast amounts of data.
35,"When a pitcher throws a baseball, Hawkeye cameras collect the data and save it collected and processed. Just as a waiter can quickly become overworked to an application that teams are able to access using an application programming fulfilling customers' needs, making continuous API requests for data creates some interface (API) owned by MLB. You can think of an API as an intermediate challenges in data pipelines. With the assistance from these data teams, however,",Chunk 35: Major League Baseball's (MLB) Hawkeye camera system is one of the most advanced in the game.
36,"connection between two computers to exchange information. The way this works we have created code to accommodate continuously collecting Statcast data is: a user sends a request to an API, the API confirms that the user has permission during a game. You can see an example of the code using a test API below. to access the data and then sends back the requested data for the user to",Chunk 36: We have created an API to allow you to access Statcast data during a game.
37,"consume. To use a restaurant as an analogy - a customer tells a waiter what they from pathlib import Path import json want to eat, the waiter informs the kitchen what the customer wants to eat, the waiter serves the food to the customer. The waiter in this scenario is the API.",Chunk 37: The API is a way for an application to communicate with another application.
38,"class def sports_api: init_(self, endpoint, api_key) : self.endpoint = endpoint self.api_key = api_key self.connection = self.endpoint + self.api_key API",Chunk 38: class def sports_api: init_(
39,"User Application def fetch_payload (self, request_ L, request_2, adls_path) url = Flaaif.comnectionlsse d= (reques 1(request_2)-",Chunk 39: The following example shows how to fetch adls
40,"Request Request 99.M"" r = requests. get (url) Response",Chunk 40: The following is a list of all the requests made
41,"Response json_data - r.json () now = time. .strftime CaYamid-HR.MIS? file_name = f'json_data_out, (now)"" file_path = Path (""dbfs:/"") Path (adls_path) / Path (file_name)",Chunk 41: The following lines from the CaYamid-HR
42,"dbutils.fs.put (str (file_path), json.dumps (json_data), True) Figure 4: Example of how an API works, using a restaurant analogy return str (file_path) Figure 5: Interacting with an API to retrieve and save data This simple method of retrieving data is called a ""batch"" style of data collection",Chunk 42: How do you use an API to retrieve and save data?
43,"and processing, where data is gathered and processed once. As noted earlier, This code decouples the steps of getting data from the API and transforming however, data is typically available through the API every 23 seconds (the average it into usable information, which in the past, we have seen, can cause latency time between pitches). This means data teams need to make continuous requests in data pipelines. Using this code, the Statcast data is saved as a file to cloud","Chunk 43: This code decouples the steps of getting data from the API and transforming it into usable information, which in the past, we have seen, can cause latency time between pitches."
44,"to the API in a method known as ""streaming"" where data is continuously storage automatically and efficiently. The next step is to ingest it for processing. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 8",Chunk 44: A new way of storing data has been developed by Google for its Android operating system.
45,"Automatically load data with Auto Loader As pitch and play data is continuously saved to cloud storage, it can be ingested automatically using a Databricks feature called Auto Loader. Auto Loader scans CSV JSON",Chunk 45: Databricks has released a new version of its award-winning pitch and
46,"files in the location they are saved in cloud storage and loads the data into A C D ""Employee"": [ ID Gender City Monthly.",Chunk 46: The BBC's technology team looks at some of
47,Databricks where data teams begin to transform it for their analytics. Auto Loader 2 ID000002(Female Delhi 20000 3 ID000004E Male Mumbai 35000 t is easy to use and incredibly reliable when scaling to ingest larger volumes of data,Chunk 47: Autobrick 20000 3 ID4E Male Mumbai 35000 t is a high
48,"4 ID000007H Male Panchkula 22500 5 ID0000081Male Saharsa 35000 ""id"":""1"", in batch and streaming scenarios. In other words, Auto Loader works just as well 6 ID0000091N Male Bengaluru 100000",Chunk 48: Here's a look at some of the key words that
49,"7 ID000010KMale Bengaluru 45000 for small and large data sizes in batch and streaming scenarios. The Python code 8 ID000011LF Female Sindhudui 70000 ""Name"": ""Ankit"", 9 ID000012NN Male Bengaluru 20000",Chunk 49: Python code 8 ID000011LF Female Sindhudui 70000 for
50,"below shows how to use Auto Loader for streaming data. 10 ID000013NMale Kochi 75000 ""Sal"": ""1000"", 11 1D000014CFemale Mumbai 30000 12 ID000016CMale Mumbai 25000",Chunk 50: You can watch INRDeals INRDeals INRDeals INRDeals INRDeals INRDeals INRDeals
51,"df = spark.readtream. format (""cloudFiles"") I 3 1D000018SFemale Surat 25000 ), f option (,) 14 ID000019TF Female Pune 24000",Chunk 51: df = spark.readtream.
52,"schema () I 15 ID000021VMale Bhubanes 27000 ID000022VFemale Howrah 28000 ""id"":""2"", .load()","Chunk 52: Images courtesy of AFP, EPA, Getty Images and"
53,"""Name"": ""Faizv"". df.writestream. format (""delta"") I option rehechpointlocation? ) Figure 7: Comparison of CSV and JSON formats trigger () 1",Chunk 53: Figure 7: Comparison of two different file formats.
54,".start () It should be obvious which of these two formats data teams prefer to work Figure 6: Setup of Auto Loader to stream data with. The goal then is to load Statcast data in the JSON format and transform it into the friendlier CSV format. To do this, we can use the semi-structured data",Chunk 54: In this article we are going to look at how to load and stream Statcast data.
55,"One challenge in this process is working with the file format in which the Statcast support available in Databricks, where basic syntax allows us to extract and is saved, a format called JSON. We are typically privileged to work with data that is transform the nested data you see in the JSON format to the structured CSV style already in a structured format, such as the CSV file type, where data is organized format. Combining the functionality of Auto Loader and the simplicity of semi-",Chunk 55: In this article we are going to look at how we can use the Auto Loader feature in Datas to transform data into a structured format.
56,"in columns and rows. The JSON format organizes data into arrays and despite its structured data support creates a powerful data ingestion method that makes the wide use and adoption, I still find it difficult to work with, especially in large sizes. transformation of JSON data easy. Here's a comparison of data saved in a CSV format and a JSON format.",Chunk 56: I'm a big fan of the SQL Server hibernate format.
57,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 9 Using Databricks' semi-structured data support with Auto Loader Auto Loader writing data to a Delta table as a stream,Chunk 57: This paper describes the use of Autobricks in the development of
58,"spark.readstream. format (""cloudFiles"" I # Define the schema and the input, checkpoint, and output pai ths. .option reloudflles.format, ""json"") 1 read_schema = (""id int, .option ""cloudriles.schematocation"", 1",Chunk 58: The spark.readstream library provides a way to stream
59,"""firstName string, .load ("""") I ""middleName string, .selectExpr ""lastName string, +","Chunk 59: string ""firstName string, .load ("""
60,"""gender string, + ""tags:page.name"", # extracts (""tags"": ""page"": ""name' 113 ""birthDate timestamp, ""taga-page.idtint"", # extracts tagaraCpagercian 133 and ""ssn string,","Chunk 60: extract ""tagaraCpagercian 133"""
61,"casts to int ""salary int"") ""tagsieventrype"" # extracts tagarteventrype? 13 json_read.path = /FileStore/streaming-uploads/people-10m' checkpoint_path - /mt/delta/people-10n/checepoints",Chunk 61: BBC Sport takes a look back at some of the
62,"save_path = me/daltapeople-o people_stream (spark I readStream I As the data is loaded in, we save it to a Delta table to start working with it further. .schema (read_schema) I",Chunk 62: The following code shows how to load data into a Delta table from
63,".option Charleaerrigger, 1) I Delta Lake is an open format storage layer that brings reliability, security and .option (""multiline', True) performance to a data lake for both streaming and batch processing and is the -json Gson_read.path))","Chunk 63: .option Charleaerrigger, 1) I Delta Lake is an open format"
64,"foundation of a cost-effective, highly scalable data platform. Semi-structured people_stream."" writeStream I support with Delta allows you to retain some of the nested data if needed. The .format( ('delta') I outputMode ('append') I","Chunk 64: Delta is an open-source, declarative data management platform."
65,"syntax allows flexibility to maintain nested data objects as a column within a Delta option checkpointlocation, checkpoint_path) I table without the need to flatten out all of the JSON data. Baseball analytics teams .start (save_path) use Delta to version Statcast data and enforce specific needs to run their analytics","Chunk 65: syntax allows flexibility to maintain nested data objects as a column within a Delta option checkpointlocation, checkpoint_path)"
66,"on while organizing it in a friendly structured format. With Auto Loader continuously streaming in data after each pitch, semi-structured data support transforming it into a consumable format, and Delta Lake organizing it for use, data teams are now ready to build analytics that gives their team the competitive edge on the field.",Chunk 66: Delta Lake is a game-changing data management platform for youth baseball.
67,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 10 Machine learning for insights Recall the Rays pulling Blake Snell from the mound during the World Series = that,Chunk 67: BBC Sport takes a look at some of the key stories from the
68,"Machine learning models are relatively easy to build and use, but data teams often decision came from insights coaches saw in their predictive models. Statistical struggle to implement them into streaming use cases. Add in the complexity of analysis of Snell's historical Statcast data provided by Billy Heylen of sportingnews. how models are managed and stored and machine learning can quickly become com indicated Snell had not pitched more than six innings since July 2019, had a",Chunk 68: How do you use machine learning to predict the performance of a pitcher?
69,"out of reach. Fortunately, data teams use MLflow to manage their machine learning lower probability of striking out a batter when facing them for the third time in a models and implement them into their data pipelines. MLflow is an open source game, and was being relieved by teammate Nick Anderson, whose own pitch data platform for managing the end-to-end machine learning lifecycle and includes","Chunk 69: MLflow is an open source game, and was being relieved by teammate Nick Anderson, whose own pitch data platform for managing the end-to-end"
70,"suggests was one the strongest closers in MLB, with a 0.55 earned run average support for tracking predictive results, a model registry for centralizing models that (ERA) and 0.49 walks and hits per innings pitched (WHIP) during the 19 regular- are in use and others in development, and a serving capability for using models in season games he pitched in 2020. Predictive models analyze data like this in real",Chunk 70: We've been looking at some of the best relievers in Major League Baseball over the past five years.
71,data pipelines. time and provide supporting evidence and recommendations coaches use to make critical decisions. MLflow Tracking MLflow Projects,Chunk 71: MLflow is a cloud-based artificial intelligence (
72,MLflow Models Model Registry Record and query Package data science code Deploy machine learning,Chunk 72: MLflow Models Model Registry Record and query Package data
73,"Store, annotate, discover, experiments: code, data, in a format to reproduce runs models in diverse serving and manage models in a config, and results",Chunk 73: In this talk I will show you how to build
74,on any platform environments central repository Read more Read more,Chunk 74: BBC News takes a look at some of the key
75,Read more Read more Figure 8: MLflow overview databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION,Chunk 75: BBC News takes a look at some of the key
76,"11 To implement machine learning algorithms and models to real-time use cases, The outputs a machine learning model creates can then be displayed in a data data teams use the model registry where a model is able to read data sitting in visualization or dashboard and used as printouts or shared on a tablet during a",Chunk 76: Machine learning is a branch of computer science that uses computers to learn from large amounts of data.
77,a Delta table and create predictions that are then used during the game. Here's game. MLB franchises working on Databricks are developing fascinating use cases an example of how to use a machine learning model while data is automatically that are being used during games throughout the season. Predictive models are loaded with Auto Loader:,Chunk 77: If you're a fan of Major League Baseball (MLB) and want to see how data can be used to predict the
78,"proprietary to the individual teams, but here's an actual use case running on Databricks that demonstrates the power of real-time analytics in baseball. Getting a machine learning model from the registry and using it with Auto Loader #get model from the model registry",Chunk 78: We've been hearing a lot lately about the use of machine learning in baseball.
79,"model = mlflow.spark. load_model ( model_uri ""model (moo name ""Production')""? #read data from bronze table as a stream events = spark.readtream I .format (""delta"") I",Chunk 79: #read data from bronze table as a stream events
80,"#.option reloudFies.marilesPerrigger"", 1)1 .schema (schema) I .table Chaseatre7 #pass stream through model model_output model.transform: (events)",Chunk 80: Find out more at www.marilesPerrigg
81,"#write stream to silver delta table events.writestrean I .format (""delta') I .outputMode (""append"") I option('checkpointlocation', ""/tmp/baseball/""? I",Chunk 81: #writeformat to silver delta table events.
82,table Cdefault.baseball.treamsilver? databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 12 Bringing it all together with spin ratios and sticky stuff,Chunk 82: The following table shows the percentage of students who scored
83,"Average adjusted fastball spin rate per season MLB introduced a new rule for the 2021 season meant to discourage pitcher's use 20 21 22 23 24 25 of ""sticky stuff,"" a substance hidden in mitts, belts or hats that when applied to a 2017",Chunk 83: Here's a look back at some of the rules changes made by Major League Baseball over the past
84,"baseball can dramatically increase the spin ratio of a pitch, making it difficult for 2018 batters to hit. The rule suspends for 10 games pitchers discovered using sticky stuff. Coaches on opposing teams have the ability to request an umpire check for 2019",Chunk 84: Umpires will be able to check on sticky stuff on pitchers' arms for the first
85,"the substance if they suspect a pitcher to be using it during a game. Spin ratio is a 2020 data point that is captured by Hawkeye cameras, and with real-time analytics and 2021 machine learning, teams are now able to make justified requests to umpires with",Chunk 85: Umpires will be able to use artificial intelligence to determine if pitchers are using performance-enhancing drugs
86,the hopes of catching a pitcher using the material. After June 3 Source: Baseball Prospectus How spin affects a pitch Direction of pitch,Chunk 86: Major League Baseball has announced that it will begin using
87,"Figure 10: Trending spin rate of fastballs per season and after rule introduction on June 3, 2021 The ball is pushed in Following the same framework outlined above, we ingest Statcast data pitch by the direction of the spin,",Chunk 87: Here's a look at how the spin rate of fastballs will change under the
88,"pitch and have a dashboard that tracks the spin ratio of the ball for all pitchers Friction pulls air making it harder to hit. during all MLB games. Using machine learning models, predictions are sent to the around the ball. 334",Chunk 88: Major League Baseball is using artificial intelligence to predict the outcome of games.
89,"dashboard that flag outliers against historical data and the pitcher's performance in the active game, which can alert coaches when they fall outside of ranges anticipated by the model. With Auto Loader, Delta Lake and MLflow, all data ingestion and analytics happen in real time. Figure 9: Illustration of how spin affects a pitch",Chunk 89: We're using artificial intelligence (AI) and machine learning (ML) to improve pitchers' performance.
90,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 13 Technologies like Statcast and Databricks have brought real-time analytics to Lovelytics Defense Analysis: Pitch Spin Rate,"Chunk 90: In this week's Tech Tent, we take a look"
91,"sports and changed the paradigm of what it means to be a data-driven team. As data volumes continue to grow, having the right architecture in place to capture 24.63  MW ) -",Chunk 91: The world's largest independent power producer (IPP) has announced it
92,real-time insights will be critical to staying one step ahead of the competition. Mwyw MV wwww Real-time architectures will be increasingly important as teams acquire and Historical Trend,Chunk 92: Real-time analytics will be increasingly important as teams acquire and
93,"2017/20181 12019j202012021 GameTrend ThisS Seas develop players, plan for the season and develop an analytically enhanced approach to their franchise. Ask about our Solution Accelerator with Databricks 25.50",Chunk 93: ThisS Seas are a professional rugby union team based in the
94,"partner Lovelytics, which provides sports teams with all the resources they need 25.0 to quickly create use cases like the ones described in this blog. te 10.6496",Chunk 94: BBC Sport takes a look at some of the use cases
95,A11516 A0.6296 V21185 V25516 V2976,Chunk 95: A11516 A0.6296 A21185 V
96,"V31186 A03316 Y0486 2463 V2615 Figure 11: Dashboardi for ""sticky stuff"" detection in real time",Chunk 96: V31186 A03316 Y0486 2
97,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 14 CHAPTER 3: Improving On-Shelf,Chunk 97: A look at some of the key findings from the
98,"Introduction Availability for Items With Retailers are missing out on nearly $1 trillion in global sales because they don't have on hand what AIOut-of-Stock Modeling customers want to buy in their stores. Adding to the challenge, a study of 600 households and several","Chunk 98: In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at the"
99,"retailers by research firm IHL Group details that shoppers encounter out-of-stocks (0OS) as often as This post was written in collaboration with Databricks partner one in three shopping trips, according to the report. And a IRI found that 20% of all out-of- Tredence. We thank Rich Williams, Vice President Data","Chunk 99: One in three shoppers encounter out-of-stocks as often as they go shopping, according to a new report"
100,"study by Engineering, and Morgan Seybert, Chief Business Officer, of stocks remain unresolved for more than 3 days. Tredencei for their contributions. Overall, studies show that the average OOS rate is about 8%. That means that one out of 13 products","Chunk 100: The UK's oil and gas industry is suffering from a lack of innovation, according to"
101,"is not purchasable at the exact moment the customer wants to get it in the store. OOS is one of the biggest problems in retail, but thankfully it can be solved with real-time data and analytics. In this write-up, we showcase the new Tredence-Databricks combined On-Shelf Availability Solution Accelerator. The accelerator is a robust quick-start guide that is the foundation for a full out-of-stock or supply chain solution. We outline how to approach out-of-stocks with the Databricks Lakehouse to",Chunk 101: Out-of-stock (OOS) is one of the biggest challenges faced by retailers.
102,"solve for on-shelf availability in real time. By Rich Williams, Morgan Seybert, Rob Saker and Bryan Smith And the impact of solving this problem? A 2% improvement in on-shelf availability is worth 1% in increased sales for retailers.",Chunk 102: How do you solve one of the biggest challenges facing retailers - how do you improve the
103,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 15 Growth in e-commerce makes item availability more important,Chunk 103: A look at some of the key findings from the
104,"The significance of this problem has been amplified by the availability of e-commerce for delivery and curbside pickup orders. While customers that face an out-of-stock at the store level may just not purchase that item, they are likely to purchase other items in the store. Buying online means that they may just switch to a different retailer.",Chunk 104: The issue of out-of-stock goods has been a problem for retailers for some time.
105,"The impact is not just limited to a bottom line loss in revenue. Research from NielsenIQ shows that 30% of shoppers will visit new stores when they can't find the product they are looking for, leading to a loss in long-term loyalty. Members of e-commerce membership programs are most likely to switch retailers in the event of an out-of-stock. IHL estimates that ""upwards of 24% of Amazon's current retail","Chunk 105: According to the International Council of Shopping Centers (IHL), out-of-stock stores cost US retailers more than $100bn a year."
106,"revenue comes from customers who first tried to buy the product in-store."" Retailers have responded to this with a variety of tactics including over-ordering of items, which increases carrying costs and lowers margins when they are forced to sell excess inventory at a discount. In some instances, retailers and distributors It's not just retailers that are impacted by OOS. Retailers, consumer goods will rush order products or use intra-delivery ""hot shots"" for additional deliveries,",Chunk 106: Online Ordering Syndrome (OOS) is a growing problem in the retail industry.
107,"companies, distributors, brokers and other firms each invest in third-party audits, which come at an additional cost. Some retailers have invested in robotics, but which typically involve employees visiting stores to identify gaps on the shelf. On many pull out of their pilots citing costs. And other retailers are experimenting with any given day, tens of thousands of individuals are visiting stores to validate item computer vision, although these approaches merely notify them when an item is",Chunk 107: The use of artificial intelligence (AI) and robotics in the retail industry has led to the emergence of a growing number of third-party audits.
108,availability. Is this really the best use of time and resources? unavailable and don't predict item availability. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 16,Chunk 108: What is the best use of time and resources?
109,"Why hasn't technology solved out-of-stocks yet? Introducing the On-Shelf Availability Solution Accelerator Out-of-stock issues have been around for decades, SO why hasn't the retail Our partners at Tredence approached us with the idea of publishing a Solution industry been able to solve an issue of this magnitude that impacts shoppers,","Chunk 109: Out-of-stock issues have been around for decades, so why hasn't the retail industry been able to"
110,Accelerator that they've created as the core of a broader Supply Chain Control retailers and brands alike? The seemingly simple solution is to require employees Tower offering. Tredence works with the largest retailers on the planet and to manually count the items on hand. But with potentially hundreds of thousands understands the nuances of modeling OOS and knew that Databricks' processing,Chunk 110: What does Databricks have in common with Tredence?
111,"of individual SKUS distributed across a large format retail location that may be and their advanced data science capabilities were a winning combination. servicing customers nearly 24 hours a day, this simply isn't a realistic task to perform on a regular basis. While the OSA solution focuses on driving sales through improved stock availability",Chunk 111: One of the UK's largest supermarket chains was looking for a way to improve stock availability.
112,"on the shelves, the broader Retail Supply Chain Control Tower solves for multiple Individual stores do perform inventory counts periodically and then rely on point- adjacent merchandising problems - inventory design for the stores, efficient store of-sale (POS) and inventory management software to track changes that drive unit replenishments, design of store network for omnichannel operations, etc. Knowing counts up and down. But with sO much activity within a store location, some of the how big a problem this is in retail, we immediately took them up on their offer.",Chunk 112: We were looking for a solution to a problem we were having with our supply chain.
113,"day-to-day recordkeeping falls through the cracks, not to mention the impact of shrinkage, which can be hard to detect, on in-store supplies. The first step in addressing OSA challenges is to examine their occurrence in the historical data. Past occurrences point to systemic issues with suppliers and So the industry falls back on modeling. But given fundamental problems in data",Chunk 113: One of the biggest challenges faced by retailers is the lack of visibility into their supply chain.
114,"internal processes, which will continue to cause problems if not addressed. accuracy, these approaches can drive a combination of false positives and false negatives that make model predictions difficult to employ. Time sensitivities To support this analysis, Tredence made available a set of historical inventory and further exacerbate the problem, as the large volume of data that often must be","Chunk 114: In our series of papers on artificial intelligence, we look at how artificial intelligence (AI) can be used to improve the accuracy of machine learning"
115,"sales data. These data sets were simulated, given the obvious sensitivities any crunched in order to arrive at model predictions must be handled fast enough for retailer would have around this information, but were created in a manner that the results to be actionable. The problem of building a reliable system for stockout frequently observed OSA challenges manifested in the data. These challenges were: prediction and alerting is not as straightforward as it might appear. 1. Phantom inventory",Chunk 115: This paper describes the challenges of building a reliable system for stockout frequently observed OSA challenges manifested in the data.
116,3. Zero-sales events 2. Safety stock violations 4. On-shelf availability databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION,Chunk 116: The BBC News website looks at some of the key
117,"17 Phantom inventory Safety stock violations In a phantom inventory scenario, the units reported to be on hand do not align Most organizations establish a threshold for a given product's inventory, below","Chunk 117: As part of our series on phantom inventory, we look at 17 examples"
118,"with units expected based on reported sales and replenishment. which replenishment orders are triggered. If set too low, inadequate lead times or even minor disruptions to the supply chain may lead to an out-of-stock scenario 160 Alert",Chunk 118: The following table shows the estimated lead times for key products sold by each of the
119,while new units are moving through the replenishment pipeline. 140 20 120 Alert,Chunk 119: The world's largest coal-fired power station
120,- 5 - 1111 1-May 2-May 3-May 4-May 5-May 6May 7-May 8-May 9-May 10-May 11-May 12-May 13-May 14-May 15-May -Replenished Units -0-On-Handl Inventory -0-Sales Units 4 Phantoml Inventory,Chunk 120: Here's a look at the key statistics for the first
121,"1-May 2-May 3-May 4-May S-May 6-May 7-May 8-May 9-May 10-May 11-May 12-May 13-May 14-May 15-May Figure 1: The misalignment of reported inventory, with inventory expected based on sales and LSafety stock -Replenishedu Units -0-On-Handi Inventory replenishment, creating phantom inventory Figure 2: Safety stock levels not providing adequate lead time to prevent out-of-stocki issues",Chunk 121: The following is a breakdown of LSafety's financial results for the first quarter of 2017.
122,"Poor tracking of replenishment units, unreported or undetected shrinkage, and out-of-band processes coupled with infrequent and sometimes inaccurate The flip side of this is that if set too high, retailers risk overstocking products that inventory counts create a situation where retailers believe they have more may expire, risk damage or theft, or otherwise consume space and capital that",Chunk 122: One of the biggest challenges faced by retailers is the lack of visibility into their inventory.
123,"units on hand than they actually do. If large enough, this phantom inventory may be better employed in other areas. Finding the right safety stock level for a may delay or even prevent the ordering of replenishment units, leading to an product in a specific location is a critical task for effective inventory management. out-of-stock scenario.",Chunk 123: The use of phantom inventory is a common problem in the manufacturing industry.
124,databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 18 Zero-sales events On-shelf availability,Chunk 124: BBC Sport takes a look at some of the key
125,"Phantom inventory and safety stock violations are the two most common While understanding scenarios in which items are not in stock is critical, it's causes of out-of-stocks. Regardless of the cause, out-of-stock events manifest equally important to recognize when products are technically available for sale themselves in periods when no units of a product are sold.",Chunk 125: Out-of-stock events are a major problem for manufacturers.
126,"but underperforming because of non-optimal inventory management practices. These merchandising problems may be due to poor placement of displays within Not every occurrence of a zero-sales event reflects an out-of-stock concern. the store, the stocking of products deep within a shelf, the slow transfer of product Some products don't sell every day, and for some slow-moving products,",Chunk 126: A zero-sales event is when a store does not sell any products for a period of time.
127,"from the backroom to shelves, or a myriad of other scenarios in which inventory multiple days may go by within which zero units are sold while the product is adequate to meet demand but customers cannot easily view or access it. remains adequately stocked. Alert",Chunk 127: There are many ways in which a company can run out of stock.
128,.35 .25 5 E 0.05,Chunk 128: The winning numbers in Saturday evening's drawing of
129,"3-May 4-May 5.May 6-Ma 9-May 10-May 11-May 12-May 13-May 14- May - Threshold (Probability) 0-SalesUnits Cumulative Probablity vofZeroSales Alert, ActualSales Expecteds Sales Units ReplenishmentUnits Deviation! belowt threshold Deviationa abovet threshold","Chunk 129: Reports from the world's biggest banks, as compiled by Reuters"
130,Figure 3: Examining the cumulative probability of consecutive: zero-sales events to identify potential OSAC alertsb basedon deviationo observedb betweenActualond: ExpectedSalesUnits out-of-stocki issues Figure 4: Depressed sales due to poor product placement leading to an on-shelf availability problem The trick for scrutinizing zero-sales events at the item level is to understand the,Chunk 130: Figure 3: Examining the cumulative probability of consecutive: zero-sales events to identify potential OSAC alerts Figure 4: De
131,"probability of which at least one unit of a product sells on a given day and to then To detect these kinds of problems, it is helpful to compare actual sales to set a cumulative probability threshold for consecutive days reflecting zero-sales. those forecasted for the period. While not every missed sales goal indicates When the cumulative probability of back-to-back zero-sales events exceeds the an on-shelf availability problem, a sustained miss might signal a problem that",Chunk 131: A zero-sales event is an on-shelf availability problem.
132,"threshold, it's time for the inventory of that product to be examined. requires further attention. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 19",Chunk 132: The time it takes for a product to reach a certain
133,How we approach out-of-stocks with the Databricks forecast for the business's needs. Scalable patterns ensure data science tasks Lakehouse Platform are also tackled in an efficient and timely manner with little deviation from the standard approaches data scientists typically employ.,Chunk 133: How Databricks and Lakehouse Platform work together.
134,"The evaluation of phantom inventories, safety stock violations, zero-sales events and on-shelf availability problems requires a platform capable of performing a And the SQL Analytics interface, as well as robust integrations with Tableau and wide range of tasks. Inventory and sales data must be aggregated and reconciled Power BI, allows analysts to consume the results of the data scientists' and data",Chunk 134: Power BI is an analytics platform that can be used by data scientists and analysts to monitor and report on the performance of their warehouse operations.
135,at a per-period level. Complex logic must be applied across these data to examine engineers' work without having to first port the data to alternative platforms. aggregate and series patterns. Forecasts may need to be generated for a wide range of products across numerous locations. And the results of all this work Getting started must be made accessible to the business analysts responsible for scrutinizing the,"Chunk 135: In the oil and gas industry, data is king."
136,"findings before soliciting action from those in the field. Be sure to check out and download the notebooks for out-of-stock modeling. As with any of our Solution Accelerators, these are a foundation for a full solution. Databricks provides a single platform capable of all this work. The elastic If you would like help with implementing a full out-of-stock or supply chain","Chunk 136: In our latest Solution Accelerator, we look at out-of-stock and supply chain management."
137,"scalability of the platform ensures that the processing of large volumes of solution, go visit our friends at Tredence. data can be performed in an efficient and timely manner. The flexibility of its development environment allows data engineers to pivot between common To see these features in action, please check out the following notebooks",Chunk 137: The Tredence Data Platform is designed to meet the needs of data scientists working with large volumes of structured and unstructured data.
138,"languages, such as SQL and Python, to perform data analysis in a variety of modes. demonstrating how Tredence tackled out-of-stocks on the Databricks platform: Pre-integrated libraries provide support for classic time series forecasting algorithms and techniques, and easy programmatic installations of alternative OSA 1: Data Preparation e libraries such as Facebook Prophet allow data scientists to deliver the right","Chunk 138: Databricks has released a new version of its time series library, designed to make it easier for data scientists to extract insights from large"
139,OSA 2: Out-of-Stocks e OSA 3: On-Shelf Availability e databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 20,Chunk 139: Research and Markets ( www.researchandmarkets
140,"CHAPTER 4: Using Dynamic Time Introduction Warping and MLflow to The phrase ""dynamic time warping,"" at first read, might evoke images of Marty McFly driving his DeLorean at","Chunk 140: In our series of letters from African journalists, film-maker"
141,"Detect Sales Trends 88 MPH in the ""Back to the Future"" series. Alas, dynamic time warping does not involve time travel; instead, it's a technique used to dynamically compare time series data when the time indices between comparison Part 1of our Using Dynamic Time Warping data points do not sync up perfectly.","Chunk 141: In the second part of our Using Dynamic Time Warping series, we look at how dynamic time warping can be used to"
142,"and MLflow to Detect Sales Trends series As we'll explore below, one of the most salient uses of dynamic time warping is in speech recognition - - determining whether one phrase matches another, even if the phrase is spoken faster or slower than its comparison. You can imagine that this comes in handy to identify the ""wake words"" used to activate your Google Home or Amazon Alexa device - even if your speech is slow because you haven't yet had your daily",Chunk 142: Dynamic time warping is one of the most interesting ways that artificial intelligence (AI) and machine learning (ML) can be used.
143,"cup(s) of coffee. Dynamic time warping is a useful, powerful technique that can be applied across many different domains. Once you understand the concept of dynamic time warping, it's easy to see examples of its applications in daily life, and its exciting future applications. Consider the following uses: By Ricardo Portilla, Brenner Heintz",Chunk 143: Dynamic time warping is a powerful technique that can be applied across many different domains.
144,"Financial markets: comparing stock trading data over similar time frames, even if they do not match and Denny Lee up perfectly. For example, comparing monthly trading data for February (28 days) and March (31 days). Try this notebook in Databricks * Wearable fitness trackers: more accurately calculating a walker's speed and the number of steps,",Chunk 144: Find out more at databricks.com
145,"even if their speed varied over time Route calculation: calculating more accurate information about a driver's ETA, if we know something about their driving habits (for example, they drive quickly on straightaways but take more time than average to make left turns) Data scientists, data analysts and anyone working with time series data should become familiar with this",Chunk 145: How do we know if a driver is speeding?
146,"technique, given that perfectly aligned time series comparison data can be as rare to see in the wild as perfectly ""tidy"" data. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 21",Chunk 146: How do you use time series data to improve your teaching?
147,"In this blog series, we will explore: The basic principles of dynamic time warping Running dynamic time warping on sample audio data Running dynamic time warping on sample sales data using MLflow Dynamic time warping",Chunk 147: Dynamic time warping is one of the most common techniques used to manipulate data in
148,The objective of time series comparison methods is to produce a distance metric between two input time series. The similarity or dissimilarity of two time series is typically calculated by converting the data into vectors and calculating the EUCLIDEAN MATCHING Euclidean distance between those points in vector space.,Chunk 148: This paper presents a time series comparison method based on the EUCLIDEAN MATCHING Euclidean distance between two
149,"Dynamic time warping is a seminal time series comparison technique that has been used for speech and word recognition since the 1970s with sound waves as the source; an often cited paper is ""Dynamic time warping for isolated word recognition based on ordered graph searching techniques."" Background","Chunk 149: In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the"
150,"This technique can be used not only for pattern matching, but also anomaly Source: Wikimedia Commons detection (e.g, overlap time series between two disjoint time periods to DYNAMIC TIME WARP MATCHING File: Euclidean.v.DTW.pe",Chunk 150: The time series between two disjoint time periods can be detected using a
151,"understand if the shape has changed significantly, or to examine outliers). For example, when looking at the red and blue lines in the following graph, note the Two time series (the base time series and new time series) are considered similar traditional time series matching (i.e., Euclidean matching) is extremely restrictive. when it is possible to map with function f(x) according to the following rules SO as","Chunk 151: When looking at a graph, it is useful to understand the relationship between time series and shape (i.e."
152,"On the other hand, dynamic time warping allows the two curves to match up to match the magnitudes using an optimal (warping) path. evenly even though the X-axes (ie., time) are not necessarily in sync. Another way f(x) maps to f(x) when i < =j to think of this is as a robust dissimilarity score where a lower number means the","Chunk 152: When the X-axes are not in sync, the two curves do not match up."
153,series is more similar. f(x) maps to f(x) only when (j - i) is within fixed range databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 22,Chunk 153: The following table lists the most popular books published in the UK
154,"Sound pattern matching Below are visualizations using matplotlib of the four audio clips: Traditionally, dynamic time warping is applied to audio clips to determine the Clip 1: This is our base time series based on the quote ""Doors and corners, similarity of those clips. For our example, we will use four different audio clips","Chunk 154: In this lesson, we will use dynamic time warping to determine the time series of sound clips."
155,"kid. That's where they get you."" based on two different quotes from a TV show called The Expanse. There are four audio clips (you can listen to them below, but this is not necessary) = three of Clip 2: This is a new time series [v2] based on clip 1 where the intonation and them (clips 1, 2 and 4) are based on the quote",Chunk 155: This is a new time series where the intonation and the words are based on the quote:
156,"speech pattern are extremely exaggerated Clip 3: This is another time series that's based on the quote ""You walk into a ""Doors and corners, kid. That's where they get you."" room too fast, the room eats you."" with the same intonation and speed as clip 1 And in one clip (clip 3) is the quote","Chunk 156: This time series is based on the quote ""Doors and corners, kid."""
157,"Clip 4: This is a new time series [v3] based on clip 1 where the intonation and speech pattern is similar to clip 1 ""You walk into a room too fast, the room eats you."" Clip1 Doors and corners, kid. Clip 2 Doors and",Chunk 157: This is a new time series based on clip 1 where the intonation and speech pattern is similar to clip
158,"kid. 1 corners, Clip1 Doors and corners, kid. Clip 2 Doors and corners, kid.","Chunk 158: Clip 1 Doors and corners, Clip 2 Doors and"
159,That's where they get you. M] That's where they get you. [v2] That's where they get you. [v] That's where they get you. [v2] 0:00/0:06,Chunk 159: Match of the Day presenter Gary Lineker explains why
160,"4 : 0:00/0:08 4) : Clip 3 You walk into a room too fast, Clip 4 Doors and corners, kid.","Chunk 160: Watch the latest news headlines from BBC News, including"
161,the room eats you. That's where they get you. [v3] 0:00/0:07 4) : 0:00/0:07,Chunk 161: In this week's episode of The One Show
162,": Clip 3 You walk into a room too fast, Clip 4 Doors and corners, kid. the room eats you. That's where they get you. [v3]",Chunk 162: Watch the video to find out what happens when you walk
163,"Quotes are from ""The Expanse"" databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 23 The code to read these audio clips and visualize them using Matplotlib can be",Chunk 163: BBC News takes a look back at some of the most memorable quotes
164,"As noted below, the two clips (in this case, clips 1 and 4) have different intonations summarized in the following code snippet. (amplitude) and latencies for the same quote. from scipy.io import wavfile from matplotlib import pyplot as plt",Chunk 164: The following code snippet shows how to import a .wav file from matplotlib
165,"from mat tplotlib.pyplot import figure # Read stored audio files for comparison fs, data = wavile.read(""/dbfs/Eolder/clipl.wav"") # Set plot style plt.style.use Cseabora-whitegria?",Chunk 165: How do I set the plot style of a Matlab
166,"# Create subplots ax plt.subplot (2, 2, 1) ax.plot (datal, color-/#67A0DA) # Display created figure fig-plt.show ()",Chunk 166: # Create subplots ax plt.sub
167,"Doors and corners, kid. That's where they get you. display (fig) The full code base can be found in the notebook Dynamic Time Warping Background. databricks",Chunk 167: If you've ever wondered what it's like
168,"EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 24 If we were to follow a traditional Euclidean matching (per the following graph), even With dynamic time warping, we can shift time to allow for a time series comparison if we were to discount the amplitudes, the timings between the original clip (blue) between these two clips.",Chunk 168: This is an example of a time series that is dynamic.
169,and the new clip (yellow) do not match. EUCLIDEAN MATCHING DYNAMIC TIME WARPING corners' that's,Chunk 169: Match of the Day presenter Gary Lineker has criticised
170,"kid where they where, they doors and doors","Chunk 170: All photographs courtesy of AFP, EPA, Getty Images"
171,kid corners get and that's,Chunk 171: BBC Sport takes a look at some of the best
172,get you you databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION,Chunk 172: BBC News NI takes a look at some of the
173,"25 For our time series comparison, we will use the fastdtw PyPi library; the 20000 instructions to install PyPi libraries within your Databricks workspace can be found here: Azure IAWS. By using fastdtw, we can quickly calculate the distance between","Chunk 173: In this week's Databricks, we compare the performance of two of the most"
174,15000 the different time series. 10000 5000 from fastdtw import fastdtw,Chunk 174: The fastest car in the world at the moment.
175,"# Distance between clip 1 and clip 2 distance = fastdtw (data_clipl, data_clip2) [0] -5000 print (""The distance between the two clips is 8s"" 8 distance) -10000",Chunk 175: BBC News takes a look at some of the key statistics
176,-15000 50000 100000 150000 200000 250000 300000 350000 400000 450000 The full code base can be found in the notebook Dynamic Time Warping Background. Some quick observations: BASE,"Chunk 176: In our series of letters from African journalists, novelist and writer"
177,QUERY DISTANCE Clip 1 Clip 2 480148446.0,Chunk 177: Watch the highlights of Wales' Six Nations victory over
178,"As noted in the preceding graph, clips 1 and 4 have the shortest distance, as the audio clips have the same words and intonations Clip 3 310038909.0 The distance between clips 1 and 3 is also quite short",Chunk 178: The distance between the words in this clip and the words in the audio clip is shown
179,"longer Clip 4 293547478.0 (though than when compared to clip 4) = even though they have different words,",Chunk 179: BBC Newsnight takes a look at some of the
180,"they are using the same intonation and speed Clips 1 and 2 have the longest distance due to the extremely exaggerated intonation and speed even though they are using the same quote As you can see, with dynamic time warping, one can ascertain the similarity of two different time series.",Chunk 180: This is a clip of two different time series being played at the same time.
181,"Next Now that we have discussed dynamic time warping, let's apply this use case to detect sales trends. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION","Chunk 181: In our series of letters from African journalists, novelist and writer"
182,26 CHAPTER 4: Using Dynamic Time Background Warping and MLflow to,Chunk 182: Researchers at the Massachusetts Institute of Technology (MIT)
183,"Imagine that you own a company that creates 3D printed products. Last year, you knew that drone Detect Sales Trends propellers were showing very consistent demand, SO you produced and sold those, and the year before you sold phone cases. The new year is arriving very soon, and you're sitting down with your manufacturing team Part 2 of our Using Dynamic Time Warping",Chunk 183: How do you keep track of demand for your products?
184,"to figure out what your company should produce for next year. Buying the 3D printers for your warehouse and MLflow to Detect Sales Trends series put you deep into debt, SO you have to make sure that your printers are running at or near 100% capacity at all times in order to make the payments on them. Since you're a wise CEO, you know that your production capacity over the next year will ebb and flow -","Chunk 184: If you're the CEO of a 3D printing company, you've probably been doing one of these things:"
185,"there will be some weeks when your production capacity is higher than others. For example, your capacity might be higher during the summer (when you hire seasonal workers), and lower during the third week of every month (because of issues with the 3D printer filament supply chain). Take a look at the chart below to see your company's production capacity estimate: By Ricardo Portilla, Brenner Heintz",Chunk 185: How do you know when your company's production capacity is high or low?
186,and Denny Lee Optimal Weekly Product Sales 35 30 Try this notebook series,"Chunk 186: All photographs courtesy of AFP, EPA, Getty Images"
187,25 (in DBC format) in Databricks * 20 I 15,Chunk 187: BBC Sport takes a look at some of the best
188,10 10 20 30 40,Chunk 188: BBC Sport takes a look back at some of the
189,50 Week databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 27,Chunk 189: BBC News NI takes a look at some of the
190,"Your job is to choose a product for which weekly demand meets your production capacity as closely as possible. You're looking over a catalog of products which includes last year's sales numbers for each product, and you think this year's sales will be similar. If you choose a product with weekly demand that exceeds your production capacity, then you'll have to cancel customer orders, which isn't good for business. On the other hand, if you choose a product without",Chunk 190: How do you choose the best product for your business?
191,"enough weekly demand, you won't be able to keep your printers running at full capacity and may fail to make the debt payments. Dynamic time warping comes into play here because sometimes supply and demand for the product you choose will be slightly out of sync. There will be some weeks when you simply don't have enough capacity to meet all of your demand, but as long as you're very close and you can make up for it by producing","Chunk 191: If you're in the printing business, you'll know that if you don't make enough money, you won't be able to pay off your debts."
192,"more products in the week or two before or after, your customers won't mind. If we limited ourselves to comparing the sales data with our production capacity using Euclidean matching, we might choose a product that didn't account for this and leave money on the table. Instead, we'll use dynamic time warping to choose the product that's right for your company this year. databricks","Chunk 192: With databricks, we're taking the guess work out of product selection."
193,EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 28 Load the product sales data set Calculate distance to optimal time series by product code We will use the weekly sales transaction data set found in the UCI Data Set,Chunk 193: How do you measure the performance of your business?
194,"# Calculate distance via dynamic time warping between product code and Repository to perform our sales-based time series analysis. (Source attribution: optimal time series import numpy as np James Tan, emestanscsussedusg Singapore University of Social Sciences)",Chunk 194: This paper describes the use of time series import to perform our sales-based time
195,"import ucrdtw def get_ keyed_values (s) : import pandas as pd return (s[01, s[1:1) # Use Pandas to read this data",Chunk 195: import pandas as pd return : import pandas
196,"def compute.distance (row) : sales_pdf - pd.read_csv (sales_dbfspath, header=': infer') return (row [0], ucrdtw.ucrdtw (list (row [1J[0:521), list (optimal pattern), 0.05, True) [1]) # Review data",Chunk 196: Returns the distance between two points on a row in a
197,"display spark.createatarame (sales_pdf)) ts_values = pd.DataFrame (np. apply_along_axis (get.keyed.values, 1, sales.paf.values)) distances = pd.DataFrame (ap.apply_along.axis (compute.distance, 1, ts Product_Code V wo W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 V W11 W12 W13",Chunk 197: Product_Code V wo W1 W2 W3 W4 W5
198,"values.values) distances.columns = 'dtw_dist'l ['pcode', P1 11 12 10",Chunk 198: The distance between two points on the Earth's
199,13 12 14 21 14 11 14 16 9 P2 7 o 3 2,Chunk 199: BBC Sport takes a look back at some of the
200,P3 7 11 10 13 12 14,Chunk 200: BBC Sport takes a look back at some of the
201,"P4 12 8 13 13 13 Using the calculated dynamic time warping ""distances"" column, we can view the P5",Chunk 201: The distances between the planets in the solar system are
202,8 5 13 11 14 11 18 distribution of DTW distances in a P6,Chunk 202: BBC Sport takes a look at some of the key
203,3 3 2 7 6 3 6 6 5 histogram. P7,Chunk 203: BBC Sport takes a look back at some of the
204,8 3 8 3 10 2 3 P8,Chunk 204: The winning numbers in Saturday evening's drawing of
205,8 10 10 0 15 0,Chunk 205: BBC Sport takes a look back at some of the
206,"DTW Distances for Each Pairwise Product Sales Comparison 30 70 60 Each product is represented by a row, and each week in the year is represented",Chunk 206: The following table shows the year-on-year sales
207,50 by a column. Values represent the number of units of each product sold per week. 40 30 There are 811 products in the data set.,Chunk 207: The following table lists the most popular food and drink products
208,20 10 10 Distances databricks,Chunk 208: BBC Sport takes a look at some of the best
209,"EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 29 From there, we can identify the product codes closest to the optimal sales trend After running this query, along with the corresponding query for the product codes (i.e., those that have the smallest calculated DTW distance). Since we're using","Chunk 209: Here's how it works: First, we run a query on the name of the product."
210,"that are furthest from the optimal sales trend, we were able to identify the two Databricks, we can easily make this selection using a SQL query. Let's display those products that are closest and furthest from the trend. Let's plot both of those that are closest. products and see how they differ. 8sql",Chunk 210: In this article we are going to look at two products that are closest to the optimal sales trend.
211,"Top 10 product codes closest to the optimal sales trend Comparing Optimal Sales Trends With P675 and P716 select pcode, cast (dtw_ dist as float) as dtw_dist from distances order 35 Optimal Sales Trend by cast (dtw_dist as float) limit 10",Chunk 211: Top 10 product codes closest to the optimal sales trend
212,30 P716 P675 25 20 6.0 15,Chunk 212: The winning numbers in Saturday evening's drawing of
213,5.5 5.0 10 4.5 4.0,Chunk 213: BBC Sport takes a look back at some of the
214,6 3.5 à 3.0 10 20 30,Chunk 214:     
215,"2.5 2.0 1.5 1.00 As you can see, Product #675 (shown in the orange triangles) represents the",Chunk 215: The following table shows the average selling price of the
216,"- 0.50 0.00 best match to the optimal sales trend, although the absolute sales are",Chunk 216: Sales figures for the first three months of the year
217,P675 P703 weekly P358 P697 P816 P601 P674 P372 P476 P694 pcode lower than we'd like (we'll remedy that later). This result makes sense since we'd,Chunk 217: BBC Sport looks at the best and worst results from the
218,"expect the product with the closest DTW distance to have peaks and valleys that somewhat mirror the metric we're comparing it to. (Of course, the exact time index for the product would vary on a week-by-week basis due to dynamic time warping.) Conversely, Product #716 (shown in the green stars) is the product with the worst match, showing almost no variability.",Chunk 218: This table shows the best- and worst-matched products against each other in terms of the time it takes for the sun to rise and set
219,"databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 30 Finding the optimal product: Small DTW distance and Using MLflow to track best and worst products,",Chunk 219: Researchers at the Massachusetts Institute of Technology (MIT) are using
220,"similar absolute sales numbers along with artifacts Now that we've developed a list of products that are closest to our factory's MLflow is an open source platform for managing the machine learning lifecycle, projected output (our ""optimal sales trend""), we can filter them down to those that including experimentation, reproducibility and deployment. Databricks notebooks",Chunk 220: We've been looking at the products that are closest to our factory's machine learning needs.
221,"have small DTW distances as well as similar absolute sales numbers. One good offer a fully integrated MLflow environment, allowing you to create experiments, candidate would be Product #202, which has a DTW distance of 6.86 versus the log parameters and metrics, and save results. For more information about population median distance of 7.89 and tracks our optimal trend very closely.","Chunk 221: If you are looking for an easy-to-use machine learning (ML) environment for your product development, you might want to look at"
222,"getting started with MLflow, take a look at the excellent documentation. # Review P202 weekly sales MLflow's design is centered around the ability to log all of the inputs and y_p202 - sales_pdf Isales_pdf 'Product_ Code'] P202'1.values [0] [1:53] outputs of each experiment we do in a systematic, reproducible way. On every pass through the data, known as a ""run,"" we're able to log our experiment's:",Chunk 222: In this post I'm going to show you how to build a machine learning experiment using MLflow.
223,"Parameters: The inputs to our model Metrics: The output of our model, or measures of our model's success Comparing Optimal Sales Trends With Weekly Sales for P2 60 P716 Optimal Sales Trend Artifacts: Any files created by our model = for example, PNG plots or","Chunk 223: The following table lists the inputs to our model, or measures of our model's success:"
224,"50 CSV data output 40 Models: The model itself, which we can later reload and use to serve predictions",Chunk 224: Here's a look at some of the key
225,10 20 30 40 50,Chunk 225: BBC Sport takes a look back at some of the
226,"databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 31 In our case, we can use it to run the dynamic time warping algorithm several times import mlflow","Chunk 226: In this paper, we show how we can use big data to"
227,"over our data while changing the ""stretch factor,"" the maximum amount of warp def run_ DTW (ts_stretch factor) : that can be applied to our time series data. To initiate an MLflow experiment, and # calculate DTW distance and Z-score for each product <strong>with mlflw.start_run () as run:</strong>","Chunk 227: To start an MLflow experiment, and # calculate DTW distance and Z-score for each product "
228,"allow for easy logging using mllow.log_param (), mllow.log.metrico. mllow.log.artifact (), and mlflow.log.model (), we wrap our main function # Log Model using Custom Flavor dtw_model = 'stretch_factor"" : float t.stretch.factor), using:",Chunk 228: In this post we are going to show you how to log into
229,"'pattern' : optimal pattern) <strongpmlflow cus om._lavor.log_model (dtw_model, artifact_ path= ""model"") </strong> iwith mllow.start_run () as run: # Log our stretch factor parameter to MLflow",Chunk 229: strongpmlflow cus om.
230,"trogallow.ogaran Catretch_factor"", ts.stretch._factor) </ strong> as shown in the abbreviated code at right. # Log the median DTW distance for this run StroNpIegte (""Median Distance"", distance._median) </",Chunk 230: The distance between the starting line and the finish line is
231,strong> # Log artifacts CSV file and PNG plot to MLflow <strongpmitow.logartifact/ascore.outliers + str (ts_stretch factor) + .csv') mllow.logartifact com.distMistogran png'),Chunk 231: # Log artifacts file and PNG plot strong
232,"return run.infoc/strong) stretch_ factors_ to_test = [0.0, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5] for n in stretch facto S to_test: run_DTW (n) With each run through the data, we've created a log of the ""stretch factor""",Chunk 232: The BBC Sport website is running a series of tests to see if the BBC Sport
233,"parameter being used, and a log of products we classified as being outliers based upon the Z-score of the DTW distance metric. We were even able to save an artifact (file) of a histogram of the DTW distances. These experimental runs are saved locally on Databricks and remain accessible in the future if you decide to view the results of your experiment at a later date.",Chunk 233: In this paper we have used Databricks to store our experimental data on a local hard drive.
234,"databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 32 Now that MLflow has saved the logs of each experiment, we can go back through reload models after they are initially constructed, as demonstrated in this","Chunk 234: In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks"
235,"and examine the results. From your Databricks notebook, select the ""Runs"" icon in blog post. For example, when using MLflow with scikit-learn, logging a model is the upper right-hand corner to view and compare the results of each of our runs. as easy as running the following code from within an experiment: wwwyoutubecom/watchpl-62PAPo-2ZU","Chunk 235: In this blog post, I'm going to show you how to run the following code from within an experiment."
236,"mlflow. sklearn.log.model (model=sk model, artifact path-sk_mode. ath"" Not surprisingly, as we increase our ""stretch factor,"" our distance metric decreases. MLflow also offers a ""Python function"" flavor, which allows you to save any Intuitively, this makes sense: as we give the algorithm more flexibility to warp the model from a third-party library (such as XGBoost or spaCy), or even a simple time indices forward or backward, it will find a closer fit for the data. In essence,",Chunk 236: MLflow is a Python-based machine learning algorithm that learns from large amounts of data.
237,"Python function itself, as an MLflow model. Models created using the Python we've traded some bias for variance. function flavor live within the same ecosystem and are able to interact with other MLflow tools through the Inference API. Although it's impossible to plan for every use case, the Python function model flavor was designed to be as universal and",Chunk 237: In this talk I'm going to show you how to use Python function as an MLflow model.
238,"Logging models in MLflow flexible as possible. It allows for custom processing and logic evaluation, which can come in handy for ETL applications. Even as more ""official"" model flavors MLflow has the ability to not only log experiment parameters, metrics and artifacts come online, the generic Python function flavor will still serve as an important",Chunk 238: MLflow is a Python-based tool for logging models in machine learning experiments.
239,"(like plots or CSV files), but also to log machine learning models. An MLflow model ""catchall,"" providing a bridge between Python code of any kind and MLflow's is simply a folder that is structured to conform to a consistent API, ensuring robust tracking toolkit. compatibility with other MLflow tools and features. This interoperability is very",Chunk 239: MLflow is a Python-based machine learning tool that can be used to track data from a variety of sources.
240,"powerful, allowing any Python model to be rapidly deployed to many different Logging a model using the Python function flavor is a straightforward process. types of production environments. Any model or function can be saved as a model, with one requirement: It must take in a pandas DataFrame as input, and return a DataFrame or NumPy",Chunk 240: Python Logging is a new feature in the Python programming language.
241,"MLflow comes pre-loaded with a number of common model ""flavors"" for many array. Once that requirement is met, saving your function as an MLflow model of the most popular machine learning libraries, including scikit-learn, Spark MLlib, involves defining a Python class that inherits from PythonModel, and overriding the PyTorch, TensorFlow, and others. These model flavors make it trivial to log and",Chunk 241: MLflow is a Python library for building machine learning models.
242,"-predict () method with your custom function, as described here. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 33 Loading a logged model from one of our runs",Chunk 242: How do you predict the outcome of elections?
243,"Next steps Now that we've run through our data with several different stretch factors, As you can see, our MLflow model is predicting new and unseen values with ease. the natural next step is to examine our results and look for a model that did And since it conforms to the Inference API, we can deploy our model on any","Chunk 243: In this post, I'm going to show you how to use the Inference API to build a machine learning model that can predict the"
244,"particularly well according to the metrics that we've logged. MLflow makes it easy serving platform (such as Microsoft Azure ML or Amazon SageMaker), deploy it as to then reload a logged model, and use it to make predictions on new data, using a local REST API end point, or create a user-defined function (UDF) that can easily the following instructions:",Chunk 244: We've been using MLflow to make predictions about what the weather will be like in the next few weeks...
245,"be used with Spark SQL. In closing, we demonstrated how we can use dynamic 1. Click on the link for the run you'd like to load our model from time warping to predict sales trends using the Databricks Unified Data Analytics Platform. Try out the Using Dynamic Time Warping and MLflow to Predict Sales 2. Copy the ""Run ID""","Chunk 245: In this session, we learned how to use dynamic time warping and MLflow to predict sales."
246,"Trends notebook with Databricks Runtime for Machine Learning today. 3. Make note of the name of the folder the model is stored in. In our case, it's simply named ""model"" 4. Enter the model folder name and Run ID as shown below: import custom flavor as mllox.custom_lavor","Chunk 246: In this tutorial, I'm going to show you how to import and run a machine learning model on"
247,"loaded_model mlflow_ custom flavor load model artifact.path-modelr run id-'e26961b25c4d4402a9a5a7a679fc8052"") To show that our model is working as intended, we can now load the model and use it to measure DTW distances on two new products that we've created within the variable new.sales_units :",Chunk 247: In this article we're going to show you how to load a model and use it to
248,# use the model to evaluate new products found in new_sales_units output - loadedmodel-predict (new_sales.units) print (output) databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION,Chunk 248: # use the model to evaluate new products found in new_
249,"34 CHAPTER 5: Detecting Financial Fraud Detecting fraudulent patterns at scale using artificial intelligence is a challenge, no matter the use case. Scale With Decision","Chunk 249: In our series of letters from African journalists, filmmaker"
250,"The massive amounts of historical data to sift through, the complexity of the constantly evolving machine at learning and deep learning techniques, and the very small number of actual examples of fraudulent Trees and MLflow on behavior are comparable to finding a needle in a haystack while not knowing what the needle looks like. In","Chunk 250: In the world of artificial intelligence, finding a needle in a haystack can be a challenge."
251,"Databricks the financial services industry, the added concerns with security and the importance of explaining how fraudulent behavior was identified further increase the complexity of the task. df Rules df.riticolum-labev, to Identify Known Fraud-based .whe",Chunk 251: The task of identifying known fraud-based activity has become more challenging over the
252,"(df.oldbalanceorg 56900) & (df.type == ""TRANSFER"") df.newbalanceDest = 105)) I (df.oldbalanceorg 56900) & (df.neubalanceoris S= 12)) I 0101 Financial Data ), (df.oldbalanceorg 56900) & (df.nevbalanceoris 12) & (df.amount 1160000)",Chunk 252: I (df.oldbalanceorg 56900) & (
253,").otherwise(0)) d Rules to Identify Fraud-based By Elena Boiarskaia, Navin Albert E and Denny Lee",Chunk 253: A guide to some of the most common frauds
254,"(df.oldbalanceorg 56900) (df.type ""TRANSFER"") C (df.oldbalanceorg 56900) & (df.newbalanceorig 12)) 0101 Financial Data",Chunk 254: The full text of this story will appear here.
255,"(df.oldbalanceorg 56900) & (df.newbalanceoris 12) & (df.amount 1160000) Try this notebook in Databricks * ).otherwise(e)) ), To build these detection patterns, a team of domain experts comes up with a set of rules based on how fraudsters typically behave. A workflow may include a subject matter expert in the financial fraud detection","Chunk 255: Financial fraud is one of the most common forms of cybercrime, but how do we stop it?"
256,"space putting together a set of requirements for a particular behavior. A data scientist may then take a subsample of the available data and select a set of deep learning or machine learning algorithms using these requirements and possibly some known fraud cases. To put the pattern in production, a data engineer may convert the resulting model to a set of rules with thresholds, often implemented using SQL. databricks",Chunk 256: Data scientists and data engineers use a variety of techniques to investigate fraud.
257,"EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 35 This approach allows the financial institution to present a clear set of characteristics that lead to the identification of a fraudulent transaction that is compliant with the General Data Protection Regulation (GDPR). However, this approach also poses",Chunk 257: The EU's General Data Protection Regulation (GDPR) came into force on 25 May 2018 and requires financial institutions
258,"numerous difficulties. The implementation of a fraud detection system using a hardcoded set of rules is very brittle. Any changes to the fraud patterns would take Data Engineer a very long time to update. This, in turn, makes it difficult to keep up with and adapt to the shift in fraudulent activities that are happening in the current marketplace.",Chunk 258: This paper describes the challenges of implementing a fraud detection system.
259,Datal Engineer Data Analyst Data Scientist 0 Data Analyst,Chunk 259: BBC News NI takes a look at some of the
260,"Data Scientist X Additionally, the systems in the workflow described above are often siloed, with In this blog, we will showcase how to convert several such rule-based detection the domain experts, data scientists and data engineers all compartmentalized.","Chunk 260: In this blog, we will showcase how to convert several such rule-based detection the"
261,"use cases to machine learning use cases on the Databricks platform, unifying the The data engineer is responsible for maintaining massive amounts of data and key players in fraud detection: domain experts, data scientists and data engineers. translating the work of the domain experts and data scientists into production level We will learn how to create a machine learning fraud detection data pipeline and code. Due to a lack of a common platform, the domain experts and data scientists visualize the data in real time, leveraging a framework for building modular features","Chunk 261: In this session, you will learn how to create a machine learning fraud detection data pipeline and code."
262,have to rely on sampled down data that fits on a single machine for analysis. This from large data sets. We will also learn how to detect fraud using decision trees leads to difficulty in communication and ultimately a lack of collaboration. and Apache SparkM MLlib. We will then use MLflow to iterate and refine the model to improve its accuracy.,Chunk 262: In this talk we will learn how to detect fraud using decision trees.
263,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 36 Solving with machine learning There is a certain degree of reluctance with regard to machine learning models in,"Chunk 263: In this article, I am going to look at some of the"
264,"This simultaneously mitigates three concerns: the financial world, as they are believed to offer a ""black box"" solution with no way 1. The lack of training labels of justifying the identified fraudulent cases. GDPR requirements, as well as financial 2. The decision of what features to use","Chunk 264: In our series of letters from African journalists, filmmaker and columnist Ahmedou Ould-Abdallah looks"
265,"regulations, make it seemingly impossible to leverage the power of data science. 3. Having an appropriate benchmark for the model However, several successful use cases have shown that applying machine learning to detect fraud at scale can solve a host of the issues mentioned above. Training a machine learning model to recognize the rule-based fraudulent behavior","Chunk 265: In our series of letters from African-American journalists, film-maker, and columnist Steve Kroft looks at some of the challenges"
266,"flags offers a direct comparison with the expected output via a confusion matrix. The Databricks Lakehouse Platform Provided that the results closely match the rule-based detection pattern, this approach helps gain confidence in machine learning-based fraud prevention with Databricks Notebooks",Chunk 266: The Databricks Lakehouse Platform has been used to develop a new flag-based fraud
267,the skeptics. The output of this model is very easy to interpret and may serve as a baseline discussion of the expected false negatives and false positives when compared to the original detection pattern. Financial Data Data Engineering,Chunk 267: This paper presents a regression model that can be used to estimate the likelihood of false positives
268,"Data Analytics Machine Learning Furthermore, the concern with machine learning models being difficult to interpret may be further assuaged if a decision tree model is used as the initial machine bastbonwi With wegeed","Chunk 268: In our series of letters from African journalists, film-maker and columnist"
269,"Data Democratization learning model. Because the model is being trained to a set of rules, the decision tree is likely to outperform any other machine learning model. The additional Training a supervised machine learning model to detect financial fraud is very benefit is, of course, the utmost transparency of the model, which will essentially",Chunk 269: A machine learning model to detect financial fraud has been developed by researchers at the Massachusetts Institute of Technology (MIT).
270,"difficult due to the low number of actual confirmed examples of fraudulent show the decision-making process for fraud, but without human intervention and behavior. However, the presence of a known set of rules that identify a particular need the to hard code any rules or thresholds. Of course, it must be understood","Chunk 270: In our series of letters from African journalists, film-maker and columnist Ahmedou Ould-Abdallah looks at"
271,type of fraud can help create a set of synthetic labels and an initial set of features. that the future iterations of the model may utilize a different algorithm altogether The output of the detection pattern that has been developed by the domain to achieve maximum accuracy. The transparency of the model is ultimately experts in the field has likely gone through the appropriate approval process to achieved by understanding the features that went into the algorithm. Having,"Chunk 271: Researchers at the University of California, Los Angeles (UCLA) have developed a machine learning model that can detect fraud."
272,"be put in production. It produces the expected fraudulent behavior flags and interpretable features will yield interpretable and defensible model results. may, therefore, be used as a starting point to train a machine learning model. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION",Chunk 272: The aim of this work is to train a machine learning model that can detect fraud.
273,"37 The biggest benefit of the machine learning approach is that after the initial Exploring the data modeling effort, future iterations are modular, and updating the set of labels, Creating the DataFrames: Now that we have uploaded the data to Databricks File","Chunk 273: In our series of letters from African journalists, filmmaker and columnist Ahmed Rashid looks at some of the"
274,"features or model type is very easy and seamless, reducing the time to production. System (DBFS), we can quickly and easily create DataFrames using Spark SQL. This is further facilitated on the Databricks Collaborative Notebooks where the domain experts, data scientists and data engineers may work off the same data set # Create df DataFrame which contains our simulated financial fraud detection dat taset","Chunk 274: In this article, we are going to use Spark to create a simulation of financial fraud."
275,"at scale and collaborate directly in the notebook environment. So let's get started! df = spark.sql (""select step, type, amount, nameOrig, oldbalanceorg, newbalanceorig, nameDest, oldbalanceDest, newbalanceDest from sim_fin fraud_detection""? Ingesting and exploring the data",Chunk 275: This week we're taking a look at some of the best examples of
276,"Now that we have created the DataFrame, let's take a look at the schema and the We will use a synthetic data set for this example. To load the data set yourself, first thousand rows to review the data. please download it to your local machine from Kaggle and then import the data via # Review the schema of your data Import Data = Azure and AWS.",Chunk 276: In this lesson we will create a DataFrame and import a synthetic data set from Kaggle.
277,df.printSchema () root  step: integer (nullable = true) The PaySim data simulates mobile money transactions based on a sample of real - type: string (nullable = true),Chunk 277: The PaySim data simulates mobile money transactions based on a
278,transactions extracted from one month of financial logs from a mobile money amount: double (nullable = true) nameOrig: string (nullable = true) service implemented in an African country. The below table shows the information oldbalanceorg: double (nullable true),Chunk 278: Details of the mobile money services provided by Oldbalance.org.
279,newbalanceOrig: double (nullable = true) that the data set provides: nameDest: string (nullable = true) oldbalanceDest: double (nullable = true) newbalanceDest: double (nullable = true),Chunk 279: Newbalance is a new and improved version of old
280,"Column Name Description step maps aunit toftimei int ther real world. Int this case stepi is1 hour oft time."" Total steps 744 (30 days simulation). type CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.",Chunk 280: The following table lists all the steps in the step-by-step
281,amount amount ofthet transactioni ink local currency. PAYMENT 707T 0154988899 183195 176087.23 M408069119 nameOrig customer who started thet transaction,"Chunk 281: The largest online retailer in the UK, Amazon,"
282,PAYMENT 7861.64 C1912850431 176087.23 168225.59 M633326333 oldbalanceOrg initial balance before thet transaction DAMME,"Chunk 282: BBC News NI is part of the BBC Group,"
283,newbalanceOrig new balance aftert thet transaction nameDest customer whoi ist ther recipient oft the transaction oldbalanceDest initial balance recipient before thet transaction. Note that therei isr noti informationi for customers that starty withMMerchants). newbalanceDest newb balance recipient aftert thet transaction. Notet thatt thereisnoti informationi for customers thats start with M (Merchants). databricks,Chunk 283: New balance aftert thet transaction name newbalanceDest customer whoi ist ther recipient oft the transaction
284,EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 38 Types of transactions 500G Let's visualize the data to understand the types of transactions the data captures,Chunk 284: What is the difference between a bank and a credit card?
285,450G 400G and their contribution to the overall transaction volume. 350G E 300G,Chunk 285: A look at some of the key players in the
286,"8sql 250G Organize by Type 200G select type, count (1) from financials group by type",Chunk 286: A look at some of the key stories in the
287,150G 100G 50G 1% 0.00,"Chunk 287: The world's smallest, lightest and most powerful"
288,TRANSFER CASH_IN CASH_OUT PAYMENT DEBIT type - TRANSFER 8%,Chunk 288: BBC Sport takes a look at some of the key
289,CASHLIN CASH_OUT PAYMENT 35% DEBIT,Chunk 289: A look at some of the key stories from the
290,"Rule-based model 22% We are not likely to start with a large data set of known fraud cases to train our model. In most practical applications, fraudulent detection patterns are identified by a set of rules established by the domain experts. Here, we create a column",Chunk 290: How do you train a fraud detection model?
291,"called label based on these rules. 34% # Rules to Identify Known Fraud-based df = df.withColumn (""label"", F.when(",Chunk 291: Rules to identify Known Fraud-based df =
292,"(df.oldbalanceorg 56900) & (dF.newbalanceorig To get an idea of how much money we are talking about, let's also visualize the 56900) & (dF.newbalanceorig > 12) & (df.amount > 1160000) data based on the types of transactions and on their contribution to the amount of , 1",Chunk 292: This table shows the value of all transactions carried out by the Oldbalanceorg and the Newbalanceorig
293,".otherwise (0)) cash transferred (i.e., sum(amount). 8sql select type, sum (amount) from financials group by type databricks",Chunk 293: sum (amount) from financials group by type
294,EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 39 Visualizing data flagged by rules Selecting the appropriate machine learning models These rules often flag quite a large number of fraudulent cases. Let's visualize the,"Chunk 294: In our series of letters from African journalists, filmmaker and columnist Ahmed Rashid looks at"
295,"number of flagged transactions. We can see that the rules flag about 4% of the In many cases, a black box approach to fraud detection cannot be used. First, the cases and 11% of the total dollar amount as fraudulent. domain experts need to be able to understand why a transaction was identified as fraudulent. Then, if action is to be taken, the evidence has to be presented",Chunk 295: The number of suspicious transactions flagged by Google's anti-fraud rules has been revealed.
296,"8sql in court. The decision tree is an easily interpretable model and is a select label, count (1) as ""Transactions', sun (amount) as 'Total Amount' great starting from financials_labeled group by label",Chunk 296: The US Federal Reserve's decision on interest rates will be published on
297,point for this use case. Transactions Total Amount 4% B:,Chunk 297: The Bank of England has announced that it is to
298,"label 4.0 B. X,4.2) (4,21.2) 11% O",Chunk 298: The winning numbers in Saturday evening's drawing of
299,"3.0 :: (X,<1.9) X,21.9) X,2.4) (X,22.4 X, 2.0 (Kxo.9) X22 *",Chunk 299: BBC Sport takes a look at some of the key
300,"1.0 9 (X,<0.4) (4,20.4) (X,0.4) (X,20.4) a 96% 89%",Chunk 300: BBC Sport takes a look at some of the key
301,"0.0 0.5 1.0 1.5 2.0 X,<1.4) (X,21.4) (X,0.9) (X,20.9) X, (a) (b)",Chunk 301: The winning numbers in Saturday evening's drawing of
302,"Creating the training set To build and validate our ML model, we will do an 80/20 split using randomSplit. This will set aside a randomly chosen 80% of the data for training and the remaining 20% to validate the results. # Split our dataset between training and test datasets","Chunk 302: In this session, we are going to build and validate a machine learning model by splitting our dataset between training and"
303,"(train, test) = df.randomsplitqt0.8, 0.2], seed-12345) databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 40 Creating the ML model pipeline",Chunk 303: The results of a study on the use of artificial
304,"Visualizing the model To prepare the data for the model, we must first convert categorical variables to Calling display () on the last stage of the pipeline, which is the decision numeric using Stringindexer. We then must assemble all of the features tree model, allows us to view the initial fitted model with the chosen decisions","Chunk 304: In this tutorial, we will learn how to build a features tree model from a data set."
305,we would like for the model to use. We create a pipeline to contain these feature at each node. This helps us to understand how the algorithm arrived at the preparation steps in addition to the decision tree model SO that we may repeat resulting predictions. these steps on different data sets. Note that we fit the pipeline to our training data,Chunk 305: We want to train a decision tree model on different data sets.
306,display (dt.model.stages [-1]) first and will then use it to transform our test data in a later step. from pyspark.ml import Pipeline from Pyspark.ml.feature import StringIndexer from Pyspark.ml.feature import VectorAssembler,"Chunk 306: In this tutorial, I'm going to show you how to"
307,"from Peparkmidasication import Decisionfreclassile: cae44 de # Encodes a string column of labels to a column of label indices indexer = StringIndexer (inputCol = ""type"", outputCol = ""typeIndexed"")",Chunk 307: The following is an example of how to create a label index
308,"# VectorAssembler is a transformer that combines a given list of columns into a single vector column va - VectorAssembler (inputCols = l'typeIndexed"", ""amount"", ""oldbalanceOrg"", ""newbalanceorig"", ""oldbalanceDest"", ""newbalanceDest"", ""orgDiff"", destDiff""), outputCol = ""features"")",Chunk 308: # VectorAssembler is a transformer that combines a given list of columns
309,"# Using the DecisionTree classifier model dt = Decisionfreclassiler (labelCol = ""label"", featuresCol = 20+ ""features"", seed = 54321, maxDepth = 5) ",Chunk 309: Find out how to train a classifier on a single
310,"# Create our pipeline stages pipeline = Pipeline (stages= [indexer, va, dt]) 6 # View the Decision Tree model (prior to CrossValidator) dt_model = pipeline.fit (train)",Chunk 310: # Create our pipeline stages pipeline = Pipeline (stages
311,Visual representation of the decision tree model databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 41 Model tuning,"Chunk 311: In this video, you'll learn how to"
312,"Model performance To ensure we have the best-fitting tree model, we will cross-validate the model We evaluate the model by comparing the Precision-Recal (PR) and area under the with several parameter variations. Given that our data consists of 96% negative ROC curve (AUC) metrics for the training and test sets. Both PR and AUC appear to",Chunk 312: The aim of this study is to train and test a tree model for predicting the likelihood of a car crash.
313,"and 4% positive cases, we will use the Precision-Recall (PR) evaluation metric to be very high. account for the unbalanced distribution. <b>from</b> pyspark.ml.tuning <b>importx/b> CrossValidator, # Build the best model (training and test datasets)","Chunk 313: In this project, we are going to build a cross-validation model for"
314,ParamcridBuilder train_pred = CVModel u .transform (train) test_pred CVModel .transform (test) # Build the grid of different parameters paramGrid = ParamdridBuildero,Chunk 314: ParamcridBuilder train_pred = CV
315,"# Evaluate the model on training datasets .addGrid (dt.maxDepth, [5, 10, 15]) I pr_train evaluatorP.evaluate (train_pred) addGrid (dt.maxBins, [10, 20, 30]) I auc_train = evaluatorAvC.evaluate (train_pred)",Chunk 315: I auc_train = evaluatorAvC.evaluate
316,".build() # Evaluate the model on test datasets # Build out the cross validation pr_test = evaluatorP.evaluate (test_pred) crossval = CrossValidator (estimator dt,",Chunk 316: .build # the model on test datasets #
317,"auc_test - evaluatorAvC.evaluate (test_pred) estimatorParamlaps = paramGrid, evaluator - evaluatorpR, # Print out the PR and AUC values numFolds = 3)",Chunk 317: Print out the PR andAUC values for the evaluator
318,"print (""PR train:"", pr_train) # Build the CV pipeline print (""AUC train:"", auc_train) pipelinecv = Pipeline (stages= [indexer, va, crossval]) print (""PR test:"", pr_test)","Chunk 318: print (""PR train:"", pr_train"
319,"print (""AUC test:"", auc_test) # Train the model using the pipeline, paramet ter grid, and preceding BinaryClassifcationivaluator CVModel_u - pipelinecv.fit (train) # Output:","Chunk 319: Train the model using the pipeline, paramet ter"
320,# PR train: 0.953789498452128 AUC train: 0.998647996459481 PR test: 0.9539170535377599 # AUC test: 0.9984378183482442 databricks,Chunk 320: Check out the best images from this year's
321,"EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 42 To see how the model misclassified the results, let's use Matplotlib and pandas to # Reset the DataFrames for no fraud (dfn') and fraud (dfy') dfn = train.filter (train.label == 0)",Chunk 321: How would you rate the performance of a train?
322,visualize our confusion matrix. dfy = train.filter (train.label == 1) # Calculate summary metrics Confusion Matrix (Unbalanced Test) N = train.count (),Chunk 322: Train is the most confusing thing in the world.
323,1200000 y = dfy.count () 1050000 P = y/N Fraud 50717,Chunk 323: The latest figures from the UK's Serious Fraud
324,"58 900000 # Create a more balanced training dataset train_b = dfn.sample (<b>Falsex/b>, P, seed = 92285) union (dfy) 750000",Chunk 324: Train_b = dfn.sample (
325,"600000 # Print out metrics print (""Total count: 8s, Fraud cases count: 8s, Proportion of fraud 450000 cases: 8s"" % (N, Y, p))",Chunk 325: The UK's Serious Fraud Office (SFO
326,"print (""Balanced training dataset count: 8s"" % train_b.count ()) No Fraud 2421 1219030 300000",Chunk 326: The UK's National Crime Agency (NCA)
327,"150000 # Output: # Total count: 5090394, Fraud cases count: 204865, Proportion of fraud Fraud No Fraud",Chunk 327: The number of cases of fraud reported to the Serious
328,cases: 0.040245411258932016 Predicted label # Balanced training dataset count: 401898 Balancing the classes # Display our more balanced training dataset,Chunk 328: All datasets are subject to change at any time.
329,"displaytraia..groupe, (""label"") .count ()) We see that the model is identifying 2,421 more cases than the original rules identified. This is not as alarming, as detecting more potential fraudulent cases label could be a good thing. However, there are 58 cases that were not detected by",Chunk 329: In this paper we present the results of a study on the use of artificial intelligence (AI) to identify
330,"a a the algorithm but were originally identified. We are going to attempt to improve our prediction further by balancing our classes using undersampling. That is, we will keep all the fraud cases and then downsample the non-fraud cases to",Chunk 330: We have developed a new method for predicting the likelihood of a crime being committed.
331,"match that number to get a balanced data set. When we visualize our new 49% 51% data set, we see that the yes and no cases are 50/50. databricks","Chunk 331: In this week's Databricks, we look at"
332,EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 43 Updating the pipeline Review the results Now let's update the ML pipeline and create a new cross validator. Because we are Now let's look at the results of our new confusion matrix. The model misidentified,"Chunk 332: In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the"
333,"using ML pipelines, we only need to update it with the new data set and we can only one fraudulent case. Balancing the classes seems to have improved the model. quickly repeat the same pipeline steps. # Re-run the same ML pipeline (including parameters grid) Confusion Matrix (Balanced Test)",Chunk 333: We are running a regression model on a new data set and need to balance the classes in the model.
334,"1200000 crossval_b CrossValidator (estimator = dt, estimatorParamaps - paramGrid, 1050000 evaluator - evaluatorAUC,",Chunk 334: The following is a list of the most frequently used
335,"numFolds = 3) Fraud 50774 1 900000 pipelinecvb = Pipeline (stages-lindexer, va, crossval_b])",Chunk 335: The latest figures from the UK's Serious Fraud
336,"750000 # Train the model using the pipeline, parameter grid, and BinaryClassifcationivaluator using the train_b dataset 600000 CVModel_b - pipelinecv_b.fit (train_b)","Chunk 336: Train the model using the pipeline, parameter grid,"
337,450000 # Build the best model (balanced training and full test datasets) NoF Fraud 488 1220963,Chunk 337: Fraud investigators at the UK's National Crime Agency
338,300000 train_predb CvModel b.transform (train_b) test_predb cwModel_D.transform (test) 150000 # Evaluate the model on the balanced training datasets,Chunk 338: The following table shows the results of the training datasets
339,pr_train_b evaluatorP.evaluate (train_pred.b) Fraud No Fraud auc_train_b evaluatorAvC.evaluate (train_predb) Predicted label,Chunk 339: Fraud No Fraud auc_train_b evaluator
340,"# Evaluate the model on full test datasets pr_test_b evaluatorP.evaluate (test_pred_b) Model feedback and using MLflow auc_test_b = evaluatorAUC.evaluate (test_pred_b) Once a model is chosen for production, we want to continuously collect feedback",Chunk 340: We want to collect feedback on the model selected for production.
341,"# Print out the PR and AUC values to ensure that the model is still the behavior of interest. Since we are print (""PR train:"", pr_train_b) identifying",Chunk 341: # Print out the AUC and PR values for the
342,"print (""AUC train: auc_train_b) starting with a rule-based label, we want to supply future models with verified true print (""PR test:"", pr_test_b) print (""AUC test:"", auc_test_b) labels based on human feedback. This stage is crucial for maintaining confidence",Chunk 342: We are working on a new way to label trains.
343,"and trust in the machine learning process. Since analysts are not able to review # Output: every single case, we want to ensure we are presenting them with carefully chosen # PR train: 0.999629161563572 # AUC train: 0.998071389056655",Chunk 343: We want to make it as easy as possible for you to see how our machine learning
344,"cases to validate the model output. For example, predictions, where the model has # PR test: 0.990470917789063 low are # AUC test: 0.9997903902201509",Chunk 344: The following examples show how to validate the results of
345,"certainty, good candidates for analysts to review. The addition of this type of feedback will ensure the models will continue to improve and evolve with the changing landscape. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION",Chunk 345: Bigbricks has added a new feature to its learning management system (LMS) to
346,"44 4 MLflow helps us throughout this cycle as we train different model versions. Conclusion We can keep track of our experiments, comparing the results of different model configurations and parameters. For example here, we can compare the PR and AUC We have reviewed an example of how to use a rule-based fraud detection",Chunk 346: How to use 44 4 MLflow to train machine learning models How to use 44 4 MLflow to train machine learning models How
347,"of the models trained on balanced and unbalanced data sets using the MLflow UI. label and convert it to a machine learning model using Databricks with MLflow. Data scientists can use MLflow to keep track of the various model metrics and any This approach allows us to build a scalable, modular solution that will help us additional visualizations and artifacts to help make the decision of which model keep up with ever-changing fraudulent behavior patterns. Building a machine","Chunk 347: In this talk, I will show you how to build a machine learning model using MLflow."
348,"should be deployed in production. The data engineers will then be able to easily learning model to identify fraud allows us to create a feedback loop that helps retrieve the chosen model along with the library versions used for training as a jar the model to evolve and identify new potential fraudulent patterns. We have seen file to be deployed on new data in production. Thus, the collaboration between how a decision tree model, in particular, is a great starting point to introduce",Chunk 348: We are looking at how decision trees can be used to identify fraud in data.
349,"the domain experts who review the model results, the data scientists who update machine learning to a fraud detection program due to its interpretability and the models, and the data engineers who deploy the models in production will be excellent accuracy. strengthened throughout this iterative process.",Chunk 349: The fraud detection team is made up of many people.
350,"A major benefit of using the Databricks platform for this effort is that it allows for data scientists, engineers and business users to seamlessly work together wwwyoutupecom/watchveK.459-ki8 throughout the process. Preparing the data, building models, sharing the results wwwyputubecom/atchveBVSyPymiw",Chunk 350: BBC News takes a look at how the BBC is using the Databricks platform to
351,"and putting the models into production can now happen on the same platform, allowing for unprecedented collaboration. This approach builds trust across the previously siloed teams, leading to an effective and dynamic fraud detection program. Try this notebook by signing up for a free trial in just a few minutes and get",Chunk 351: This case study shows how one of the world's largest banks is using artificial intelligence (AI) and machine learning to
352,started creating your own models. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 45 CHAPTER 6:,Chunk 352: How do you build a 3D model of your
353,Fine-Grained Time Series Advances in time series forecasting are enabling retailers to generate more reliable demand forecasts. The challenge now is to produce these forecasts in a timely manner and at a level of granularity that allows Forecasting at Scale With the business to make precise adjustments to product inventories. Leveraging Apache Spark and Facebook,Chunk 353: This report looks at how Apache Spark and Facebook are enabling retailers to generate more reliable demand forecasts.
354,"Prophet and Apache Spark"" Prophet, more and more enterprises facing these challenges are finding they can overcome the scalability and accuracy limits of past solutions. In this chapter, we'll discuss the importance of time series forecasting, visualize some sample time series data, and then build a simple model to show the use of Facebook Prophet. Once you're comfortable building a single model, we'll combine Facebook Prophet with the magic of Spark to show you how to train hundreds",Chunk 354: Time series forecasting and hundreds training are two of the biggest challenges facing data scientists today.
355,"of models at once, allowing you to create precise forecasts for each individual product-store combination at a level of granularity rarely achieved until now. Accurate and timely forecasting is now more important than ever Improving the speed and accuracy of time series analyses in order to better forecast demand for products and services is critical to retailers' success. If too much product is placed in a store, shelf and storeroom",Chunk 355: Time series forecasting has never been so easy.
356,"space can be strained, products can expire, and retailers may find their financial resources are tied up in By Bilal Obeidat, Bryan Smith inventory, leaving them unable to take advantage of new opportunities generated by manufacturers or shifts and Brenner Heintz in consumer patterns. If too little product is placed in a store, customers may not be able to purchase the",Chunk 356: Too little product in a store can have a negative impact on a retailer's bottom line.
357,"products they need. Not only do these forecast errors result in an immediate loss of revenue to the retailer, Try this time series forecasting but over time consumer frustration may drive customers toward competitors. notebook in Databricks * New expectations require more precise time series forecasting methods and models",Chunk 357: Time series forecasting has become a major problem for retailers.
358,"For some time, enterprise resource planning (ERP) systems and third-party solutions have provided retailers with demand forecasting capabilities based upon simple time series models. But with advances in technology and increased pressure in the sector, many retailers are looking to move beyond the linear models and more traditional algorithms historically available to them. New capabilities, such as those provided by Facebook Prophet, are emerging",Chunk 358: Demand forecasting has become an increasingly important part of the supply chain.
359,"PRGPHET from the data science community, and companies are seeking the flexibility to apply these machine learning models to their time series forecasting needs. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 46",Chunk 359: We are looking for a Data Scientist to work with our machine learning models to predict
360,"This movement away from traditional forecasting solutions requires retailers and Next, by viewing the same data on a monthly basis, we can see that the year- the like to develop in-house expertise not only in the complexities of demand over-year upward trend doesn't progress steadily each month. Instead, we see a forecasting but also in the efficient distribution of the work required to generate",Chunk 360: In our series of letters from UK business journalists we look at the changing nature of retail forecasting.
361,"clear seasonal pattern of peaks in the summer months and troughs in the winter hundreds of thousands or even millions of ML models in a timely manner. Luckily, months. Using the built-in data visualization feature of Databricks Collaborative we can use Spark to distribute the training of these models, making it possible to Notebooks, we can see the value of our data during each month by mousing",Chunk 361: We use Spark to train ML models for our big data projects.
362,"predict both demand for products and services and the unique demand for each over the chart. product in each location. sale Dec 1,2017 695,170 Visualizing demand seasonality in time series data",Chunk 362: The chart below shows the year-on-year change in demand
363,"1.0M 800k To demonstrate the use of Prophet to generate fine-grained demand forecasts 600k for individual stores and products, we will use a publicly available data set from",Chunk 363: We will use Prophet to generate demand forecasts for individual stores and
364,Kaggle. It consists of 5 years of daily sales data for 50 individual items across 10 2013 2015 MONTH 2017 different stores.,Chunk 364: This is a visualisation of the best-selling items
365,"To get started, let's look at the overall yearly sales trend for all products and stores. At the weekday level, sales peak on Sundays (weekday 0), followed by a hard drop As you can see, total product sales are increasing year over year with no clear sign on Mondays (weekday 1), then steadily recover throughout the rest of the week. of convergence around a plateau. year 30k",Chunk 365: Let's take a look at the seasonality of retail sales.
366,2013 11M 20k 2014 10M,Chunk 366: BBC Sport takes a look back at some of the
367,2015 e 9.5M 10k 2016 2017 9.0M,Chunk 367: BBC Sport takes a look back at some of the
368,8.5M 0.00 8.OM WEEKDAY 3 5,Chunk 368: BBC News NI takes a look at some of the
369,2013 2014 2015 2016 2017,Chunk 369: BBC Sport takes a look back at some of the
370,year databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 47 Getting started with a simple time series forecasting,Chunk 370: BBC News takes a look at some of the key
371,"model on Facebook Prophet As illustrated above, our data shows a clear year-over-year upward trend in sales, Now that we have fit our model to the data, let's use it to build a 90-day forecast. along with both annual and weekly seasonal patterns. It's these overlapping In the code below, we define a data set that includes both historical dates and 90",Chunk 371: Let's take a look at Facebook's latest sales figures.
372,"patterns in the data that Facebook Prophet is designed to address. days beyond, using Prophet's make.ature.Aatafrane method: Facebook Prophet follows the scikit-learn API, SO it should be easy to pick up for future_pd = model.make_future_dataframe ( periods=90,",Chunk 372: In this post I'm going to show you how to use Facebook Prophet to
373,"anyone with experience with sklearn. We need to pass in a two-column pandas freq-'d', DataFrame as input: the first column is the date, and the second is the value to includeMistory-Trve predict (in our case, sales). Once our data is in the proper format, building a model",Chunk 373: I'm trying to build a model to predict the likelihood of a sale happening.
374,"# predict over the dataset is easy: forecast_pd model.predict (future_pd) import pandas as pd That's it! We can now visualize how our actual and predicted data line up, as well as",Chunk 374: Here's how to build a model that predicts the import of
375,"from fbprophet import Prophet a forecast for the future using Prophet's built-in -plot method. As you can see, the # instantiate the model and set parameters weekly and seasonal demand patterns we illustrated earlier are in fact reflected in model = Prophet","Chunk 375: In our series of posts on how to use Prophet, we looked at how to create a forecast"
376,"interval width=0.95, the forecasted results. growth=' linear', alyeaoaltytal, predict_fig - model.plot (forecast_pd, xlabel-'date', ylabel-'sales')",Chunk 376: The following table shows the expected sales growth for the
377,"elysesaltyt, display (fig) ayeaoalityrte, seasonalitymode-'multiplicative # fit the model to historical data",Chunk 377: seasonalitymode'multiplicative # fit the
378,model.fit (history_pd) databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 48 This visualization is a bit busy. Bartosz Mikulski,Chunk 378: BBC News takes a look back at some of the
379,"provides an excellent breakdown of it that is 50 well worth checking out. In a nutshell, the black HISTORICAL FORECASTED",Chunk 379: If you've ever wanted to know more about
380,"DATA DATA dots represent our actuals, with the darker 40 blue line representing our predictions and","Chunk 380: BBC Sport tracks all the key stories, stats and"
381,the lighter blue band representing our (95%) uncertainty interval. 30 a 20,Chunk 381: BBC Sport takes a look back at some of the
382,10 0 2017-02 2017-04 2017-06 2017-08 2017-10 2017-12 2018-02 2018-04 date databricks,Chunk 382: BBC News NI takes a look at some of the
383,"EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 49 Training hundreds of time series forecasting models Of course, training models on a cluster of worker nodes (computers) requires more in parallel with Prophet and Spark",Chunk 383: This course is for people who want to learn how to use artificial intelligence (AI
384,"cloud infrastructure, and this comes at a price. But with the easy availability of on-demand cloud resources, companies can quickly provision the resources they Now that we've demonstrated how to build a single model, we can use the power need, train their models and release those resources just as quickly, allowing them of Spark to multiply our efforts. Our goal is to generate not one forecast for the","Chunk 384: We've shown how to use Spark to build a single model, train it and release it just as quickly, allowing them of Spark to multiply our efforts."
385,"to achieve massive scalability without long-term commitments to physical assets. entire data set, but hundreds of models and forecasts for each product-store combination, something that would be incredibly time-consuming to perform as a The key mechanism for achieving distributed data processing in Spark is the sequential operation. DataFrame. By loading the data into a Spark DataFrame, the data is distributed","Chunk 385: In this paper, we describe how we have used Spark to create a distributed data warehouse."
386,"across the workers in the cluster. This allows these workers to process subsets Building models in this way could allow a grocery store chain, for example, to of the data in a parallel manner, reducing the overall amount of time required to create a precise forecast for the amount of milk they should order for their perform our work.","Chunk 386: In our paper, we present a novel way to process large amounts of data: we cluster all of the data in a cluster into a"
387,"Sandusky store that differs from the amount needed in their Cleveland store, based upon the differing demand at those locations. Of course, each worker needs to have access to the subset of data it requires to do its work. By grouping the data on key values, in this case on combinations of store and item, we bring together all the time series data for those key values onto",Chunk 387: This is a great example of how you can use time series data to work out the differences in demand between two stores.
388,"How to use Spark DataFrames to distribute the processing a specific worker node. of time series data store_item history Data scientists frequently tackle the challenge of training large numbers of models groupBy ('store', 'item')",Chunk 388: How to use Spark DataFrames to distribute the processing a specific worker node
389,"using a distributed data processing engine such as Spark. By leveraging a Spark cluster, individual worker nodes in the cluster can train a subset of models in We share the groupBy code here to underscore how it enables us to train parallel with other worker nodes, greatly reducing the overall time required to train many","Chunk 389: In our series of papers on artificial intelligence, we will be looking at how we can reduce the time it takes to train models"
390,"the entire collection of time series models. models in parallel efficiently, although it will not actually come into play until we set up and apply a custom pandas function to our data in the next section. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION",Chunk 390: We are working on a new way to store time series data.
391,"50 Leveraging the power of pandas user-defined functions With our time series data properly grouped by store and item, we now need to def forecaststore.iten (history_pd: pd.DataFrame) -> pd.DataFrame: train a single model for each group. To accomplish this, we can use a pandas","Chunk 391: In this paper, we will look at how pandas can be used to train models for time series data"
392,"# instantiate the model, configure the parameters function, which allows us to apply a custom function to each group of data in our model = Prophet ( DataFrame. interval.width-0.S,",Chunk 392: We are going to create a model of a group of data
393,"growth=' linear', aly,easoalty-ral, This function will not only train a model for each group, but also generate a result ely.sescmaltyt, aryacatyrte,",Chunk 393: This function will train a model for each group of
394,"set representing the predictions from that model. But while the function will train seasonalltymode-""'ntiplicative and predict on each group in the DataFrame independent of the others, the results returned from each group will be conveniently collected into a single resulting # fit the model",Chunk 394: This function will train a seasonalltymode-'ntiplicative model and predict on each group
395,DataFrame. This will allow us to generate store-item level forecasts but present our model.fit (history_pd) results to analysts and managers as a single output data set. # configure predictions future_pd = model.make future_ dataframe (,Chunk 395: The following code is used to create a newFrame for our store-item
396,"periods=90, As you can see in the abbreviated code below, building our function is relatively freq=' d', straightforward. Unlike in previous versions of Spark, we can declare our functions include.history-Trve","Chunk 396: In this tutorial, we'll show you how to declare a"
397,"in a fairly streamlined manner, specifying the type of pandas object we expect to receive and return, ie., Python type hints. # make predictions results_pd = model.predict (future_pd) Within the function definition, we instantiate our model, configure it and fit it to the","Chunk 397: In this example, we'll be making predictions about the future of pandas."
398,"data it has received. The model makes a prediction, and that data is returned as # return predictions the output of the function. return results_pd databricks",Chunk 398: Returns the result of a function that makes a model of
399,"EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 51 Now, to bring it all together, we use the groupBy command we discussed earlier Next steps to ensure our data set is properly partitioned into groups representing specific","Chunk 399: In our series of letters from African journalists, filmmaker and columnist Farai Sevenzo looks"
400,"store and item combinations. We then simply add the applylnPandas function We have now constructed a forecast for each store-item combination. Using a to our DataFrame, allowing it to fit a model and make predictions on each grouping SQL query, analysts can view the tailored forecasts for each product. In the chart of data. below, we've plotted the projected demand for product #1 across 10 stores. As you",Chunk 400: We've constructed a forecast for each store-item combination.
401,"can see, the demand forecasts vary from store to store, but the general pattern is The data set returned by the application of the function to each group is updated consistent across all of the stores, as we would expect. to reflect the date on which we generated our predictions. This will help us keep track of data generated during different model runs as we eventually take our",Chunk 401: This function returns demand forecasts for each of the following groups of stores:
402,"Mar28,2018 23.0 functionality into production. 35 from Pypark.sgl.functions import current_date results = (",Chunk 402: Pypark.sgl.functions import current
403,"store_item.history 0 groupBy ('store', 'item') Vane DATE",Chunk 403: A selection of photos from around the world from the
404,"Mar01 apply(forecast_store_item) withColumn training.datel, current date ()) As new sales data arrives, we can efficiently generate new forecasts and append these to our existing table structures, allowing analysts to update the business's",Chunk 404: The table below shows sales growth rates over the past five years for the
405,"expectations as conditions evolve. To generate these forecasts in your Databricks environment, please import the following notebook: Fine-Grained Demand Forecasting With Spark 3. To access the prior version of this notebook, built for Spark 2.0, please click this link.",Chunk 405: Forecasts are subject to change at any time.
406,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 52 CHAPTER 7: Applying Image,Chunk 406: BBC News NI takes a look at some of the
407,Introduction Classification With PyTorch PyTorch Lightning is a great way to simplify your PyTorch code and bootstrap your deep learning workloads. Lightning on Databricks Scaling your workloads to achieve timely results with all the data in your lakehouse brings its own,"Chunk 407: In this course, you will learn how to use PyTorch Lightning and Databricks to"
408,"challenges, however. This article will explain how this can be achieved and how to efficiently scale your code with Horovod. Increasingly, companies are turning to deep learning in order to accelerate their advanced machine learning applications. For example, computer vision techniques are used nowadays to improve defect inspection for manufacturing: natural language processing is utilized to augment business processes with chatbots and","Chunk 408: With Horovod, you can build deep learning applications on top of your existing Python code."
409,"neural network based recommender systems are used to improve customer outcomes. Training deep learning models, even with well-optimized code, is a slow process, which limits the ability of data science teams to quickly iterate through experiments and deliver results. As such, it is important to know how to best harness compute capacity in order to scale this up. In this article we will illustrate how to first structure your codebase for maximum code reuse, then show how",Chunk 409: Deep learning is one of the fastest growing areas of computer science.
410,to scale this from a small single node instance across to a full GPU cluster. We will also integrate it all with MLflow to provide full experiment tracking and model logging. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 53,Chunk 410: We will be running a large-scale experiment to see if we can use machine learning to predict
411,"Part 1 - Data Loading and Adopting PyTorch Lightning First, let's start with a target architecture. Cluster setup When scaling deep learning, it is important to start small and gradually scale up","Chunk 411: In this two-part series, we'll look at how to"
412,"To follow through the notebooks, an instance type with at least 64GB RAM is the experiment in order to efficiently utilize expensive GPU resources. Scale up required. The modeling process is memory intensive and it is possible to run out your code to run on multiple GPUS within a single node before looking to scale of RAM with smaller instances, which can result in the following error.","Chunk 412: In this tutorial, I will show you how to scale up your code to run on multiple GPUS in a single node."
413,"across multiple nodes to reduce code complexity. Databricks supports single-node clusters to support this very usage pattern. Fatal error: The Python kernel is unresponsive. See: Azure Single Node Clusters, AWS Single Node Clusters, GCP Single Node code",Chunk 413: Databricks supports single-node clusters to reduce code complexity.
414,"The was built and tested on Databricks Runtime 10.4 LTS for Machine Learning Clusters. In terms of instance selection, NVIDIA T4 GPUS provide a cost-effective and also 11.1 ML. On DBR 10.4 LTS ML only Pytorch-ightning up to 1.6.5 is supported. instance type to start with. On AWS these are available in G4 instances. On On DBR 11.1 ML, Pytorch-ightning 1.7.2 has been tested. We have installed our",Chunk 414: Pytorch-ightning is a Python library that provides a way to train deep learning algorithms.
415,"Azure these are available in NCasT4_v3 instances. On GCP these are available libraries as workspace level libraries. Unlike using %pip, which installs libraries only as A2 instances. for the active notebook on the driver node, workspace libraries are installed on all",Chunk 415: libraries are installed on the active notebook on the driver node.
416,"nodes, which we will need later for distributed training. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 54 Target Architecture","Chunk 416: In our series of letters from African journalists, filmmaker"
417,DBR 10.4 LTS ML Configuration Install library Library Source PyTorch Lightaing Upload DBFS/S3 PyPI Maven CRAN Workspace,Chunk 417: The latest release of the PyTorch ML library for
418,DELTA LAKE Pétastorm DataLoader mlflow Package Pytorch-ighting:-165,Chunk 418: DataLoader mlflow Package Pytorch-ight
419,Lightning Model land Training Loop with MLflow experiment Repository 0 logging,Chunk 419: Researchers at the Massachusetts Institute of Technology (MIT)
420,Optional Cancel Install Figure 2: Key components DBR 11.1 ML Configuration Install library,Chunk 420: BBC Sport takes a look at some of the key
421,"x The goal of this article is to build up a codebase structured as above. We will store our data using the open source Linux Foundation project Delta Lake. Library Source Under the hood, Delta Lake stores the raw data in Parquet format. Petastorm","Chunk 421: In this article, we are going to learn how to store and retrieve raw data from the Peta"
422,Upload DBFS/S3 PyPI Maven CRAN Workspace takes on the data loading duties and provides the interface between the Package lakehouse and our deep learning model. MLflow will provide experiment Pytorch-ighing:-172,Chunk 422: Maven CRAN and MLflow have been added to the PyPI
423,"tracking tools and allow for saving out the model to our model registry. Repository 0 With this setup, we can avoid unnecessary data duplication costs as well as Optional Cancel Install","Chunk 423: In this article, we will look at how to set up a"
424,govern and manage the models that we are training. Figure 1:Library configuration databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 55,"Chunk 424: As part of our commitment to continuous improvement, we"
425,"Part 2 = Example Use Case and Leveraging your data lake for deep learning with Petastorm Library Overview Historically, data management systems like lakehouses and data warehouses have developed in parallel with, rather than in integration with, machine learning","Chunk 425: In this two-part series, we look at how data management systems can be used"
426,"Example use case frameworks. As such, PyTorch DataLoader modules do not support the Parquet format out of the box. They also do not integrate with lakehouse metadata For this use case example, we will use the TensorFlow flowers data set. This data set structures like the hive metastore.",Chunk 426: This example shows how to use PyTorch DataLoader with Parquet.
427,will be used for a classification type problem where we are trying to identify which class of flower is which. The Petastorm project provides the interface between your lakehouse tables and PyTorch. It also handles data sharding across training nodes and provides a caching layer. Petastorm comes prepackaged in the Databricks Runtime for,Chunk 427: In this talk we will be looking at the Petastorm project and how it could be used to solve some of the
428,Machine Learning. Let's first become familiar with the data set and how to work with it. Of tulips is that (2),"Chunk 428: In our series of letters from African journalists, filmmaker"
429,sunflowers (3) sunfiowers (3) note all we need to do to transform a Spark DataFrame into a Petastorm object is the code:,Chunk 429: In our series of tutorials on how to use Spark
430,"peta_conv_df = make.spark.converte: (preprocessed.dr) Once we have the spark_converter object, we can convert that into a PyTorch roses (4) sunflowers (3) dandelion (0)","Chunk 430: To make a PyTorch rose, we need to"
431,DataLoader using: with peta_ conv df.make_ toro taloader (transfomm.spec-transfomm._fune) as converted dataset This then provides a converted.dataset DataLoader that we can use in our dandelion (0),Chunk 431: This example shows how to use a DataLoader to
432,PyTorch code as per normal. dandelion (0) dandelion (0) Open and follow the notebook titled Exploring the flowers dataset. A standard ML runtime cluster will be sufficient; there is no need to run this on a GPU cluster.,Chunk 432: This is an example of how to use PyTorch to run ML on a dataset of
433,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 56 Simplify and structure your model = enter PyTorch Lightning,Chunk 433: BBC Sport takes a look back at some of the
434,"By default, PyTorch code can get quite verbose. There is the model definition, This will help to make our code more portable and also improve organization. the training loop and the setup of the dataloaders. By default all this code is mixed These classes and functions will all be pulled into the main execution notebook, together, making it hard to swap data sets and models in and out, which can be via Srun, where the training hyperparameters will be defined and the code",Chunk 434: In this post we are going to look at how to make PyTorch code less verbose.
435,key for fast experimentation. actually executed. PyTorch Lightning helps to make this simpler by greatly reducing the boilerplate Model Class required to set up the experimental model and the main training loop. It is an,"Chunk 435: With PyTorch Lightning, it is now easier than ever to run experiments on"
436,"Model Type (ie Resnet/ViT) opinionated approach to structuring PyTorch code, which allows for more readable Execution Notebook Metrics Optimiser (ie (ie CrossEntropyl Adam / SGD) maintainable code.",Chunk 436: An overview of some of the key features of PyTorch
437,"Accuracy) Imports other modules For our project, we will break up the code into three main modules (ie Sets batch Hyperparameters size) Dataloaders Data Module (ie train/val/t Class test)",Chunk 437: The following code snippet will show you how to set up a
438,Loads Spark dataframes Transformations (ie random PyTorch Model Creates Petastorm crop),Chunk 438: Loads of Spark dataframes Transformations (ie random
439,DataLoaders and Transformations loader Train Loop Main Training Loop pl.Trainer Callbacks,Chunk 439: DataLoaders and Transformations loader Train Loop Main
440,Figure 3: Code layout databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 57 Model definition:,Chunk 440: BBC News takes a look at some of the key
441,"Main training loop: This module contains the code for the model architecture itself in a model class, This is the main training function. It takes the lightningPataModule and LightningModule. This is where the model architecture lives. For reference, this the LightningModule defining the model before feeding it into the Trainer","Chunk 441: This module contains the code for the model architecture itself in a model class, This is the main training loop."
442,"is the module that needs updating to leverage popular model frameworks like class. We will instantiate the PyTorch Lightning Trainer and define all necessary timm, HuggingFace and the like. This module will also contain the definitions for callbacks here. optimizers. In this case, we just use SGD, but it can be parameterized to test out","Chunk 442: In this tutorial, we will create a new module called Lightning Trainer."
443,"other types of optimizers. As we scale up the training process later on, we do not need some processes like MLflow logging to be run on all the processing nodes. As such, we will restrict DataLoader class: these to run on the first GPU only.",Chunk 443: We are going to restrict some of the MLflow processes to run on the first GPU only.
444,"Unlike with native PyTorch, where DataLoader code is intermixed with the model code, PyTorch Lightning allows us to split it out into a separate if device_id == 0: ughtningDataModule class. This allows for easier management of data sets and # we only need this on node 0",Chunk 444: PyTorch Lightning is a wrapper around PyTorch DataLoader.
445,"the ability to quickly test different interactions of your data sets. mltow.pyorch.autolog () When building a LightningDataModule with a Petastorm DataLoader, we feed in the spark_converter object rather than the raw spark dataframes. The Checkpointing our model during training is important for preserving progress, but",Chunk 445: Petastorm's DataLoader makes it easy to build Spark-like models from raw data
446,"Spark DataFrame is managed by the underlying Spark cluster, which is already will default PyTorch Lighting by handle this for us and we do not need to add code. distributed, whereas the PyTorch DataLoader will be distributed through other",Chunk 446: We are going to use Spark DataFrame instead of the PyTorch DataLoader to
447,means later. Follow along in the Building the PyTorch Lightning Modules notebook. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 58,"Chunk 447: In our series of letters from African journalists, filmmaker"
448,"def train_hvd) : Part 3 = Scaling the Training Job hvd.init () # MLflow setup for the worker processes While single-GPU training is much faster than CPU training, it is often not enough.",Chunk 448: This is the second part of a three-part series on how to
449,"mlflow. set_tracking.uri (""databricks"") oS. environ L DATABRICKS_HOST = db_host Proper production models can be large and the data sets required to train these os.environ! L DATABRICKS.TOMEN = db_token properly will be large too. Hence, we need to look into how we can scale our","Chunk 449: In this paper, we are going to look at how to scale up the data sets that can be"
450,"training across multiple GPUS. hvd_model = Latclasifcationkadel (class_count-5, learning_rate-le- 5*hvd.size (), device_id-hvd.rank (), device,count-hvd.sise ()) The main approach to distributing deep learning models is via data parallelism, hvd_datamodule = FlowersDataModule (train_converter, val_converter,","Chunk 450: The main approach to distributing deep learning models is via data parallelism, hv"
451,"device_id-hvd.rank (), device.count-hvd.size ()) where we send a copy of the model to each GPU and feed in different shards of # gpus' paramet ter here should be 1 because the parallelism is data to each. This lets us increase the batch size and leverage higher learning rates controlled by Horovod",Chunk 451: Here's an example of how to send a model to multiple GPUs at the same time.
452,"to improve training times as discussed in this article. return train (hvd model, hvd_datamodule, gpus=1, strategy--horovoa"", device.ld-hvd.ranko, device.count-hd.asise ()) To assist us in distributing the training job across GPUS, we can leverage Horovod. Horovod is another Linux Foundation project that offers us an alternative to","Chunk 452: In this article, we will look at how the Linux Foundation's Horovod project can be"
453,This function will start Horovod hvd.init () and ensure that our DataModule and manually triggering distributed PyTorch processes across multiple nodes. train function are triggered with the correct node number hvd.rank () and total Databricks Runtime for Machine Learning includes by default the HorovodRunner number of devices hvd.size (). As discussed in this Horovod article we scale up,Chunk 453: This function will start Horovod hvd.init () and ensure that our Databricks manually triggering PyTorch processes across multiple
454,"class, which helps us scale on both single-node and multi-node training. the learning rate with the number of GPUS. In order to leverage Horovod, we need to create a new ""super"" train loop. hvd_model = LAtClasifcationkodal (class_ count=5, learning_rate-le- 5*hvd.size (), device.id-hvd.rank 0, device_ int-hvd.size ())",Chunk 454: We've been using Horovod to train our neural networks.
455,Then we return the normal train loop with the GPU count set to 1as Horovod is handling the parallelism. Follow along in the Main Execution notebook and we will go through the ways to go from single- to multi-GPU. databricks,Chunk 455: We start with a single-GPU train loop with the GPU count set to 0.
456,"EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 59 Step 1- Scaling on one node In our code, we set the default_dir parameter to a DBFS location in the train function. This is where PyTorch Lightning will save out the checkpoints. If we set a",Chunk 456: In this post we are going to show you how to set up a python script to run on a Raspberry
457,"ckpt_ restore path to point to ckpt, the train function will resume training from that checkpoint. Driver Node FullBatch -","Chunk 457: ckpt_restore path to checkpoint, the"
458,"V def train (model, dataloader, gpus:int-0, strategy :str-None, device_ id:int=0, Batch Batch Batch",Chunk 458: The BBC's James Reynolds looks at some of
459,"Shard1 Shard2 Shardn device_ count:int-1, logging evel-loging.INPO, default dir: CI dbfs, CI trainei logs ckpt restoreistr-None,","Chunk 459: device_count:int-1, logging evel-"
460,"GPU1 GPU1 (usuallyr GPUn mllow.experiment_id:str-lone) out at Kore To scale out our train function to multiple GPUS on one node, we will use","Chunk 460: In this tutorial, we will show you how to"
461,"HorovodRunner: Figure 4: Single-node scaling from sparkdl import HorovodRunner hr = HorovodRunner (np=-4, driver_log.verbosity-'all) Scaling on one node is the easiest way to scale. It is also very performant, as it","Chunk 461: In this article, I will show you how to scale a single"
462,"hvd_model = hr.run (train_hvd) avoids the network traffic required for multi-node training. Unlike Spark-native ML Libraries, most deep learning training processes do not automatically recover Setting np to negative will make it run on the single driver node with 4 GPUS. from node failures. PyTorch Lightning, however, does automatically save out",Chunk 462: PyTorch Lightning supports multi-node deep learning training.
463,A positive np value will spread the training across other worker nodes. checkpoints for recovering training epochs. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 60,Chunk 463: The purpose of this study is to assess the impact of training
464,"Step 2 - Scaling across nodes Databricks runtime version 0 Runtime: 10.4 LTS ML (GPU, Scala 2.12, Spark 3.2.1) NVIDIA EULA 0 Use Photon. Acceleration 0 Preview",Chunk 464: Step 2 -Scaling across nodes Databricks version 0
465,Worker Node1 Worker Node 2 Worker Node n Use your own Docker container 0 Full Batch,Chunk 465: Worker Node1 Worker Node 2 Worker Node n Use
466,Autopilot options - Enable autoscaling 0 Batch Batch Batch Batch,Chunk 466: Check out our selection of the best of the best
467,Batch Batch Enable autoscaling local storage 0 Split1 Split 2 Split3 Split4 Splitn Split n+l,Chunk 467: BBC Sport tracks all of the key stories from the
468,V V M Terminate after 50 minutes ofi inactivity 0 Worker type 0 Workers,Chunk 468: BBC Sport looks at some of the key talking points
469,"GPU1 GPU1 GPU3 GPU4 GPUn GPUn+1 g4dn.4xlarge 64GB Memory, 1 GPU V B","Chunk 469: Nvidia has released details of its latest graphics cards,"
470,"Driver type g4dn.4xlarge 64 GB Memory, 1 GPU V Figure 5: Multi-node scaling DBU /hour: 25.65 0",Chunk 470: GPU V Figure 5: Multi-node scaling DBU
471,g4dn.4xlarge We have already wrapped our training function with a Horovod wrapper and we Advanced options have already successfully leveraged HorovodRunner for single-node multi-GPU processing. The final step is to go to a mutt-node/muti-GPU setup. If you have,Chunk 471: In this post we are going to look at how to use HorovodRunner to
472,"Figure 6: Multi-node cluster setup been following along with a single-node cluster, this is the point where we will move to a multi-node cluster. For the code that follows, we will use the cluster When running distributed training on Databricks, autoscaling is not currently configuration shown at right:",Chunk 472: In this article we will be running distributed training on Databricks using a multi-node cluster.
473,"supported, SO we will set our workers to a fixed number ahead of time. hr = HorovodRunner (np=8, driver_log.verbosity-'all) hvd_model = hr.run (train_hvd) databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION",Chunk 473: We are looking for an enthusiastic and self-motivated individual to join our team
474,61 A common problem that will occur as you scale up your distributed deep learning Analysis job is that the Petastorm table has not been partitioned well enough to ensure that all the GPUS get a batch split. We need to make sure that we have at least as many,"Chunk 474: In this article, I will show you how to partition the Petastorm table."
475,"When training deep neural networks, it is important to make sure we do not data partitions as we have GPUS. overfit the network. The standard way to manage this is to leverage early stopping. We address this in our code by setting the number of GPUS in the prepare data This process checks to make sure that with each epoch, we are still seeing function with the num devices variable.",Chunk 475: In this talk we will show how to train a deep neural network using GPUS.
476,"improvements to the metric that we set it to monitor. In this case, val_loss. For our experiments, we set min_ delta to 0.01, SO we expect to see at least 0.01 flowers_df, train_converter, val_converter = prepare_data (data_dir-Data Directory, .derierUKDEVICE) improvement to val_ loss each epoch. We set patience to be 10 SO the train","Chunk 476: For our experiments, we set min_ delta to 0.01, so we expect to see at least 0.01 flowers_df, train"
477,"loop will continue to run up to 10 epochs of no improvement before the training datamodule = FlowersDataModule (train_ C train converter, val_converter val converter) stops. We set this to make sure that we can eke out the last drop of performance.",Chunk 477: loop will continue to run up to 10 epochs of no improvement before the training datamodule =
478,"To keep the experimentation shorter, we also set a stopping.threshold of 0.55 SO we will stop the training process once our val_ loss drops below this level. This simply calls a standard Spark repartition command. We set the number of partitions to be a multiple of the num devices, the number of GPUS, to make sure With those parameters in mind, the results of our scaling experiments are as that the data set has sufficient partitions for all the GPUS we have allocated for the follows:",Chunk 478: We set the val_ loss to be the number of partitions we have allocated for our data set.
479,training process. Insufficient partitions is a common cause of idling GPUS. Running Time VS Cluster Setup flowers_dataset = flowers_dataset.repartition (num_devices*2) 40 30,Chunk 479: Insufficient partitions are a common cause of idling GPU
480,20 I 10 0 1GPU Single Node 2GPU Single Node 4GPU Single Node 8GPU Multi Node,Chunk 480: GPU Single Node 2GPU Single Node 4GPU Multi
481,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 62 Get started Val Loss (Lower is Better) VS Cluster Setup,Chunk 481: BBC News takes a look at some of the key
482,"0.8 We have shown how you can leverage PyTorch Lightning within Databricks and wrap it with the HorovodRunner to scale across multiple nodes, as well as 0.6 provided some guidance on how to leverage EarlyStopping. Now it's your turn",Chunk 482: This week we are looking at how you can use PyTorch Lightning within Databricks
483,to try. 3 0.4 a - 0.2,Chunk 483: A selection of photos from around the world this week
484,Notebooks: - Exploring the flowers dataset * 0.0 1 GPU Single Node 2GPU Single Node 4 GPU Single Node 8GPU Multi Node,Chunk 484: Researchers at the Massachusetts Institute of Technology (MIT)
485,"Building the PyTorch Lightning Modules * Main Execution Notebook * As we can see, in the Running Time VS Cluster Setup chart, we nearly halved the training time as we increased the system resources. The scaling is not quite linear, which is due to the overhead of coordinating the training process across",Chunk 485: We've been building the PyTorch Lightning for the past few months and have been able to significantly reduce the time it
486,"See Also: different GPUS. When scaling deep learning, it is common to see diminishing HorovodRunner * returns and hence it is important to make sure that the train loop is efficient prior to adding GPUS.",Chunk 486: HorovodRunner returns true if the train loop is efficient and false if
487,"Petastorm * Deep Learning Best Practices * That is not the full picture, however, as per the best practices advised in our How (Not) to Scale Deep Learning * previous blog article, How (Not) To Scale Deep Learning in 6 Easy Steps, we used","Chunk 487: In our series of letters from African journalists, film-maker and columnist Petastorm looks at some"
488,"Leveling the Playing Field: HorovodRunner for Distributed EarlyStopping hence it is important to check the final validation loss achieved Deep Learning Training * by the various training runs as well. In this case, we set the stopping_threshold of 0.55. Interestingly, the single-GPU setup stopped at a worse validation loss","Chunk 488: In this paper, we set the stopping_threshold of HorovodRunner for Distributed EarlyStopping to 0."
489,than the multi-GPU setups. The single-GPU training ran till there were no more improvements in the val loss. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 63,Chunk 489: This study compared the performance of single-GPU and multi-GPU
490,"CHAPTER 8: Processing Geospatial The evolution and convergence of technology has fueled a vibrant marketplace for timely and accurate geospatial data. Every day, billions of handheld and loT devices along with thousands of airborne and Data at Scale With",Chunk 490: Geospatial technology is changing the way we work and live.
491,satellite remote sensing platforms generate hundreds of exabytes of location-aware data. This boom of Databricks geospatial big data combined with advancements in machine learning is enabling organizations across industries to build new products and capabilities. FRAUD AND ABUSE,Chunk 491: Big data analytics is transforming the way we work and live.
492,"RETAIL FINANCIAL SERVICES HEALTHCARE Detect patterns of fraud and Site selection, urban planning, Economic distribution, loan risk Identifying disease epicenters, collusion (e.g. claims fraud,",Chunk 492: An investigation by the Independent Police Complaints Commission (I
493,"foot traffic analysis analysis, predicting sales at environmental impact on credit card fraud) retail, investments",Chunk 493: BBC News takes a look at some of the key
494,"health, planning care DISASTER RECOVERY DEFENSE AND INTEL INFRASTRUCTURE ENERGY",Chunk 494: Health Secretary Jeremy Hunt has defended the government's
495,"By Nima Razavi and Michael Johns Flood surveys, earthquake Reconnaissance, threat Transportation planning, Climate change analysis, energy mapping, response planning detection, damage assessment agriculture management, asset inspection, oil discovery",Chunk 495: A selection of some of the key stories from around the world
496,"housing development Maps leveraging geospatial data For example, numerous companies provide localized drone-based services such as mapping and site are used widely across industries, inspection (reference Developing for the Intelligent Cloud and Intelligent Edge). Another rapidly growing spanning multiple use cases, including disaster recovery, defense and intel, industry for geospatial data is autonomous vehicles. Startups and established companies alike are amassing",Chunk 496: The use of geospatial data in a wide range of applications is growing rapidly.
497,"infrastructure and health services. large corpuses of highly contextualized geodata from vehicle sensors to deliver the next innovation in self- driving cars (reference Databricks fuels wejo's ambition to create a mobility data ecosystem). Retailers and government agencies are also looking to make use of their geospatial data. For example, foot-traffic analysis (reference Building Foot-Traffic Insights Data Set) can help determine the best location to open a new store or, in the public sector, improve urban planning. Despite all these investments in geospatial data, a",Chunk 497: Geospatial data has become an increasingly important part of our daily lives.
498,number of challenges exist. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 64 Challenges analyzing geospatial at scale,"Chunk 498: In our series of letters from African journalists, filmmaker"
499,The first challenge involves dealing with scale in streaming and batch applications. Compatibility with various spatial formats poses the second challenge. There are The sheer proliferation of geospatial data and the SLAS required by applications many different specialized geospatial formats established over many decades as overwhelms traditional storage and processing systems. Customer data has been well as incidental data sources in which location information may be harvested:,Chunk 499: Location-based services (LBS) are facing two major challenges.
500,"spilling out of existing vertically scaled geodatabases into data lakes for many Vector formats such as GeoJSON, KML, shapefile and WKT years now due to pressures such as data volume, velocity, storage cost and strict schema-on-write enforcement. While enterprises have invested in geospatial data, Raster formats such as ESRI Grid, GeOTIFF, JPEG 2000 and NITF",Chunk 500: This report looks at the changing landscape of geospatial data storage.
501,"few have the proper technology architecture to prepare these large, complex data Navigational standards such as used by AIS and GPS devices sets for downstream analytics. Further, given that scaled data is often required for Geodatabases accessible via JDBC/ODBC connections such as advanced use cases, the majority of Al-driven initiatives are failing to make it from",Chunk 501: This report looks at the challenges faced by the oil and gas industry in accessing and storing large volumes of data.
502,"PostgresQU/Postais pilot to production. Remote sensor formats from hyperspectral, multispectral, lidar and radar platforms OGC web standards such as WCS, WFS, WMS and WMTS",Chunk 502: The European Space Agency (ESA) and the French
503,"Geotagged logs, pictures, videos and social media Unstructured data with location references In this blog post, we give an overview of general approaches to deal with the two main challenges listed above using the Databricks Unified Data Analytics Platform. This is the first part of a series of blog posts on working with large volumes of",Chunk 503: How do you deal with large volumes of unstructured data?
504,geospatial data. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 65 Scaling geospatial workloads with Databricks,Chunk 504: A look at some of the key findings from the
505,"Databricks offers a unified data analytics platform for big data analytics and 2. Wrapping single-node libraries such as GeoPandas, Geospatial Data machine learning used by thousands of customers worldwide. It is powered by Abstraction Library (GDAL) or Java Topology Suite (JTS) in ad hoc user- Apache Spark, Delta Lake and MLflow with a wide ecosystem of third-party and","Chunk 505: Databricks, a leading data analytics platform for big data analytics and machine learning, has announced the launch of Databricks Geo,"
506,"defined functions (UDFS) for processing in a distributed fashion with Spark available library integrations. Databricks UDAP delivers enterprise-grade security, DataFrames. This is the simplest approach for scaling existing workloads support, reliability and performance at scale for production workloads. Geospatial without much code rewrite; however, it can introduce performance","Chunk 506: Databricks UDAP is a distributed, open-source Geospatial processing platform."
507,"workloads are typically complex, and there is no one library fitting all use cases. drawbacks as it is more lift-and-shift in nature. While Apache Spark does not offer geospatial Data Types natively, the open source community as well as enterprises have directed much effort to develop 3. Indexing the data with grid systems and leveraging the generated index to",Chunk 507: Apache Spark is an open source software platform that allows developers to query large amounts of data.
508,"spatial libraries, resulting in a sea of options from which to choose. perform spatial operations is a common approach for dealing with very large-scale or computationally restricted workloads. S2, GeoHex and Uber's There are generally three patterns for scaling geospatial operations such as spatial H3 are examples of such grid systems. Grids approximate geo features such","Chunk 508: In our series of letters from African journalists, film-maker and columnist Ahmedou Ould-Abdallah looks at some of"
509,"joins or nearest neighbors: as polygons or points with a fixed set of identifiable cells, thus avoiding expensive geospatial operations altogether, and thus offer much better 1. Using purpose-built libraries that extend Apache Spark for geospatial scaling behavior. Implementers can decide between grids fixed to a single","Chunk 509: Apache Spark is an open-source framework for building web services, and one of its key features is its"
510,"analytics. GeoSpark, GeoMesa, GeoTrellis and RasterFrames are a few of accuracy that can be somewhat lossy yet more performant or grids with such libraries used by our customers. These frameworks often offer multiple accuracies that can be less performant but mitigate against lossines. multiple language bindings and have much better scaling and performance",Chunk 510: In this talk we will be looking at the performance benefits of using frameworks with multiple accuracies.
511,"than non-formalized approaches, but can also come with a learning curve. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 66 The following examples are generally oriented around a New York City taxi",Chunk 511: Transport for London (TfL) and the Department for Transport (DfT
512,Geospatial operations using geospatial pickup/drop-off data set found here. NYC Taxi Zone data with geometries will libraries for Apache Spark also be used as the set of polygons. This data contains polygons for the five boroughs of NYC as well the neighborhoods. This notebook will walk you through,Chunk 512: New York City (NYC) Taxi Zone data will be used to create a 3D model of New York City
513,"Over the last few years, several libraries have been developed to extend the preparations and cleanings done to convert the initial CSV files into Delta Lake capabilities of Apache Spark for geospatial analysis. These frameworks bear the tables as a reliable and performant data source. brunt of registering commonly applied user-defined types (UDT) and functions",Chunk 513: Apache Spark is an open-source framework for building web applications.
514,"Our base DataFrame is the taxi pickup/drop-off data read from a Delta Lake Table (UDF) in a consistent manner, lifting the burden otherwise placed on users and using Databricks. teams to write ad hoc spatial logic. Please note that in this blog post, we use several different spatial frameworks chosen to highlight various capabilities. We éscala","Chunk 514: In this blog post, I'll show you how we've developed a new way to retrieve and store taxi and drop-"
515,understand - that other frameworks exist those which,Chunk 515: BBC Sport takes a look at some of the key
516,"val dfRaw spark.read.format: (""delta"").load""/ml/blogs/geospatial/ beyond highlighted, you might delta/nyc-green""? also want to use with Databricks to process your spatial workloads.",Chunk 516: Do you have a dataset that you would like to
517,"display (dfRaw) // showing first 10 columns Earlier, we loaded our base data into a DataFrame. Now we need to turn the latitude/longitude attributes into point geometries. To accomplish this, we will pickup datetim dropolf latetime mes 2017-09-30235743","Chunk 517: In this tutorial, we are going to learn how to display the first 10 columns of"
518,use UDFS to perform operations on DataFrames in a distributed fashion. Please mmas 2017-09-3023.5530 N refer to the provided notebooks at the end of the blog for details on adding these mes 2017-09-30233729 N frameworks to a cluster and the initialization calls to,Chunk 518: The following examples show how to use UDFS to perform operations on DataFrames in a distributed
519,UDFS and N register UDTS. For mes 2017-09-3023.54.59,Chunk 519: The United Nations Development Programme (UNDP) has
520,"2017-09-30 2017-09-302331.49 N 89 2.35 starters, we have added GeoMesa to our cluster, a framework especially adept thef first 1000rows.","Chunk 520: We have added a new framework to our cluster,"
521,"at handling vector data. For ingestion, we are mainly leveraging its integration of JTS with Spark SQL, which allows us to easily convert to and use registered JTS Figure 1: Geospatial data read from al Delta Lake table using Databricks geometry classes. We will be using the function st_makePoint that, given a latitude and longitude, create a Point geometry object. Since the function is a UDF, we can","Chunk 521: In this article, we will be using Databricks to ingest and store geospatial data from al Delta Lake table."
522,"apply it to columns directly. Sscala val df = dfRaw .withColumn Cpickup.point"", st_makePoint (col Cpichup.longitude?, col (""pickup latitude"")))",Chunk 522: This Choreo shows how to pick up a column
523,"withColumn (""dropoff_point"" st makePoint (col (""dropoff longitude""),col (""dropof: atitude""))) display (df.select (""dropoff_point"" dropotf.datetime?) databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION",Chunk 523: View Khajuraho Khajuraho Khajuraho Khajuraho Khajuraho Khajuraho Khajuraho Khajuraho Khajuraho
524,67 (2) SparkJobs Wrapping single-node libraries in UDFS Figure 2: Using UDFS to perform dropoff.point,Chunk 524: The following examples show how to use UDFS to
525,dropoft. datetime operations on DataFrames in POINT 73.98411560058594 40.695980072021484) 2016-04-010 00:05:53 a distributed fashion to turn In addition to purpose-built distributed,Chunk 525: datetime operations on DataFrames in 73.
526,"POINT (73.8504409790039. 40.724143981933594 2016-04-01 00:05:55 using spatial frameworks, existing single- geospatial data latitudellongitude Showingi thefi first 10001 rows.","Chunk 526: Researchers at the University of California, Los Angeles,"
527,"attributes into point geometries. node libraries can also be wrapped in ad hoc UDFS for performing geospatial operations on DataFrames in a distributed fashion. This pattern is available to all Spark language bindings - Scala, Java, Python, R and SQL = and is a can also",Chunk 527: node libraries can be wrapped in ad hoc UDFS for performing geospatial operations on DataFrames in a distributed
528,"simple We perform distributed spatial joins, in this case using GeoMesa's approach for leveraging existing workloads with minimal code changes. To provided st_contains UDF to produce the resulting join of all polygons against",Chunk 528: GeoMesa's spatial join library provides an easy-to-
529,"demonstrate a single-node example, let's load NYC borough data and define pickup points. UDF find_boroughC-) for point-in-polygon operation to assign each GPS location éscala to a borough using geopandas. This could also have been accomplished with a","Chunk 529: In this talk, I'm going to show you how to use UDF"
530,"val joinedDF = wktDF.join (df, st_contains (S""the_geom"", ""pickup.point"") display GoinedDF.select: (""zone"" ""borough"" Pichp.poinePichp vectorized UDF for even better performance datetime"")) Spython","Chunk 530: display GoinedDF.select: (""zone"
531,"# read the boroughs polygons with geopandas (2) Spark. Jobs gdf = gdp.read_file C/dbfs/ml/blogs/geospatial/nyc_boroughs. geojson"") zone borough pickup.point",Chunk 531: # read the boroughs with geopandas (2) Spark
532,pickup.datetime Fort Greene Brooklyn POINT 73.98098486084453- 40.689029693603516 2016-06-09 10:35:08 b_gdf = sc.broadcast (gdf) # broadcast the geopandas dataframe to all,Chunk 532: The BBC's geopanda service is broadcast live
533,Crown Heights North Brooklyn POINT 73.95674896240234 40.67413330078125) 2016-06-09 10:42:15 nodes of the cluster Brooklyn Heights,Chunk 533: All photographs courtesy of the New York City Department of
534,"Brooklyn POINT (73.9929428100586 40.69749069213867 2016-06-09 10:47:38 def find_borough (latitude, longitude): Brooklyn Heights Brooklyn POINT 73.991172790527344 40.6959114074707","Chunk 534: Brooklyn Heights is a borough in New York City,"
535,"2016-06-09 10:46:09 mgdf b.gdf.value-apply (lambda x: x[""boro_name"") if xI'geometry""l. Williamsburg (South Side) Brooklyn POINT (73.98204376220703- 40.70991516113281), 2016-06-09 10:06:12",Chunk 535: New York City is one of the world's
536,"intersects (Point (longitude, latitude)) East HarlemNorth Manhattan POINT (73.93933868408203- 40.80525207519531) 2016-06-09 10:58:19 idx = mgdf .first_valid index ()",Chunk 536: New York City is the most densely populated city in
537,Steinway Queens POINT (73.9175796508789 40.7569954681398484) 2016-06-091 10:45:41 return mgdf.loclidx) if idx is not None else None Morningsidel Heights,Chunk 537: A selection of photos from around New York City on
538,"Manhattan POINT 73.96385192871094 40,80808639526367 2016-06-09 10:36:34 Showing the first 1000r rows. Doi n01e",Chunk 538: Footage has emerged of the moment the world's
539,"findborough_udf = udf (find_borough, StringType ()) Figure 3: Using GeoMesa's provided st_contains UDF, for example, to produce the resulting join of all polygons against pickup points databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION",Chunk 539: Figure 2: Using GeoMesa's UDF to find points on a map
540,"68 Now we can apply the UDF to add a column to our Spark DataFrame, which assigns Grid systems for spatial indexing a borough name to each pickup point. Geospatial operations are inherently computationally expensive. Point-in-polygon, Spython",Chunk 540: We've been using the Open Geospatial Consortium's Unified Data Format (U
541,"# read the coordinates from delta spatial joins, nearest neighbor or snapping to routes all involve complex operations. df = spark.read.format: rdelta.loadcmi/Bloga/geospatial/delta/nyc- By indexing with grid systems, the aim is to avoid geospatial operations altogether. green"")",Chunk 541: The following code is part of a larger project by the New York City Department of
542,"df with_boroughs = df.withColumn Cpickup.borough"", find_borough This approach leads to the most scalable implementations with the caveat of udf (col CPichp.atita-7eat (pickup.longitude)) approximate operations. Here is a brief example with H3. display (df with boroughs.select:",Chunk 542: The simplest way to display a list of boroughs is to use df (
543,"""pickup_ datetime ""pickup_ latitude"" pickup.longitude"" ""pickup_ borough"")) Scaling spatial operations with H3 is essentially a two-step process. The first step is to compute an H3 index for each feature (points, polygons, ) defined as UDF geoTOH3(.). The second step is to use these indices for spatial operations such",Chunk 543: The following examples show how to scale spatial operations with H3 using UDF geoTOH3(.
544,"(2)Spark.Jobs as spatial join (point-in-polygon, k-nearest neighbors, etc.), in this case defined as pickup. datetime pickup.Jatitude pickup.Jongitude",Chunk 544: (1) Spark.Jobs as spatial join (point-
545,pickup. borough UDF mutiPolygonfoH34.. 2016-04-01 00:06:39 40.718135833740234 -73.9595108032266,Chunk 545: A man has been arrested in connection with the theft
546,Manhattan 2016-04-01 00:06:28 40.86066818237305 -73.889864080810547 Manhattan,Chunk 546: New York City Mayor Bill de Blasio and his wife
547,2016-04-01 00:07:25 40.73863983154297 -73.88591766357422 Manhattan 2016-04-01 00:09:44,Chunk 547: New York City Mayor Bill de Blasio and his wife
548,40.69947814941406 -73.92366790771484 Manhattan 2016-04-01 00:16:02 40.691192626953125,Chunk 548: New York City Mayor Bill de Blasio and his wife
549,-73.9872055053711 Manhattan 2016-04-01 00:14:52 40.761085510253906 -73.92341613769531,"Chunk 549: The world's tallest building, the Burj Khalifa"
550,Manhattan 2016-04-01 00:11:00 40.686092376708984 -73.97399139404297 Manhattan,"Chunk 550: All photographs courtesy of AFP, EPA, Getty Images"
551,2016-04-01 00:17:17 40.79181671142578 -73.944580078125 Manhattan Emgnam rows.,Chunk 551: New York City Mayor Bill de Blasio and his wife
552,"Figure 4: The result of a single-node example, where GeoPandas is used to assign each GPS location to an NYC borough databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 69","Chunk 552: New York City is one of the most densely populated cities in the world,"
553,"éscala val multiPolygonToH3 = udf! (geometry: Geometry, resolution: Int) => import com.uber.h3core.H3Core var points: List[GeoCoord) = List () import com.uber.h3core.util.GeoCoord",Chunk 553: var points: List () import com.
554,"var holes: List [java.util.List [GeoCoord]] = List () import scala.collection..Javaconversions. if (geometry-getceometryType == ""MultiPolygon"") ( import scala. collection.JavaConverters. val numGeometries = ESADNCtIN ()",Chunk 554: java.lang.Illegal.geometry.get
555,if (numGeometries > 0) ( object H3 extends Serializable ( points = List ( val instance = H3Core.newInstance () geometry,Chunk 555: If (Object H3) extends Serializable ( points
556,"getGeometryN (0) getCoordinates () val geoTOH3 = udf! (latitude: Double, longitude: Double, resolution: toList Int) =>",Chunk 556: The longitude and latitudes of the Faroe Islands are
557,".map (coord => new GeoCoordicoord.y, coord.x)): H3.instance.georon3 (latitude, longitude, resolution) ) if (numGeometries > 1) ( holes = (1 to (numGeometries - 1)).toList.map (n => (",Chunk 557: A map of the world's landmasses has been
558,"val polygonTOH3 = udf! (geometry: Geometry, resolution: Int) => List( var points: List[GeoCoord) = List() geometry var holes: Listljava.util.List/Geocoordl) = List()",Chunk 558: polygonTOH3 = udf! (ge
559,"-getGeometryN (n) if (geometry-getceometryType == ""Polygon"") ( getCoordinates () points = List ( .toList",Chunk 559: -getGeometryN (n) if (
560,"geometry .map (coord => new GeoCoord (coord.y, coord.x)): *).asJava getCoordinates () 1) toList",Chunk 560: Java class geometry .map (coord new
561,") .map (coord => new GeoCoord (coord.y, coord.x)): *) ) H3.instance.polylll (points, holes.asJava, resolution) .toList H3.instance.polynll (points, holes.asJava, resolution).tolist",Chunk 561: The following code has been compiled and tested by the
562,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 70 We can now apply these two UDFS to the NYC taxi data as well as the set of éscala,Chunk 562: New York City's Taxi and Limousine Commission (TLC) has
563,"val dfwithBorougha3 = dfH3.join WAtDFB3,""A3index? borough polygons to generate the H3 index. display (df_with_borough._h3.select (""zone"", ""borough' "", ""pickup_ éscala point"", ""pickup.datetime"", ""h3index""))",Chunk 563: display (df_with_borough._h
564,"val res = 7 //the resolution of the H3 index, 1.2km val dfH3 = df.withColumn ( ""h3index"", geoTOH3 (col (""pickup latitude""), col (""pickup_ longitude""), lit (res)) )","Chunk 564: The resolution of the H3 index, a measure of"
565,(1) SparkJobs val wktDFH3 wktDF zone borough pickup point pickup. datetime h3index,Chunk 565: Find out how to get a job with the Spark
566,"withColumn (""h3index"", multiPolygonToH3 (col (""the_geom""), lit (res))) Morningsidel Heights Manhattan POINT 73.95296478271484 40.80758285522461) 2016-06-09 10:14:34 61322962300085247 withColumn (""h3index"", explode (S""h3index"")) Central Harlem",Chunk 566: A massive explosion has ripped through a New York City
567,Manhattan POINT (73.94908905029297 40.80293655395508) 2016-06-09 10:04:08 613229523028148223 Brooklyn Heights Brooklyn POINT (73.99422454833984 40.69488525390625) 2016-06-09 10:52:24 61322955141100391 Van Nest/Moris Park Bronx POINT 73.84475708007812 40.847774505615234) 2016-06-091 10:23:52 61329520937287679,Chunk 567: All photographs courtesy of the New York City Department of Transportation
568,Astoria Queens POINT (73.9139633178711 40.76524353027344) 2016-06-09 10:25:38 613229624726841343 Morningsidel Heights Manhattan POINT 673.95944213867188 40.8091239291992) 2016-06-09 10:42:56 61322962300085247 Park Slope,Chunk 568: All photographs courtesy of the New York City Department of
569,"Brooklyn POINT 73.98164367675781 40.6669464113281) 2016-06-09 10:29:28 613229852660905983 Park Slope Brooklyn POINT (7397588348388672 40.67397689819336) 2016-06-09 10:53:01 61329562669294591 Given a set of lat/lon points and a set of polygon geometries, it is now possible Ln Mlanl",Chunk 569: New York City's Brooklyn Bridge is one of the world's most
570,n. Showingt thet first 1000rows. to perform the spatial join using h3index field as the join condition. These assignments can be used to aggregate the number of points that fall within each Figure 5: DataFrame table representing the spatialj join of a set of lat/lon points and,Chunk 570: These assignments can be used to aggregate the number of points that fall within each Figure 5: DataFrame table representing
571,"polygon, for instance. There are usually millions or billions of points that have to polygon geometries, using a specific field as the join condition be matched to thousands or millions of polygons, which necessitates a scalable approach. There are other techniques not covered in this blog that can be used for indexing in support of spatial operations when an approximation is insufficient.",Chunk 571: In this blog I'm going to look at some of the techniques that can be used for indexing in support of spatial operations when an approximation is
572,"databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 71 Here is a visualization of taxi drop-off locations, with latitude and longitude Handling spatial formats with Databricks",Chunk 572: BBC News takes a look at some of the key stories of
573,"binned at a resolution of 7 (1.22km edge length) and colored by aggregated counts within each bin. Geospatial data involves reference points, such as latitude and longitude, to physical locations or extents on the Earth along with features described by attributes. Nev",Chunk 573: The following table contains the most up-to-date geospatial data for Nevada.
574,"While there are many file formats to choose from, we have picked out a handful of Canaan uffern Spring Valley Norwalk representative vector and raster formats to demonstrate reading with Databricks.","Chunk 574: With Databricks, it is easy to create, share and"
575,Darien STAMFORD Vector data Oakland Wyckoff,Chunk 575: A look back at some of the most memorable moments
576,"Vector data is a representation of the world stored in X (longitude), y (latitude) coordinates in degrees, and also Z (altitude in meters) if elevation is considered. Wayne PATERSON The three basic symbol types for vector data are points, lines and polygons.","Chunk 576: In our series of letters, we look at some of the most common symbols for vector data."
577,"Well-known-text (WKT), GeoJSON and shapefile are some popular formats for Clifton en Cove Oyster Bay HUNTINGTON storing vector data we highlight below. Let's read NYC Taxi Zone data with geometries stored as WKT. The data structure",Chunk 577: Vector data can be stored in a variety of formats.
578,"we want to get back is a DataFrame that will allow us to standardize with other APIs Hempstead es and available data sources, such as those used elsewhere in the blog. We are able Union",Chunk 578: We want to make it easier for you to read our blog.
579,Lin to easily convert the WKT content found in field the_geom into its ELIZAE corresponding Freeport,Chunk 579: This page displays the WKT content found in the
580,"JTS Geometry class through the st_geomFromXT) UDF call. ea éscala val wktDFText = gContest.read.forms t (""csv"") .option (""header"", ""true"")",Chunk 580: val wktDFText = gContest.forms
581,"option (""inferSchema "", ""true"") loadc/ai/Bloga/goepatial/nyesax, zones.wkt.csv"") Figure 6: Geospatial visualization of taxi drop-off locations, with latitude and longitude binned at a resolution of 7( (1.22km edgel length) and colored by aggregated counts within each bin val wktDF = WAEDFTeXtE.MithCOlumA (""the_geom"", st_geomFi romWKT (col (""the_",Chunk 581: Figure 6: Geospatial taxi drop-off locations with latitude and longitude.
582,"geom"")).cache databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 72 GeoJSON is used by many open source GIS packages for encoding a variety of",Chunk 582: The following is a list of some of the most popular
583,"From there, we could choose to hoist any of the fields up to top level columns using geographic data structures, including their features, properties and spatial extents. Spark's built-in explode function. For example, we might want to bring up geometry, For this example, we will read NYC Borough Boundaries with the approach taken properties and type and then convert geometry to its corresponding JTS class, as depending on the workflow. Since the data is conforming to JSON, we could use","Chunk 583: In this lesson, we will use Spark's built-in explode function to create a map of the boroughs of New York City."
584,was shown with the WKT example. the Databricks built-in JSON reader with optioncmutlineuue) to load the data Spython with the nested schema. from pyspark.sql import functions as F,Chunk 584: In this article I will show how to use the Databrick
585,"json_explode_df = ( json_df.select Spython ""features"", json_df = spark.read.option (""multiline"", ""true"")-json (""nyc_boroughs. ""type"",",Chunk 585: New York City is one of the world's
586,"geojson"") F.explode (F.col CFeatures.properties?).alias (""properties"") ). select ( F.explode (F. col : features.geometry""7).alias (""geometry"")). drop (""features"")) D json_df: PpdtmDaNfens",Chunk 586: F.explode (F.col CFeatures
587,asplydan.-ploa.an features: array V element: struct geometry: struct coordinates: array,Chunk 587: Asplydan.-ploa.an is
588,type properties geometry element: array FeatureCollection wobject wobject,Chunk 588: A selection of some of the quirkier snippets from
589,"element: array boro_code:2 coordinates I-73.89680885 357), element: array",Chunk 589: BBC Sport takes a look at some of the key
590,"boro name: Bronx [-73.8972 038 843939 994), element: double shape_area: 1186612476.97",Chunk 590: The name of the New York City borough of Bronx
591,"173.89857332865558, 73.8919434249981, ,40 960691 40 79634). type: string shape_Jeng: 462958.186921 73.897882532",Chunk 591: shape_Jeng: 462958.186
592,properties: struct -73.896 boro_code: string boro_name: string 3.8,Chunk 592: properties: struct -73.896 boro
593,shape_area: string -73.8883725137 shape_leng: string type: string type: string,Chunk 593: A selection of the most popular strings from the BBC
594,"Figure 8: Using the Spark's built-in explode function to raise a field to the top level, displayed within al DataFrame table Figure 7: Using the Databricks built-in JSON reader opioncmutlinetue), to load the data with the nested schema databricks",Chunk 594: Figure 8: Using the Spark's built-in explode function to raise a field to
595,"EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 73 We can also visualize the NYC Taxi Zone data within a notebook using an existing DataFrame or directly rendering the data with a library such as Folium, a Python library for rendering spatial data. Databricks File System (DBFS) runs over a",Chunk 595: New York City's Taxi and Limousine Commission (TLC) is using Webrick's DataFrames to store and
596,"distributed storage layer, which allows code to work with data formats using familiar file system standards. DBFS has a FUSE Mount to allow local API calls that perform file read and write operations, which makes it very easy to load data with non-distributed APIs for interactive rendering. In the Python open(..) command Figure 9: We can also visualize the NYC Taxi Zone data, for example, within a","Chunk 596: In this tutorial, we'll look at how to use the Django Big Data Server (DBFS) to store and query large amounts of data."
597,"below, the ""/dbfs/."" prefix enables the use of FUSE Mount. notebook using an existing DataFrame or directly rendering the data with al library such as Folium, al Python library for rendering geospatial data Spython import folium","Chunk 597: The ""/dbfs/"" prefix enables the use of FUSE Mount."
598,"import json Shapefile is a popular vector format developed by ESRI that stores the geometric with open /dbfs/ml/blogs/geospatial/nyc_boroughs. .geojson"", ""r"") as location and attribute information of geographic features. The format consists myfile:",Chunk 598: This file contains a . Shapefile and a .geo file
599,"of a collection of files with a common filename prefix (*.shp, *shx and *.dbf are bore.data-ylle.reado # read GeoJSON from DBFS using FuseMount mandatory) stored in the same directory. An alternative to shapefile is KML, also m = folium.Map ( used by our customers but not shown for brevity. For this example, let's use NYC","Chunk 599: A shapefile is a collection of files with a common filename prefix (*.shp, *shx and *.d"
600,"location-[40.7128, -74.00601, tiles-'Stamen Terrain', Building shapefiles. While there are many ways to demonstrate reading shapefiles, zoom_start-12 we will give an example using GeoSpark. The built-in ShapefileReader is used to",Chunk 600: In this video we will show how to read shapefiles using
601,") folium.GeoJson (json.loads (boro_data)) .add_to (m) generate the rawSpatialDf DataFrame. m # to display, also could use displayHTML (.. .) variants éscala",Chunk 601: folium.GeoJson (.loads
602,"var spatialRDD = new SpatialRDD [Geometryl spatialRDD = ShapenleReader.readregeometryRDD (sc, ""/ml/blogs/ PAEAANPeeT var rawSpatialDf = Adapter.toDf (spatialRDD, spark) rawdpatialDf.createoneplacerempviex C""rawSpatialDE""? //DataFrame now",Chunk 602: varRDD = new SpatialRDD [Geo
603,"available to SQL, Python, and R databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 74 By registering rawSpatialDf as a temp view, we can easily drop into pure Spark","Chunk 603: With the release of Spark 2ND, it is now possible to run"
604,"Raster data SQL syntax to work with the DataFrame, to include applying a UDF to convert the Raster data stores information of features in a matrix of cells (or pixels) organized into shapefile WKT into Geometry. rows and columns (either discrete or continuous). Satellite images, photogrammetry","Chunk 604: syntax to work with the DataFrame, to include applying a UDF to convert the Raster data stores"
605,"and scanned maps are all types of raster-based Earth Observation (EO) data. 8sql SELECT *, ST_ GeomFromwKT (geometry) AS geometry GeoSpark UDF to convert WKT to The following Python example uses RasterFrames, a Dataframe-centric spatial Geometry FROM rawspatialdf",Chunk 605: RasterFrames can be used to convert raw raster-based Earth observation data into
606,"analytics framework, to read two bands of GeoTIFF Landsat-8 imagery (red and near-infrared) and combine them into Normalized Difference Vegetation Index. We can use this data to assess plant health around NYC. The rf_ipython module Additionally, we can use Databricks' built-in visualization for in-line analytics, such is used to manipulate RasterFrame contents into a variety of visually useful forms, as charting the tallest buildings in NYC.",Chunk 606: We are using Databricks' ipython.
607,"such as below where the red, NIR and NDVI tile columns are rendered with color ramps, using the Databricks built-in displayHTMLC) command to show the results 8sql within the notebook. SELECT name,","Chunk 607: In this tutorial, I will show you how to display the results of"
608,"round (Cast (num floors AS DOUBLE), 0) AS num_floors --String to Number FROM rawspatialdf Spython WHERE name # construct a CSV ""catalog"" for RasterFrames 'raster' reader",Chunk 608: The BBC's science and technology correspondent Nick Triggle
609,"ORDER BY num_floors DESC LIMIT 5 # catalogs can also be Spark or orid TradeCtr WTC Figure 10: AI Databricks built-in visualization for in-line analytics charting, for example,",Chunk 609: The World Trade Center (WTC) in New York is
610,"the tallest buildings in NYC databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 75 Through its custom Spark DataSource, RasterFrames can read various raster","Chunk 610: The world's tallest building, the Burj Khalifa in"
611,"* LE results: PpMagatmaDifiem: [ongitude.Jatitude: udt, rf.tile(red): udt. -2moret fields] Showing only top 51 rows formats, including GeOTIFF, JP2000, MRF and HDF, from an array of services. longitude_Jatitude rf_tile(red) rf_tile(NIR) rf_tile(NDVI)",Chunk 611: Satellite images courtesy of the European Space Agency.
612,"It also supports reading the vector formats GeoJSON and WKT/WKB. RasterFrame contents can be filtered, transformed, summarized, resampled and rasterized POINT (75.6431054921628. 41.35507991091221) through over 200 raster and vector functions, such as st_reproject() and st_centroid.) used in the example above. It provides APIs for Python, SQL and",Chunk 612: Ster-centroid is a Python program that reads the raster data from the Ster-centroid
613,Scala as well as interoperability with Spark ML. POINT (75.55129747458508 41.3555632722104) POINT (75.64242580157753 41.285904858936576) POINT (75.55071479581207 41.28638012434911) POINT (7545900176161878: 41.286782331725604),Chunk 613: A new version of the Scala programming language has been
614,"Screenshot Figure 11: RasterFrame contents can bei filtered, transformed, summarized, resampled and rasterized through over 200 raster and vector functions databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION","Chunk 614: RasterFrames are a new way to store, share and"
615,76 Geodatabases Getting started with geospatial analysis on Databricks Geodatabases can be file based for smaller scale data or accessible via JDBC/ODBC connections for medium scale data. You can use Databricks to,Chunk 615: Get started with 76 Geodatabases Getting started with 76 Geodatabases
616,Businesses and government agencies seek to use spatially referenced data in query many SQL databases with the built-in JDBC/ODBC Data Source. conjunction with enterprise data sources to draw actionable insights and deliver Connecting to PostgreSQL is shown below and is commonly used for smaller on a broad range of innovative use cases. In this blog we demonstrated how the,Chunk 616: How do you connect disparate data sources to deliver insights?
617,"scale workloads by applying PostGIS extensions. This pattern of connectivity Databricks Unified Data Analytics Platform can easily scale geospatial workloads, allows customers to maintain as-is access to existing databases. enabling our customers to harness the power of the cloud to capture, store and analyze data of massive size.","Chunk 617: PostGIS extends the power of the cloud to capture, store and analyze data of massive size."
618,"éscala display ( Context.read.fomat (""jdbc"") In an upcoming blog, we will take a deep dive into more advanced topics for .option (""url"", jdbcUrl)",Chunk 618: Check out our selection of some of the most interesting
619,"geospatial at-scale with Databricks. You will find additional details option (""driver"", or.poatyreglpriver? processing .option (""dbtable"",","Chunk 619: You will find additional details option (""driver"","
620,about the spatial formats and highlighted frameworks reviewing Data MIn * by (SELECT FROM ellocsripata.taging,"Chunk 620: In our series of letters from African journalists, journalist"
621,"OFFSET 5 LIMIT 10) AS t"""") //predicate pushdown Prep Notebook, GeoMesa + H3 Notebook, GeoSpark Notebook, GeoPandas option (""user"", jdbcUsername) Notebook, and RasterFrames Notebook. Also, stay tuned for a new section in our option 'jdbcPassword"", jdbcPassword)",Chunk 621: Stay up-to-date with the latest news and features in thejdbc
622,.load) documentation specifically for geospatial topics of interest. pep. pickup. 2019-01-0 2019-0,Chunk 622: This page is a collection of (i.e
623,2019 1:27 2019 2019 2019 2019,Chunk 623: BBC Sport takes a look back at some of the
624,2019-0 019-01 57:03 databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 77,Chunk 624: BBC News NI looks at some of the key stories
625,CHAPTER 9: Exploring Twitter Introduction Sentiment and Crypto The market capitalization of cryptocurrencies increased from $17 billion in 2017 to $2.25 trillion in 2021.,Chunk 625: The value of cryptocurrencies has more than doubled in
626,"Price Correlation Using That's over a 13,000% ROI in a short span of 5 years! Even with this growth, cryptocurrencies remain Databricks incredibly volatile, with their value being impacted by a multitude of factors: market trends, politics, technology : and Twitter. Yes, that's right. There have been instances where their prices were impacted on","Chunk 626: Over the past 5 years, the value of cryptocurrencies has increased by more than 50%."
627,"account of tweets by famous personalities. As part of a data engineering and analytics course at the Harvard Extension School, our group worked on a project to create a cryptocurrency data lake for different data personas - including data engineers, ML practitioners and BI analysts = to analyze trends over time, particularly the impact of social media on the price volatility of a crypto asset, such as Bitcoin (BTC). We leveraged the Databricks Lakehouse Platform","Chunk 627: As part of a data engineering and analytics course at the Harvard Extension School, our group worked on a project to create a cryptocurrency data lake for different data personas - including data engineers"
628,"to ingest unstructured data from Twitter using the Tweepy library and traditional structured pricing data from Yahoo Finance to create a machine learning prediction model that analyzes the impact of investor sentiment on crypto asset valuation. The aggregated trends and actionable insights are presented on a Databricks SQL dashboard, allowing for easy consumption to relevant stakeholders. By Monica Lin, Christoph Meier,",Chunk 628: Databricks has teamed up with Tweepy Inc.
629,Matthew Parker and Kiran Ravella This blog walks through how we built this ML model in just a few weeks by leveraging Databricks and its collaborative notebooks. We would like to thank the Databricks University Alliance program and the extended team for all the support. databricks,Chunk 629: We used Databricks to build a machine learning (ML) model for a big data analytics project.
630,EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION 78 Overview One advantage of cryptocurrency for investors is that it is traded 24/7 and The full orchestration workflow runs a sequence of Databricks notebooks that,Chunk 630: Blockchain technology is changing the way we interact with the world around us.
631,the market data is available round the clock. This makes it easier to analyze the perform the following tasks: correlation between the Tweets and crypto prices. A high-level architecture 1. Data ingestion pipeline of the data and ML pipeline is presented in figure 11 below.,Chunk 631: Twitter has developed a high-level architecture for ingestion and ML of its crypto market data.
632,Imports the raw data into the Cryptocurrency Delta Lake Bronze tables 2. Data science Cleans data and applies the Twitter sentiment machine learning Data Sources Chryptocurrency Delta Lake,Chunk 632: The following table lists the key data points from the 2016 European
633,model into Silver tables Cryp Aggregates the refined Twitter and Yahoo Finance data into an aggregated Gold table Rawtweets (Bronze),Chunk 633: Cryp aggregates the refined Twitter and Yahoo Finance data
634,Refinedrweets (Silver) Computes the correlation ML model between price and sentiment A 3. Data Yfina,Chunk 634: Find out more at: www.yfina.
635,analysis Rawticker data Refinedticker data Runs updated SQL BI queries on the Gold table (Bronze),Chunk 635: BBC News takes a look at some of the key
636,(Silver) DataQualiy AWSS3 Cloud Native Storage The lakehouse paradigm combines key capabilities of data lakes and data warehouses to enable all kinds of BI and Al use cases. The use of the lakehouse,Chunk 636: The DataQualiy AWSS3 Cloud Native Storage lakehouse paradigm
637,architecture enabled rapid acceleration of the pipeline creation to one 1: week. Figure Crypto Lake using Delta just,Chunk 637: Delta is an open-source software platform for building
638,"As a team, we played specific roles to mimic different data personas, and this paradigm facilitated the seamless handoffs between data engineering, machine learning and business intelligence roles without requiring data to be moved across systems. databricks","Chunk 638: In this episode of the Databricks podcast, databricks co-founder"
639,EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 79 Data/ML pipeline Data science Ingestion using a medallion architecture,Chunk 639: BBC News takes a look back at some of the
640,"The data science portion of our project consists of three major parts: exploratory The two primary data sources were Twitter and Yahoo Finance. A lookup table data analysis, sentiment model and correlation model. The objective is to build was used to hold the crypto tickers and their Twitter hashtags to facilitate the a sentiment model and use the output of the model to evaluate the correlation",Chunk 640: The objective of this project is to build a sentiment model to evaluate the correlation between the price of Ethereum and the price of Bitcoin.
641,"subsequent search for associated tweets. between sentiment and the prices of different cryptocurrencies, such as Bitcoin, Ethereum, Coinbase and Binance. In our case, the sentiment model follows a We used yfinance python library to download historical crypto exchange market supervised, multi-class classification approach, while the correlation model","Chunk 641: In this paper, we present a sentiment-based model for predicting the price of cryptocurrencies."
642,"data from Yahoo Finance's API in 15-minute intervals. The raw data was stored in a uses a linear regression model. Lastly, we used MLflow for both models' lifecycle Bronze table containing information such as ticker symbol, datetime, open, close, management, including experimentation, reproducibility, deployment and a central high, low and volume. We then created a Delta Lake Silver table with additional model registry. MLflow Model Registry collaboratively manages the full lifecycle of",Chunk 642: We used MLflow to run two regression models on approx.
643,"data, such as the relative change in price of the ticker in that interval. Using Delta an MLflow model by offering a centralized model store, set of APIs and UI. Some of Lake made it easy to reprocess the data, as it guarantees atomicity with every its most useful features include model lineage (which MLflow experiment and run operation. It also ensures that schema is enforced and prevents bad data from","Chunk 643: Google's artificial intelligence (AI) platform, Lake, has been used by the Wall Street Journal (WSJ) to build a machine learning (ML) model"
644,"produced the model), model versioning, stage transitions (such as from staging to creeping into the lake. production or archiving), and annotations. We used Tweepy Python library to download Twitter data. We stored the raw tweets in a Delta Lake Bronze table. We removed unnecessary data from the","Chunk 644: In this project, we used Python to create a 3D model of a lake in the middle of a"
645,Bronze table and also filtered out non-ASCII characters like emojis. This refined data was stored in a Delta Lake Silver table. databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 80,Chunk 645: The table below shows the top 10 most popular books on Amazon in the
646,Exploratory data analysis amp Anonymous fransactions Private 4s 30 ttT :0,Chunk 646: http://www.firstcoastnews.com/
647,"using Smashcash The EDA section provides insightful visualizations on the data set. For example, we 25 ETH Blockchain looked at the distribution of tweet lengths for each sentiment category using violin","Chunk 647: In this section, we look at some of the key findings from"
648,20 SmashcashPlatforn SHIBAINU want plots from Seaborn. Word clouds (using Matplotlib and wordcloud libraries) for 6 15,Chunk 648: BBC Sport looks at some of the more unusual plots
649,INU Transaction: make SHIBA 5 positive and negative tweets were also used to show the most common words for 10 EICMI-XRP Anywhere Close,Chunk 649: As part of our series of letters from African journalists
650,"the two sentiment types. Lastly, an interactive topic modeling dashboard was built, 5 yer su E E  CO","Chunk 650: In our series of letters from African journalists, filmmaker"
651,"using Gensim, to provide insights on the top most common topics in the data set positive neutral negative BTCGasparino",Chunk 651: BTCGasparino and Gensim have teamed up
652,"and the most frequently used words in each topic, as well as how similar the topics sentiment XRP BTX ETH are to each other.",Chunk 652: Find out what people are saying about BTX ETH
653,Figure 2: Violin plots for text length Figure 3: Word cloud for positive and negative tweets Topic Previous Topi Nerl Topi Cles Topi Slide tric2),Chunk 653: Figure 2: Violin plots for text length Figure 3: Word
654,"A-1 Intertopic Distance Map (van mutidimensional: scaling) Top-30 Most Salient Terms"" Figure 4: Interactive topic modeling dashboard databricks",Chunk 654: BBC News takes a look at some of the most
655,"EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 81 Sentiment analysis model Developing a proper sentiment analysis model has been one of the core tasks In this project, we focused on the latter two approaches since they are supposed","Chunk 655: In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at"
656,"within the project. In our case, the goal of this model was to classify the polarities to be the most promising. Thereby, we used SparkNLP as the NLP library of choice that are expressed in raw tweets as input using a mere polar view of sentiment due to its extensive functionality, its scalability (fully supported by Apache (i.e., tweets were categorized as ""positive,"" ""neutral"" or ""negative""). Since sentiment SparkM) and accuracy (e.g. it contains multiple state-of-the-art embeddings and","Chunk 656: In our case, we used sentiment SparkM as the NLP library of choice that are expressed in raw tweets as input using a mere polar view of sentiment due to its extensive functionality, its"
657,"analysis is a problem of great practical relevance, it is no surprise that multiple ML allows users to make use of transfer learning). First, we built a sentiment analysis strategies related to it can be found in literature: pipeline using the aforementioned classical ML algorithms. The following figure shows its high-level architecture consisting of three parts: preprocessing, feature Sentiment lexicons algorithms","Chunk 657: In this paper, we present a novel approach to sentiment analysis using classical ML algorithms (i.e."
658,"Off-the-shelf sentiment analysis systems vectorization and finally training including hyperparameter tuning. Compare each word in a tweet to a database Exemplary systems: Amazon Comprehend, of words that arel labeled as having positive or Google Cloud Services, Stanford Core NLP negative sentiment","Chunk 658: In this talk, I will show how sentiment analysis can be used to predict the likelihood of"
659,"Pros: do not require great preprocessing Preprocessing stages At tweet with more positive words than negative of the data and allow the user to directly would be scored as a positive and vice versa start a prediction ""out of the box"" Pros: straightforward approach",Chunk 659: BBC Sport takes a look at some of the ways artificial intelligence (AI) could be used to predict
660,Cons: limited fine-tuning for the underlying Document Tokenizer Normalizer Stopwords Lemmatizer Stemmer Finisher use-case (retraining might be needed to Assembler Cleaner,Chunk 660: Pros: easy-to-use tool that can
661,Cons: performs poorly in general and greatly adjust the model performance) depends ont the quality of the database of words Classical ML algorithms Deep Learning (DL) algorithms Feature Vectorization,Chunk 661: Pros: performs well in general and greatly adjust the model performance
662,"Training & Tuning Application of traditional supervised classifiers Application of NLP related neural network like logistic regression, random forest, support architectures like BERT, GPT-2/ GPT-3 mainly vector machine or Naive Bayes via transfer learning",Chunk 662: Research and development (R&D) of artificial intelligence (AI) and
663,"HashingTF IDF Classifier (LogReg, NB, RF, SVC) Pros: well known, often financially and Pros: many pretrained neural networks for",Chunk 663: BBC News takes a look at the pros and cons
664,"computationally cheap, easy toi interpret word embeddings and sentiment prediction already exist (particularly helpful for transfer Figure 5: Machine learning model pipeline Cons: in general, performance on unstructured learning), DL models scale effectively with data",Chunk 664: Deep Learning (DL) has the potential to transform the way we work with
665,data like text is expected to be worse compared to structured data and necessary Cons: difficult and computationally expensive preprocessing can be extensive to tune architecture and hyperparameters databricks,"Chunk 665: Pros: low-cost, easy-to-use,"
666,"EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 82 We run this pipeline for every classifier and compare their corresponding Correlation model accuracies on the test set. As a result, the Support Vector Classifier achieved the",Chunk 666: A group of students from the University of Bristol have been given a set of classifiers
667,"highest accuracy with 75.7%, closely followed by Logistic Regression (75.6%), Naïve The project requirement included a correlation model on sentiment and price; Bayes (74%) and finally Random Forest (71.9%). To improve the performance, other therefore, we built a linear regression model using scikit-learn and mlflow.sklearn supervised classifiers like XGBoost or Gradientoostedtres could be tested. for this task. Besides, the individual algorithms could be combined to an ensemble, which is","Chunk 667: In this paper, we present the results of a study on sentiment and price prediction."
668,"We quantified the sentiment by assigning negative tweets a score of -1, neutral then used for prediction (e.g., majority voting, stacking). tweets a score of O, and positive tweets a score of 1. The total sentiment score In addition to this first pipeline, we developed a second Spark pipeline with a for each cryptocurrency is then calculated by adding up the scores for each",Chunk 668: We developed a pipeline to measure the sentiment of tweets about cryptocurrencies.
669,"similar architecture making use of the rich Spark NLP functionalities regarding cryptocurrency in 15-minute intervals. The linear regression model is built using pretrained word embeddings and DL models. Starting with the standard Document the total sentiment score in each window for all companies to predict the Assembler annotator, we only used a Normalizer annotator to remove Twitter percentage change in cryptocurrency prices. However, the model shows no clear","Chunk 669: In this paper, we show how to predict the price of cryptocurrencies using sentiment analysis."
670,"handles, alphanumeric characters, hyperlinks, html tags and timestamps but no linear relationship between sentiment and change in price. A possible future further preprocessing related annotators. In terms of the training stage, we used improvement for the correlation model is using sentiment polarity to predict the a pretrained (on the well-known IMDb data set) sentiment DL model provided",Chunk 670: In this paper we present a novel sentiment-based annotator for the IMDb website.
671,"change in price instead. by Spark NLP. Using the default hyperparameter settings, we already achieved a test set accuracy of 83%, which could potentially be even enhanced using other pretrained word embeddings or sentiment DL models. Thus, the DL strategy Pre-processing stages I","Chunk 671: In this paper, I will show you how to train a Deep Learning (DL) model to"
672,Training MAE: 1.6 clearly outperformed the pipeline in figure 5 with the Support Vector Classifier by MSE: 29.4 around 7.4 percent points.,Chunk 672: Figures from the UK's Office for National Statistics
673,RMSE: 5.42 R2: 0.01 Finance & Transformation I Linear Regression Sentiment,Chunk 673: The number of people claiming to be victims of fraud
674,Figure 6: Correlation model pipeline databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 83 Business intelligence,Chunk 674: A look at some of the key findings from the
675,"Understanding stock correlation models was a key component of generating buy/ The use of the SQL Editor in Databricks was key to making the process fast and sell predictions, but communicating results and interacting with the information is simple. For each query, the editor GUI enables the selection of different views of equally critical to make well-informed decisions. The market is sO dynamic, SO a the data including tables, charts and summary statistics to immediately see the",Chunk 675: Databricks has been used to generate buy and sell recommendations on the FTSE 100.
676,"real-time visualization is required to aggregate and organize trending information. output. From there, views could be imported directly into the dashboards. This Databricks Lakehouse enabled all of the BI analyst tasks to be coordinated in one eliminated redundancy by utilizing the same query for different visualizations. place with streamlined access to the lakehouse data tables. First, a set of SQL","Chunk 676: Databricks Lakehouse is a business intelligence (BI) platform that allows users to query, query, query, query."
677,"queries were generated to extract and aggregate information from the lakehouse. Visualization Then the data tables were easily imported with a GUI tool to rapidly create For the topic of Twitter sentiment analysis, there are three key views to help users dashboard views. In addition to the dashboards, alert triggers were created to",Chunk 677: This case study shows how Twitter sentiment analysis was used to create a lakehouse dashboard.
678,"interact with the data on a deeper level. notify users of critical activities like stock movement up/down by > X%, increases in Twitter activity about a particular crypto hashtag or changes in overall positivel View 1: Overview Page, taking a high-level view of Twitter influencers, stock negative sentiment about each cryptocurrency. movement and frequency of tweets related to particular cryptos.",Chunk 678: Twitter's Hashtag Report is a new way for investors to monitor the performance of cryptocurrencies.
679,"12,658 @ethereum Tweet Count Dashboard generation binance TheWizardo:Doge The business intelligence dashboards were created using Databricks SQL. This",Chunk 679: Databricks has released a new version of its
680,"Top Influencer system provides a full ecosystem to generate SQL queries, create data views XRP-USD DOTI-USD and charts, and ultimately organizes all of the information using Databricks Dashboards.",Chunk 680: Databricks has been chosen by the Bank of England to
681,"eerd @dogecoin @ethereum Cripple Figure 7: Overview Dashboard"" View with top level statistics",Chunk 681: Cripple is the world's largest virtual currency
682,"databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 84 View 2: Sentiment Analysis, to understand whether each tweet is positive, negative Summary or neutral. Here you can easily visualize which cryptocurrencies are receiving the",Chunk 682: This chart shows the difference between positive and negative sentiment on Twitter.
683,"Our team of data engineers, data scientists and BI analysts was able to most attention in a given time window. leverage the Databricks tools to investigate the complex issue of Twitter usage Analysis and cryptocurrency stock movement. The lakehouse design created a robust",Chunk 683: The team at Databricks worked with us to create a lakehouse-style data warehouse.
684,"BarChart rypto data environment with smooth ingestion, processing and retrieval by the whole team. The data collection and cleaning pipelines deployed using Delta tables were easily managed even at high update frequencies. The data was analyzed by a natural language sentiment model and a stock correlation model using MLflow,",Chunk 684: The BarChart team needed a scalable and easy-to-use data collection and analysis platform.
685,"which made the organization of various model versions simple. Powerful analytics dashboards were created to view and interpret the results using built-in SQL and Dashboard features. The functionality of Databricks' end-to-end product tools Figure 8: Sentiment Analysis Dashboard removed significant technical barriers, which enabled the entire project to be","Chunk 685: Databricks was selected for the project because of its strong track record of delivering high-quality, cost-"
686,"View 3: Stock Volatility to provide the user with more specific information about completed in less than 4 weeks with minimal challenges. This approach could the price for each cryptocurrency with trends over time. easily be applied to other technologies where streamlined data pipelines, machine learning and BI analytics can be the catalyst for a deeper understanding",Chunk 686: View 2: Price Trending to provide the user with more specific information about completed in less than 4 weeks with minimal challenges.
687,of your data. I - Figure 9: Stock Ticker Dashboard databricks EBOOK: BIG BOOK OF MACHINE LEARNIN G USE CASES 2ND EDITION,Chunk 687: The UK's National Crime Agency (NCA)
688,"85 Our findings These are additional conclusions from the data analysis to highlight the extent of Overall, the use of Databricks to coordinate the pipeline from data ingestions, the Twitter users' influence on the price of cryptocurrencies.",Chunk 688: An analysis of Twitter users' influence on the price of cryptocurrencies has been carried
689,"lakehouse data structure and the BI reporting dashboards was hugely beneficial to completing this project efficiently. In a short period of time, the team was able Volume of tweets correlated with volatility in cryptocurrency price to build the data pipeline, complete machine learning models and produce high- There is a clear correlation in periods of high tweet frequency to the movement of quality visualizations to communicate results. The infrastructure provided by the","Chunk 689: The team at lakehouse was able to build a data pipeline, complete machine learning models and produce high- There is a clear correlation in periods of high tweet frequency to the"
690,"a cryptocurrency. Note that this happens before and after a stock price change, Databricks platform removed many of the technical challenges and enabled the indicating some tweet frenzies precede price change and are likely influencing project to be successful. value, and others are in response to big shifts in price.",Chunk 690: Databricks is a social media platform that allows users to influence the price of a company's stock.
691,"While this tool will not enable you to outwit the cryptocurrency markets, we Twitter users with more followers don't actually have more influence strongly believe it will predict periods of increased volatility, which can be on crypto stock price advantageous for specific investing conditions.",Chunk 691: We've put together a list of some of the most influential people in the world using Twitter to
692,"This is often discussed in media events, particularly with lesser-known currencies. Disclaimer: This article takes no responsibility for financial investment decisions. Some extreme influencers like Elon Musk gained a reputation for being able to drive Nothing contained in this website should be construed as investment advice. enormous market swings with a small number of targeted tweets. While it is true","Chunk 692: One of the most frequently asked questions in the investment community is: ""What's the best way to make money?"""
693,"that a single tweet can impact cryptocurrency price, there is not an underlying correlation between number of followers to movement of the currency price. There Try notebooks is also a slightly negative correlation to number of retweets VS. price movement, Please try out the referenced Databricks notebooks",Chunk 693: There Try notebooks is a slightly negative correlation to number of retweets VS.
694,indicating the Twitter activity by influencers might have broader reach as it moves into other mediums like new articles rather than reaching directly to investors. Data Science * Merge to Gold * The Databricks platform was incredibly useful for solving complex problems like,Chunk 694: A look at some of the key stories from the world of social media this week.
695,Orchestrator * Inference * merging Twitter and stock data. Tweepy * Y_Finance *,Chunk 695: A look at some of the key stories from the
696,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 86 CHAPTER 10: CUSTOMER CASE STUDIES Comcast delivers the,Chunk 696: BBC News takes a look back at some of the
697,"As a global technology and media company that connects millions of customers to personalized experiences, future of entertainment Comcast struggled with massive data, fragile data pipelines and poor data science colaboration. By using Databricks - including Delta Lake and MLflow = they were able to build performant data pipelines for petabytes of data and easily manage the lifecycle of hundreds of models, creating a highly innovative, unique","Chunk 697: In this episode of Databricks, we look at the challenges faced by Comcast's future of entertainment team."
698,"and award-winning viewer experience that leverages voice recognition and machine learning. Use case: In the intensely competitive entertainment industry, there's no time to press the Pause button. Comcast realized they needed to modernize their entire approach to analytics, from data ingest to the deployment of machine learning models that deliver new features to delight their customers. Solution and benefits: Armed with a unified approach to analytics, Comcast can now fast-forward into the",Chunk 698: What we know: Comcast is one of the world's largest providers of broadband video services.
699,future of Al-powered entertainment = keeping viewers engaged and delighted with competition-beating customer experiences. Emmy-winning viewer experience: Databricks helps Comcast to create a highly innovative and award- COMCAST winning viewer experience with intelligent voice commands that boost engagement,Chunk 699: Databricks helps Comcast create a highly innovative and award-winning viewer experience with intelligent
700,"Reduced compute costs by 10x: Delta Lake has enabled Comcast to optimize data ingestion, replacing 640 machines with 64 = while improving performance. Teams can spend more time on analytics and ""With Databricks, we can now be more informed about the decisions we make, less time on infrastructure management.",Chunk 700: Comcast has been using Databricks' Delta Lake platform to ingest petabytes of unstructured data from Comcast's
701,"and we can make them faster."" Higher data science productivity: The upgrades and use of Delta Lake fostered global collaboration among data scientists by enabling different programming languages through a single interactive Jim Forsythe Senior Director, Product Analytics and Behavioral Sciences","Chunk 701: In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at"
702,"workspace. Delta Lake also enabled the data team to use data at any point within the data pipeline, Comcast allowing them to act much quicker in building and training new models. Faster model deployment: By modernizing, Comcast reduced deployment times from weeks to minutes as operations teams deployed models on disparate platforms",Chunk 702: Comcast is using Delta Lake to speed up the development and deployment of new video-on-demand (VoD) services
703,Learn more databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 87 CHAPTER 10: CUSTOMER CASE STUDIES,"Chunk 703: All photographs courtesy of AFP, EPA, Getty Images"
704,"Regeneron accelerates Regeneron's mission is to tap into the power of genomic data to bring new medicines to patients in need. with Yet, transforming this data into life-changing discovery and targeted treatments has never been more drug discovery",Chunk 704: Meet the scientists at Regeneron who are pioneering a new approach to drug discovery.
705,"challenging. With poor processing performance and scalability limitations, their data teams lacked what genomic sequencing they needed to analyze petabytes of genomic and clinical data. Databricks now empowers them to quickly analyze entire genomic data sets quickly to accelerate the discovery of new therapeutics. Use case: More than 95% of all experimental medicines that are currently in the drug development pipeline",Chunk 705: One of the world's largest pharmaceutical companies is using Databricks to speed up the discovery and development of new drugs.
706,"are expected to fail. To improve these efforts, the Regeneron Genetics Center built one of the most comprehensive genetics databases by pairing the sequenced exomes and electronic health records of more than 400,000 people. However, they faced numerous challenges analyzing this massive set of data: Genomic and clinical data is highly decentralized, making it very difficult to analyze and train models against their entire 10TB data set","Chunk 706: In an effort to improve the diagnosis and treatment of diseases such as cancer and cardiovascular disease, scientists are increasingly turning to genetic testing."
707,Difficult and costly to scale their legacy architecture to support analytics on over 80 billion data points Data teams were spending days just trying to ETL the data SO that it could be used for analytics REGENERON Solution and benefits: Databricks provides Regeneron with a Unified Data Analytics Platform running on Amazon Web Services that simplifies operations and accelerates drug discovery through improved data science,"Chunk 707: One of the world's leading pharmaceutical companies, Regeneron Pharmaceuticals, was struggling with a number of challenges."
708,"productivity. This is empowering them to analyze the data in new ways that were previously impossible. ""The Databricks Unified Data Analytics Platform is enabling everyone in our integrated drug Accelerated drug target identification: Reduced the time it takes data scientists and development process = from physician-scientists to","Chunk 708: Databricks, the leader in data analytics for the pharmaceutical and medical device industries, has announced that a"
709,"computational biologists to run queries on their entire data set from 30 minutes down to 3 seconds - computational biologists = to easily access, analyze a 600x improvement! and extract insights from all of our data."" Increased productivity: Improved collaboration, automated DevOps and accelerated pipelines","Chunk 709: Researchers at the University of California, Los Angeles (UCLA) are using artificial intelligence (AI) to"
710,"Jeffrey Reid, Ph.D. (ETL in 2 days VS. 3 weeks) have enabled their teams to support a broader range of studies Head of Genome Informatics Regeneron Learn more",Chunk 710: Researchers at Regeneron have developed a new approach to the analysis of
711,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 88 CHAPTER 10: CUSTOMER CASE STUDIES Nationwide reinvents,Chunk 711: BBC News takes a look at some of the key
712,"The explosive growth in data availability and increasing market competition are challenging insurance providers to provide better pricing to their customers. With hundreds of millions of insurance records insurance with actuarial to analyze for downstream ML, Nationwide realized their legacy batch analysis process was slow and modeling",Chunk 712: Nationwide Insurance is a leading property and casualty (P&C) insurer in the United States.
713,"inaccurate, providing limited insight to predict the frequency and severity of claims. With Databricks, they have been able to employ deep learning models at scale to provide more accurate pricing predictions, resulting in more revenue from claims. Use case: The key to providing accurate insurance pricing lies in leveraging information from insurance claims. However, data challenges were difficult, as they had to analyze insurance records that were volatile",Chunk 713: A leading insurance company is using Databricks to improve the accuracy of their insurance pricing.
714,because claims were infrequent and unpredictable - resulting in inaccurate pricing. Solution and benefits: Nationwide leverages the Databricks Unified Data Analytics Platform to manage the entire analytics process from data ingestion to the deployment of deep learning models. The fully managed platform has simplified IT operations and unlocked new data-driven opportunities for their data science teams.,Chunk 714: Nationwide Insurance needed an analytics platform that would allow them to:
715,"Data processing at scale: Improved runtime of their entire data pipeline from 34 hours to less than Nationwide 4 hours, a 9x performance gain Faster featurization: Data engineering is able to identify features 15x faster = from 5 hours to around ""With Databricks, we are able to train models",Chunk 715: Nationwide Insurance is using Databricks to speed up the analysis of their customer data.
716,"20 minutes against all our data more quickly, resulting in more accurate pricing predictions that have Faster model training: Reduced training times by 50%, enabling faster time-to-market of new models had a material impact on revenue.""","Chunk 716: ""We have reduced the time it takes to train our models from approx."
717,"Improved model scoring: Accelerated model scoring from 3 hours to less than 5 minutes, a 60x Bryn Clark improvement Data Scientist Nationwide","Chunk 717: Bryn Clark, a PhD student at the University of"
718,databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 89 CHAPTER 10: CUSTOMER CASE STUDIES Condé Nast boosts,Chunk 718: BBC News takes a look back at some of the
719,"Condé Nast is one of the world's leading media companies, counting some of the most iconic magazine titles in its portfolio, including The New Yorker, Wired and Vogue. The company uses data to reach over 1 billion reader engagement with people in print, online, video and social media. experiences driven by",Chunk 719: Condé Nast is looking for a data scientist to work in the company's New York office.
720,"data and Al Use case: As a leading media publisher, Condé Nast manages over 20 brands in their portfolio. On a monthly basis, their web properties garner 100 million-plus visits and 800 million-plus page views, producing a tremendous amount of data. The data team is focused on improving user engagement by using machine learning to provide personalized content recommendations and targeted ads.",Chunk 720: This case study shows how a leading media company is using artificial intelligence (AI) and machine learning to improve the user experience.
721,"Solution and benefits: Databricks provides Condé Nast with a fully managed cloud platform that simplifies operations, delivers superior performance and enables data science innovation. Improved customer engagement: With an improved data pipeline, Condé Nast can make better, faster and more accurate content recommendations, improving the user experience Built for scale: Data sets can no longer outgrow Condé Nast's capacity to process and glean insights",Chunk 721: How Condé Nast is using Databricks to:
722,"More models in production: With MLflow, Condé Nast's data science teams can innovate their products CONDÉ faster. They have deployed over 1,200 models in production. NAST Learn more",Chunk 722: Machine learning (ML) and artificial intelligence (AI) are
723,"""Databricks has been an incredibly powerful end-to- end solution for us. It's allowed a variety of different team members from different backgrounds to quickly get in and utilize large volumes of data to make actionable business decisions.""",Chunk 723: How has Databricks changed your business?
724,- Paul Fryzel Principal Engineer of AI Infrastructure Condé Nast databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION,Chunk 724: BBC News takes a look at some of the key
725,"90 CHAPTER 10: CUSTOMER CASE STUDIES Showtime leverages ML SHOWTIME is a premium television network and streaming service, featuring award-winning original series to deliver data-driven",Chunk 725: A look back at some of the best examples of
726,"and original limited series like ""Shameless,"" ""Homeland,"" ""Billions,"" ""The Chi,"" ""Ray Donovan,"" ""SMILF,"" ""The Affair,"" ""Patrick Melrose,"" ""Our Cartoon President,"" ""Twin Peaks"" and more. content programming Use case: The Data Strategy team at Showtime is focused on democratizing data and analytics across the organization. They collect huge volumes of subscriber data (e.g., shows watched, time of day, devices used,","Chunk 726: About Showtime: Showtime is the world's most-watched cable network, with more than 20 million subscribers."
727,"subscription history, etc.) and use machine learning to predict subscriber behavior and improve scheduling and programming. Solution and benefits: Databricks has helped Showtime democratize data and machine learning across the organization, creating a more data-driven culture. 6x faster pipelines: Data pipelines that took over 24 hours are now run in less than 4 hours, enabling",Chunk 727: Showtime is using Databricks to ingest petabytes of data (e.g.
728,"teams to make decisions faster Removing infrastructure complexity: Fully managed platform in the cloud with automated cluster management allows the data science team to focus on machine learning rather than hardware SHOWTIME configurations, provisioning clusters, debugging, etc.",Chunk 728: Data science teams can focus on machine learning rather than hardware:
729,"Innovating the subscriber experience: Improved data science collaboration and productivity has reduced time-to-market for new models and features. Teams can experiment faster, leading to a better, ""Being on the Databricks platform has allowed a team of exclusively data scientists to make huge more personalized experience for subscribers.",Chunk 729: The BBC's science team looks at how the BBC Science app has changed the way scientists work and experiment
730,"strides in setting aside all those configuration headaches that we were faced with. It's Learn more dramatically improved our productivity."" - Josh McNutt","Chunk 730: ""It's Learn more dramatically improved our productivity"
731,Senior Vice President of Data Strategy and Consumer Analytics Showtime databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION,Chunk 731: BBC News takes a look at some of the key
732,91 CHAPTER 10: CUSTOMER CASE STUDIES Shell innovates with Shell is a recognized pioneer in oil and gas exploration and production technology and is one of the world's solutions for,Chunk 732: Shell is one of the world's largest independent oil and gas
733,"leading oil and natural gas producers, gasoline and natural gas marketers and petrochemical manufacturers. energy a cleaner world Use case: To maintain production, Shell stocks over 3,000 different spare parts across their global facilities.",Chunk 733: Shell is one of the world's largest independent oil and gas producers.
734,"It's crucial the right parts are available at the right time to avoid outages, but equally important is not overstocking, which can be cost-prohibitive. Solution and benefits: Databricks provides Shell with a cloud-native unified analytics platform that helps with improved inventory and supply chain management. Predictive modeling: Scalable predictive model is developed and deployed across more than 3,000",Chunk 734: Oil and gas companies face a number of challenges when it comes to managing their supply chains.
735,"types of materials at 50-plus locations Historical analyses: Each material model involves simulating 10,000 Markov Chain Monte Carlo iterations to capture historical distribution of issues Massive performance gains: With a focus on improving performance, the data science team reduced the inventory analysis and prediction time to 45 minutes from 48 hours on a 50 node Apache SparkIM","Chunk 735: A team of researchers at the University of California, Los Angeles (UCLA) has developed a novel approach to inventorying and predicting the"
736,"cluster on Databricks = a 32x performance gain Reduced expenditures: Cost savings equivalent to millions of dollars per year ""Databricks has produced an enormous amount of value for Shell. The inventory optimization tool Learn more",Chunk 736: Shell's oil and gas exploration and production (E&P) business
737,"[built on Databricks] was the first scaled up digital product that came out of my organization and the fact that it's deployed globally means we're now delivering millions of dollars of savings every year."" - Daniel Jeavons","Chunk 737: ""We've been using Databricks for three years and it's"
738,General Manager Advanced Analytics COE Shell databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION 92,Chunk 738: BBC News NI takes a look at some of the
739,"CHAPTER 10: CUSTOMER CASE STUDIES Riot Games leverages Al Riot Games' goal is to be the world's most player-focused gaming company. Founded in 2006 and based in and LA, Riot Games is best known for the League of Legends game. Over 100 million gamers play every month.",Chunk 739: Al Riot Games is a leading developer and publisher of video games.
740,to engage gamers reduce churn Use case: Improving gaming experience through network performance monitoring and combating in-game abusive language. Solution and benefits: Databricks allows Riot Games to improve the gaming experience of their players by,Chunk 740: Databricks has been chosen by Riot Games to help improve the gaming experience
741,"providing scalable, fast analytics. Improved in-game purchase experience: Able to rapidly build and productionize a recommendation engine that provides unique offers based on over 500B data points. Gamers can now more easily find the content they want. Reduced game lag: Built ML model that detects network issues in real time, enabling Riot Games to","Chunk 741: Riot Games, one of the world's leading developers and publishers of video games, has announced that it has adopted artificial intelligence"
742,"avoid outages before they adversely impact players Faster analytics: Increased processing performance of data preparation and exploration by 50% compared to EMR, significantly speeding up analyses m RIDT Learn more",Chunk 742: Oil and gas companies are increasingly turning to artificial intelligence (AI)
743,"""We wanted to free data scientists from managing clusters. Having an easy-to-use, managed Spark solution in Databricks allows us to do this. Now our teams can focus on improving the gaming experience.""",Chunk 743: Gaming giant Electronic Arts (EA) is using Databricks Spark to
744,- Colin Borys Data Scientist Riot Games databricks EBOOK: BIG BOOK OF MACHINE LEARNING USE CASES 2ND EDITION,Chunk 744: BBC News NI takes a look at some of the
745,"93 CHAPTER 10: CUSTOMER CASE STUDIES Eneco uses ML to reduce Eneco is the technology company behind Toon, the smart energy management device that gives people and",Chunk 745: The BBC News website takes a look at some of the
746,"control over their energy usage, their comfort, the security of their homes and much more. Eneco's smart energy consumption devices are in hundreds of thousands of homes across Europe. As such, they maintain Europe's largest operating costs energy data set, consisting of petabytes of loT data, collected from sensors on appliances throughout",Chunk 746: Eneco is one of Europe's leading manufacturers of smart energy consumption devices.
747,"the home. With this data, they are on a mission to help their customers live more comfortable lives while reducing energy consumption through personalized energy usage recommendations. Use case: Personalized energy use recommendations: Leverage machine learning and loT data to power their Waste Checker app, which provides personalized recommendations to reduce in-home energy consumption.",Chunk 747: British Gas uses artificial intelligence (AI) and machine learning to learn how their customers use energy.
748,"Solution and benefits: Databricks provides Eneco with a unified data analytics platform that has fostered a scalable and collaborative environment across data science and engineering, allowing data teams to more quickly innovate and deliver ML-powered services to Eneco's customers. Lowered costs: Cost-saving features provided by Databricks (such as auto-scaling clusters and Spot Eneco","Chunk 748: Databricks, a leading provider of data analytics and machine learning (ML) software, has announced that Eneco,"
749,"instances) have helped Eneco significantly reduce the operational costs of managing infrastructure, while still being able to process large amounts of data Faster innovation: With their legacy architecture, moving from proof of concept to production took ""Databricks, through the power of Delta Lake and over 12 months. Now with Databricks, the same process takes less than eight weeks. This enables","Chunk 749: Eneco, one of the world's largest independent power producers, has been using Databricks to reduce the time it takes to build"
750,"structured streaming, allows us to deliver alerts Eneco's data teams to develop new ML-powered features for their customers much faster. and recommendations to our customers with a very limited latency, sO they're able to react Reduced energy consumption: Through their Waste Checker app, Eneco has identified over 67 million","Chunk 750: Eneco, one of the world's largest waste management companies, is using artificial intelligence (AI) and"
751,"to problems or make adjustments within their kilowatt hours of energy that can be saved by leveraging their personalized recommendations home before it affects their comfort levels."" - Stephen Galsworthy Learn more",Chunk 751: Energy experts from the University of Bath are offering homeowners tips on how to
752,Head of Data Science Eneco databricks About Databricks Databricks is the lakehouse company. More than,Chunk 752: As part of our series of letters from African journalists
753,"7,000 organizations worldwide = including Comcast, Condé Nast, H&M and over 50% of the Fortune 500 - rely on the Databricks Lakehouse Platform to unify their data, analytics and Al. Databricks is headquartered in San Francisco, with offices around",Chunk 753: Databricks is a leading provider of data management and analytics software.
754,"the globe. Founded by the original creators of Apache Spark, Delta Lake and MLflow, Databricks is on a mission to help data teams solve the world's toughest problems. To learn more, follow Databricks on Twitter, Linkedin and Facebook.",Chunk 754: Databricks is the world's leading artificial intelligence (AI) and machine learning
755,"- Schedule a personalized demo Sign up for a free trial databricks C Databricks 2022. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation. Privacy Policyl Terms of Use",Chunk 755: C Databricks is an open-source databricks platform that allows you to
