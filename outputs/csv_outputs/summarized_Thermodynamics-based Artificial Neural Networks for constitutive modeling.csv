Chunk Number,Original Text Chunk,Summary
1,"HAL open science Thermodynamics-based Artificial Neural Networks for constitutive modeling Filippo Masi, Ioannis Stefanou, Paolo Vannucci, Victor Maffi-Berthier",Chunk 1: Researchers at the Massachusetts Institute of Technology (MIT)
2,"To cite this version: Filippo Masi, Ioannis Stefanou, Paolo Vannucci, Victor Maffi-Berthier. Themmodynamic-based Ar- tificial Neural Networks for constitutive modeling. Journal of the Mechanics and Physics of Solids, 2021, 147, pp.104277. 10,06/ApaDTT. hal-03079127 HAL Id: hal-03079127","Chunk 2: ""Themmodynamic-based Neural Networks"" by Filippo Masi, Ioan"
3,"https//halscience/la-03079127V1 Submitted on 17 Dec 2020 HAL is a multi-disciplinary open access L'archive ouverte pluridisciplinaire HAL, est archive for the deposit and dissemination of sci- destinée au dépôt et à la diffusion de documents",Chunk 3: The HAL archive is a multi-disciplinary open archive for the deposit and dissemination
4,"entific research documents, whether they are pub- scientifiques de niveau recherche, publiés ou non, lished or not. The documents may come from émanant des établissements d'enseignement et de teaching and research institutions in France or recherche français ou étrangers, des laboratoires abroad, or from public or private research centers. publics ou privés. Thermodynamics-based Artificial Neural Networks for constitutive",Chunk 4: The aim of this project is to bring together all the research papers published in France in the last five years.
5,"modeling Filippo Masia,b, Ioannis Stefanou**, Paolo Vannuccir, Victor Maffi-Berthierb aInstitut de Recherche en Génie Civil et Mécanique, UMR 6183, CNRS, Ecole Centrale de Nantes, Université de Nantes, 1 rue de la Noe, F-44300, Nantes, France.",Chunk 5: In: Proceedings of the 6th International Conference on Modelling and Simulation of Earth
6,"bIngérop Conseil et Ingénierie, 18 rue des Deux Gares, F-92500, Rueil-Malmaison, France. PLMV, UMR 8100, Université de Versailles et Saint-Quentin, 55 avenue de Paris, F-78035, Versailles, France. Abstract",Chunk 6: The following is a list of contact details for some of the
7,"Machine Learning methods and, in particular, Artificial Neural Networks (ANNs) have demonstrated promising capabilities in material constitutive modeling. One of the main drawbacks of such approaches is the lack of a rigorous frame based on the laws of physics. This may render physically inconsistent the predictions of a trained network, which can be even dangerous for real applications. Here we propose a new class of data-driven, physics-based, neural networks for constitutive",Chunk 7: Machine learning and artificial intelligence (AI) have emerged as key areas of research in the fields of computer science and engineering.
8,"modeling of strain rate independent processes at the material point level, which we define as Thermodynamic-based Artificial Neural Networks (TANNs). The two basic principles of thermo- dynamics are encoded in the network's architecture by taking advantage of automatic differentiation to compute the numerical derivatives of a network with respect to its inputs. In this way, derivatives of the free-energy, the dissipation rate and their relation with the stress and internal state variables",Chunk 8: A new class of artificial neural networks has been developed for the study of thermo-dynamic processes.
9,"are hardwired in the architecture of TANNS. Consequently, our approach does not have to identify the underlying pattern of thermodynamic laws during training, reducing the need of large data- sets. Moreover the training is more efficient and robust, and the predictions more accurate. Finally and more important, the predictions remain thermodynamicaly consistent, even for unseen data. Based on these features, TANNS are a starting point for data-driven, physics-based constitutive","Chunk 9: In this paper we present TANNS, a novel method for training predictions of thermodynamic laws."
10,"modeling with neural networks. We demonstrate the wide applicability of TANNS for modeling elasto-plastic materials, using both hyper- and hypo-plasticity models. Strain hardening and softening are also considered for the hyper-plastic scenario. Detailed comparisons show that the predictions of TANNS outperform those of standard ANNS.","Chunk 10: In this paper, we report the results of a novel method for predicting the properties of elas"
11,"Finally, we demonstrate that the implementation of the laws of thermodynamics confers to TANNS high degrees of robustness to the presence of noise in the training data, compared to standard approaches. TANNS ' architecture is general, enabling applications to materials with different or more complex behavior, without any modification.","Chunk 11: In this paper, we present a new approach to the training of materials, called TANNS, based on"
12,"Keywords: Data-driven modeling; Machine learning; Artificial neural network; Thermodynamics; Constitutive model. Masi, et al. (2020) preprint, Journal of the Mechanics and Physics of Solids. doi: 101016/mpa202A.0E7 1. Introduction A large spectrum of constitutive models have been proposed in the literature, based on","Chunk 12: A large spectrum of models have been proposed in the literature, based on the following:"
13,"observations and experimental testing. Existing constitutive laws can account for phenomena taking place at various length scales. This is achieved either through heuristic approaches and assumptions or through asymptotic approximations and averaging (e.g. Lloberas Valls et al., 2019; Nitka et al., 2011; Feyel, 2003; Bakhvalov and Panasenko, 1989). The history and the state of a material is commonly taken into account through ad hoc enrichment of simpler",Chunk 13: The history and state of a material can be taken into account through ad hoc enrichment of simpler laws.
14,"constitutive laws and extensive calibration. For this purpose, the laws of thermodynamics offer a useful framework for deriving more sophisticated laws, by intrinsically respecting the energy balance and the entropy production requirements (see e.g. Houlsby and Puzrin, 2000; Einav et al., 2007; Houlsby and Puzrin, 2007; Einav, 2012, among others). An important limitation in constitutive modeling is the availability of data at different","Chunk 14: The aim of this project is to develop new methods for the study of the laws of thermodynamics, i.e."
15,"time- and length-scales. However, with the increase of computational power, it is nowadays possible to foresee micromechanical simulations that can account for realistic physics and explore stress paths and non-linear phenomena, which are experimentally inaccessible with the current methods. Of course, some constitutive assumptions will be always necessary, but these might be at a smaller scale, where the material properties are measurable and",Chunk 15: The aim of this project is to develop a new generation of simulations for the study of micromechanical phenomena.
16,"easier to identify. This scale is for instance the scale of the microstructure of a material (e.g. the scale of sand grains, crystals, alloys' grains, composites' fibers, masonry bricks' etc. including their topological configuration). However, it is likely that the existing constitutive models might not be sufficient for describing complex material behaviors emerging from the microstructure. Therefore, calibration",Chunk 16: The aim of this project is to develop a new scale for describing the microstructure of materials.
17,"(parameter fitting) of known constitutive descriptors might be insufficient for representing the full space of material response, provided by sophisticated micromechanical simulations. Moreover, micromechanical simulations have currently a tremendous calculation cost, which is impossible to afford in large-scale, non-linear, incremental simulations (e.g. Finite Elements) that are usually needed in applications (cf. Masi et al., 2020, 2018; Rattez et al., 2018a,b;",Chunk 17: The current state-of-the-art in materials science and engineering is limited by the limited number of descriptors that can be used to represent
18,"Collins-Craft et al., 2020; Lloberas Valls et al., 2019; Nitka et al., 2011; Eijnden et al., 2016; Feyel, 2003). A promising solution to this issue seems to be Machine Learning. According to Geron (2015), ""Machine Learning is the science (and art) of programming computers SO they can learn from data"" "" In the context of computer programming, learning is defined by Mitchell","Chunk 18: One of the major challenges in the area of computer science is the problem of learning from large amounts of data, which has been the focus of many"
19,"et al. (1997) as follows: ""A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience"". In the frame of constitutive modeling, a Machine Learning program can learn the stress-strain behavior of a material, given examples of stress-strain increments, which are either determined experimentally or through detailed micromechanical",Chunk 19: Machine learning is a branch of computer science that aims to learn from experience.
20,"simulations. The data that the system uses to learn are called the training data-set and ""Corresponding author. Email addresses: lippo.masilec-mantes.fr (Filippo Masi), ioannis.stefanougec-nantes.fr (Ioannis Stefanou), paolovammuciduwsg.tr (Paolo Vannucci), victor.mafi-berthiertherdingerop.com (Victor Maffi-Berthier)",Chunk 20: This article is part of a series of letters from leading scientists about
21,"2 each training example is called a training instance (or sample). In our case, the task T, for instance, can be the prediction of the stress for a given strain increment and internal state of the material. The experience E is the training data-set and the performance measure P can be the prediction error. Machine Learning is a general term to describe a large spectrum",Chunk 21: Machine Learning is a branch of computer science that aims to train computers on a large scale.
22,"of numerical methods. Some of them offer very rich interpolation spaces, which, in theory, could be used for approximating complicated functions belonging to uncommon spaces. Here we focus on the method of Artificial Neural Networks (ANNs), which is considered to be a sub-class of Machine Learning methods. According to Cybenko (1989) and Chen and Chen (1995), ANNs have proved to be universal approximators, due to their rich interpolation","Chunk 22: In this paper, we deal with the problem of approximators."
23,"space. Therefore, they seem to be a useful and promising tool for approximating constitutive laws of many materials (e.g. sand, masonry, alloys, ceramics, composites etc.). Recognizing this potential, there is an increasing amount of new literature employing ANNS successfully in constitutive modeling of non-linear materials from model identification based on experiments and detailed numerical simulations. Starting form the seminal work",Chunk 23: Non-linear models of materials are emerging as an important tool in the study of materials science and engineering.
24,"of Ghaboussi et al. (1991) and without being exhaustive, we refer to Ghaboussi and Sidarta (1998); Lefik and Schrefler (2003); Jung and Ghaboussi (2006); Settgast et al. (2019); Liu and Wu (2019); Lu et al. (2019); Xu et al. (2020); Huang et al. (2020); Liu and Wu (2019); Gajek et al. (2020); Gorji et al. (2020) and references therein. The main idea in these works is to appropriately train ANNs, feeding them with material data, and predict the material",Chunk 24: We present a series of works on artificial neural networks (ANNs).
25,"response at the material point level. In this sense ANNs can be seen as rich interpolation spaces, able to represent complex material behavior. For instance, we record the works of Heider et al. (2020); Ghavamian and Simone (2019); Mozaffar et al. (2019); Frankel et al. (2019); Gonzalez et al. (2019); Gorji et al. (2020), who demonstrated that Recurrent Neural Networks (RNNs), an extension of neural networks, can be particularly useful for modeling","Chunk 25: Neural networks (ANNs) have the potential to represent a wide range of phenomena, such as:"
26,"path-dependent plasticity models. RNNs, differently from ANNS, process time sequences. As suggested by Gorji et al. (2020), the history-dependent variables of RNNs can potentially mimic the role of physical quantities. The Boundary Value Problem (BVP), set to determine the behavior of a solid under mechanical and/or multiphysics couplings, is then solved by replacing the standard constitutive equations","Chunk 26: In this paper, a novel model of solid under mechanical couplings, known as Reaction Neural Networks (RNNs), is proposed"
27,"or algorithms by the trained ANN. This replacement is straightforward and non-intrusive in Finite Element (FE) codes. We record, without being exhaustive, the successful embedding of ANNs as material description subroutines in FE codes by Lefik and Schrefler (2003); Jung and Ghaboussi (2006); Lefik et al. (2009); Settgast et al. (2019). Ghavamian and Simone (2019) further implemented ANNs in a FE2 scheme for accelerating multiscale FE",Chunk 27: The aim of this paper is to replace the following:
28,"simulations for materials displaying strain softening, with Perzyna viscoplaticity model. It is worth emphasizing that the aforementioned data-driven approaches are different from another promising data-driven method (i.e., data driven computing Kirchdoerfer and Ortiz, 2016) in which the BVP is solved directly from experimental material data (measurements), bypassing the empirical material modeling step, involving the calibration of constitutive","Chunk 28: In this paper, we present a new data-driven approach to the buckling stiffness model (BVP) for materials, based on"
29,"parameters (Kirchdoerfer and Ortiz, 2016; Ibanez et al., 2017; Kirchdoerfer and Ortiz, 2018,?; Ibanez et al., 2018; Eggersmann et al., 2019). While data-driven computing can be extremely powerful in many applications (Eggersmann et al., 2019), the first class of methods above-mentioned (based on the constitutive behavior at the material point level) 3",Chunk 29: The aim of this paper is to introduce a new class of methods for data-driven computing.
30,"can be advantageous when modeling complex and abstract constitutive behaviors, which are not a priori known. Moreover, they can be used even if the BVP does not have a unique solution due to important non-linearities and bifurcation phenomena (e.g. loss of uniqueness, strain localization at the length of interest, multiphysics, runway instabilities etc.). Nevertheless, until now ANNs for constitutive modeling are mainly used as a 'black-box'",Chunk 30: ANNs are 'black-box' solvers that can be used in the context of classical buckling theory (BVP).
31,"mathematical operator, which once trained on available data-sets, does not embody the basic laws of thermodynamics. As a result, vast amount of high quality data (e.g. with reduced noise and free of outliers) are needed to enable ANNs to identify and learn the underlying thermodynamic laws. Moreover, nothing guarantees that the predictions of trained ANNs will be thermodynamically consistent, especially for unseen data.",Chunk 31: The aim of this paper is to train a new class of artificial intelligence (ANNs) based on the principles of thermodynamics.
32,"In this paper, we encode the two basic laws of thermodynamics in the architecture of neural networks. This assures thermodynamically consistent predictions, even for unseen data (which can exceed the range of training data-sets). Therefore, we assure thermodynamicaly consistent network's predictions, both for seen and unseen data (which can exceed the range of the training data-sets). Moreover, our network does not have to identify/learn the",Chunk 32: The goal of this paper is to improve the prediction accuracy of neural networks.
33,"underlying pattern of thermodynamical laws. Consequently, smaller data-sets are needed in principle, the training is more efficient and the accuracy of the predictions higher. The price to pay, in comparison with existing approaches, is the need of two additional scalar functions (outputs) in the training data-set. These are the free-energy and the dissipation rate. However, these quantities are easily accessible in micromechanical simulations (e.g.",Chunk 33: The aim of this paper is to develop a new approach for the training and prediction of thermodynamical laws in micromechanical simulations.
34,"Nitka et al., 2011; Eijnden et al., 2016; Feyel, 2003) and can also be obtained experimentally in some cases. Then, based on classical derivations in thermodynamics (e.g. Houlsby and Puzrin, 2007; Einav, 2012) specific interconnections are programmed inside our ANN architecture to impose the necessary thermodynamic restrictions. These thermodynamic restrictions concern the stresses and internal state variables and their relation with the free-","Chunk 34: In our work, we have developed an artificial neural network (ANN) that can be used to constrain the free-to-free relationships between stresses"
35,"energy and the dissipation rate. Our approach is inspired by the so-called Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019), in which reverse-mode autodiff (Baydin et al., 2017) is used, allowing the numerical calculation of the derivatives of an ANN with respect to its inputs. The calculation of these derivatives, imposes some numerical requirements regarding","Chunk 35: In this paper, we present a new method for the calculation of the derivatives of an anti-Stokes neural network (ANN"
36,"the mathematical class of the activation functions to be used. More specifically, the internal ANN restrictions, derived from the first law of thermodynamics, require activation functions whose second gradient does not vanish. Otherwise, the problem of second-order vanishing gradients, as it is called here (cf. classical vanishing gradients problem in ANNs, e.g. Geron, 2015), can inhibit back-propagation and make training to fail. This new problem and its",Chunk 36: In this paper we present a new problem for the training of artificial neural networks (ANNs).
37,"remedy is extensively explored and discussed herein. For the sake of simplicity and for distinguishing our approach from existing ones, we call the proposed ANN architecture Thermodynamics-based Artificial Neural Networks (TANNs). In our opinion TANNS should be the starting point for data-driven and physics-based constitutive modeling at the material point level.",Chunk 37: In this paper we propose a new architecture for artificial neural networks (ANNs) based on the Thermodynamics-based
38,"The paper is structured as follows. Section 2 presents a brief summary of the theoretical background of thermodynamics. In Section 3 an overview of the methodology proposed and architecture of TANNs is given. The main differences with classical, standard ANNs for 4 material constitutive modeling are also discussed. Particular attention is given to the choice",Chunk 38: The aim of this paper is to provide an overview of the state-of-the-art in the design and implementation of
39,"of activation functions and the issue of second-order vanishing gradient is investigated in detail. Generation of material data-sets, with which the training of ANNs is performed, is presented in Section 4. In a first phase, we apply TANNs for the constitutive modeling of three-dimensional elasto-plastic material models, Section 5. In particular, we consider both hyper-plasticity models and smoother hypo-plasticity ones. Extensive comparisons with","Chunk 39: In this paper, we consider the training of artificial neural networks (ANNs) for the modelling of three-dimensional elasto-plastic"
40,"standard ANNs, which are not based on thermodynamics, are also presented. In a second phase, we investigate the performance and robustness of TANNS with the presence of noise in the training data, Section 6. This is achieved by generating a set of pseudo-experimental data, adding several levels of artificial noise. Supplementary figures and data are available in Supplementary data file. For the implementation of Artificial Neural Networks and","Chunk 40: In the first phase of this work, we investigate the performance and robustness of thermodynamics-based artificial neural networks (TANNS), Section 4."
41,"Thermodynamic-based Artificial Neural Networks, we leverage Tensorflow v2.0. All code accompanying this manuscript is available upon request. 2. Thermodynamics principles: energy conservation and dissipation inequality 2.1. Energy conservation A convenient way to express the (local) energy conservation is",Chunk 41: This manuscript is copyrighted and may not be reproduced in any form without the written permission of
42,"pè = a VSymy divq + ph, (1) with p being the material density; e the specific internal energy (per unit mass); a the Cauchy stress tensor; Vv the spatial velocity gradient tensor; 9 the rate of heat flux per unit area; h the specific energy source (supply) per unit mass, and ""."" denotes contraction",Chunk 42: The coefficients of the Cauchy stress tensor are as follows:
43,of adjacent indices. 2.2. Second principle The second law of thermodynamics can be formulated in terms of the local Clausius- Duhem inequality ph,Chunk 43: The second law of thermodynamics can be formulated in
44,4 ps N div (2) 8,Chunk 44: BBC Sport takes a look at some of the key
45,"A with S being the specific (per unit mass) entropy; h/0 and -(q n)/0 the rate of entropy supply and flux, respectively. By removing the heat supply h between the energy equation (1) and the entropy inequality (2) leads to q VO",Chunk 45: The entropy inequality between the energy equation (1) and the entropy inequality (2) is given as the following:
46,"p(es - é) + a VSymy N 0, (3) 8 where the first two terms represent the rate of mechanical dissipation D = p(0s = è) +",Chunk 46: p(es - é) + a VSymy
47,"a . VSymy and the latter the thermal dissipation rate, i.e., Dth = 9-V8 The thermal dissipation is non-negative because heat only flows from regions of higher temperature to lower temperature-that is, the heat flux 9 is always in the direction of the negative thermal gradient. As it follows we argue that the mechanical dissipation rate must itself be non- negative (point-wise), i.e., D N 0.",Chunk 47: The thermal dissipation rate and the mechanical dissipation rate are both non-negative.
48,5 2.3. Dissipation function The definition of the (mechanical) dissipation rate D leads to pé = pes + 0. VSymy - D. (4),Chunk 48: The following is a list of some of the most
49,"Let us define the specific (per unit volume) internal energy E = pe and entropy S = ps and further assume constant material density, i.e., dp = 0-that is, É = pè and $ = ps. We shall assume a small strain regime, i.e., Vu A 1, with 8 : VSymu the small strain tensor, where u is the displacement vector field, and & : VSymy its rate of change. Equation (4) hence becomes","Chunk 49: In this paper, we are going to solve the equation for the internal energy of a material, i.e., its entropy."
50,"É eS + a -D. (5) Let assume a strain-rate independent material such that the energy potential is E:=E(S,,Z), (6)",Chunk 50:  eS + a -D.
51,"and the dissipation rate, being a first-order homogeneous function of Z, is D:=D(S.6,Z.Z). (7) where Z = (Gis 5N) denotes a set of N (additional) internal state variables, Sis i = 1, N. We define here (thermodynamic) state variables those macroscopic quantities",Chunk 51: The coefficients of the coefficients of the coefficients of the coefficients of the coefficients of the coefficients of the
52,"characterizing the state of a system, see e.g. Maugin and Muschik (1994). The physical representation of Si is not a priori prescribed. For instance, in the case of isotropic damage, 5 is a scalar; in anistotropic damage, a tensor; in the case of elasto-plasticity, a second order tensor, etc. The generalization to a finite-strain formulation can be achieved by considering the deformation gradient, F, and the first Piola-Kirchhoff tensor, P, as strain",Chunk 52: A finite-strain is a tensor that represents the state of a system.
53,"and stress measures, respectively (see e.g. Mariano and Galano, 2015; Anand et al., 2012). Nevertheless, as it would presented in Section 3, an incremental formulation of the material response is herein adopted. Therefore, the hypothesis of a small strain regime is usually realistic, at least for a large class of materials and an updated Lagrangian scheme. Time differentiation of the internal energy gives",Chunk 53: The material response to a small strain regime is investigated in the context of a large class of materials and an updated Lagrangian scheme.
54,OE QE OE É = & +,Chunk 54:     
55,"(8) as ds 1=1 05 which is equal to (5) and, grouping terms, it leads to",Chunk 55: The winning numbers in Saturday's drawing of the
56,QE OE OE + + =,Chunk 56: BBC Sport looks at some of the best images from
57,"(9) as dE as The arbitrariness of $, é, and 5 leads to the following relations","Chunk 57: The arbitrariness of $, é"
58,OE 8 = (10a) as QE,Chunk 58: BBC Sport looks at some of the key statistics behind
59,a (10b) ds N OE >,Chunk 59: BBC Sport takes a look back at some of the
60,"5i+D=0. (10c) i=1 a5; 6 Further introducing the thermodynamic stress, conjugate to Si X = (X1, ,XN), with",Chunk 60: The coefficients of the Bose-Einstein condensate (B
61,"OE Xi : Vie[1,N], (11) asi",Chunk 61: China's National People's Congress (NPC
62,"we obtain the following, alternative definition of the dissipation D=2x5 (12) i=1 2.4. Isothermal processes",Chunk 62: In this paper we present the results of a study
63,"In the case of isothermal process, the (specific) Helmholtz free-energy, F : E - Se = F(,,Z), which is the Legendre transform conjugate of e, is preferable. In this case, the dissipation rate is such that D : D(,,Z,2). The equations presented above (9-30) still hold (by replacing E with F) OF","Chunk 63: The equation for isothermal process, E - Se = F(,,Z), which is the Legendre transform"
64,OF OF S = a X5,Chunk 64: Find out more at www.bbc.co.
65,(13) 80 ds a5i 3. Thermodynamic-based Artificial Neural Networks,"Chunk 65: Researchers at the University of California, Los Angeles,"
66,"Within the framework of ANN or RNN material models, we can distinguish two main classes. The first consists of direct, so-called ""black-box"", approaches, where the information flow passes through the machine learning tool which operates as a mere regression operator, see e.g. Ghaboussi et al. (1991); Lefik and Schrefler (2003). The second class coincides with ANN and/or RNN models incorporating some knowledge in an informed, guided graph","Chunk 66: In this paper, we study the relationship between artificial neural networks (ANNs) and graph-based models."
67,"with intermediate history-dependent variables or detecting history-dependent features, see Heider et al. (2020); Mozaffar et al. (2019); Gorji et al. (2020), among others. Whilst the latter case has demonstrated to be extremely successful for path-dependent plasticity models, both classes are affected by the lack of physics, being the predictions not always compatible with thermodynamic principles (at least). Figure la depicts the direct approach",Chunk 67: This paper presents a direct approach to the problem of model-based plasticity.
68,AE Aa AE Aa T,Chunk 68: AE Aa AE Aa Aa Aa
69,0 Da I Qt AE,Chunk 69: A look at some of the best images from this
70,"(b) informed neural network (i- (c) informed neural network (i- (a) black-box (BB) network. NN1). NN2). Figure 1: Examples of direct, black-box (BB) (a) and informed (b, c) neural networks for material laws modeling. Inputs are highlighted in gray (0), outputs in black (0)","Chunk 70: Figure 1: Examples of direct, black-box (a) and informed (b) neural networks for"
71,"7 (BB), in which ANNs, usually Feed-Forward Neural Networks (FFNNs), are used to predict the stress increment, (output, 0) O = Ao = o+Ar a', from the input I = (E,AE), being g' the precedent strain and As its increment. In concise form, we write O = BB@ I. In this scheme, g' and As can be regarded as the state variables, namely the ANN state variables","Chunk 71: In this paper, we present a novel artificial intelligence (ANN) scheme for predicting the stress increment in a precedent strain."
72,"(not necessarily coinciding with those introduced in Sect. 2), on which the updated material stress depends on. Two examples of guided, informed ANNS, either FFNNS or RNNS, are illustrated in Figures 1b and lc. In both cases, the neural network intrinsically accounts for path-dependency, see e.g. Heider et al. (2020), making sequence of predictions of the main output. The network i-NN1 makes use of the last predicted output, i.e., a', to make","Chunk 72: guided, informed and path-dependency-based artificial neural networks (ANNS) have been developed for the modelling of material properties, i.e."
73,"predictions of the next output, O = Ao. The inputs are hence I = (E,AE, a'). We shall notice that, differently from BB, the stress at the precedent state, d', is also considered to be an ANN state variable. Other alternatives exist in the selection of the ANN variables of state. One may chose, as we shall see in Section 5, thermodynamic) state variables to be ANN state variables.","Chunk 73: In this chapter, we will see how the stress at the precedent state, d', is considered to be an ANN state variable."
74,"In the case of temperature-dependent material response, the second case (1-NN2) allows to make predictions that depend on the precedent temperature state, 8, namely O = i-NN2@ I, with I = (E,AE, a',0) and O = (Ac, A8). The main aim of this work is to change the classical paradigm of data-driven ANN material modeling into physics-based ANN material modeling. We propose a new class","Chunk 74: We present two new classes of artificial intelligence-based material models, one for temperature-dependent material response and one for non-temperature-dependent material response"
75,"of ANNs based on thermodynamics, which are Thermodynamic-based Artificial Neural Networks (TANNs). By exploiting the theoretical background presented in Section 2, we propose neural networks which, by definition, respect the thermodynamic principles, holding true for any class of material. In this framework, TANNS posses the special feature that the entire constitutive response of a material can be derived from definition of only","Chunk 75: In this paper, we present a new framework for artificial neural networks (ANNs)."
76,"two (pseudo-) potential functions: an energy potential and a dissipation pseudo-potential (Houlsby and Puzrin, 2007). TANNS are fed with thermodynamics ""information"" by relying on the automatic differentiation technique (Baydin et al., 2017) to differentiate neural networks outputs with respect to their inputs. This strategy allows to construct a general framework of neural networks material models which, in principle, can be exploited to","Chunk 76: In this paper, we develop a novel material model for topological acoustic networks (TANNS) based on the idea of two potential functions."
77,"predict the behavior of any material and assure that the predictions of TANNS will be thermodynamically consistent even for inputs that exceed the training range of data. In this paper, we only focus on strain-rate independent processes. Moreover, our approach can be extended, following the developments in Houlsby and Puzrin (2000), to materials showing viscosity and strain-rate dependency.","Chunk 77: In this paper, we develop a new method for predicting the behaviour of materials, called TANNS, based on the assumption that the"
78,"The model relies on an incremental formulation and can be used in existing Finite Element formulations (among others), see e.g. Lefik and Schrefler (2003). Figure 2 illustrates the scheme of TANNS. The model inputs are the strain increment, the previous material state at time t, which is identified herein through the material stress, a', temperature, e, and the internal state variables, 5 as well as the time increment At, namely I = (,A6,0,0.6,A0).","Chunk 78: This paper presents a novel strain-induced material state model (TANNS), which can be used to study the behaviour of a variety of materials."
79,"The primary outputs, O1, are internal variables increment, AKi, the temperature increment, AB, and the energy potential at time f+At, F'+Ar, i.e. Oi = (AKi, AB, FA). Secondary outputs, O2-that is, outputs computed by differentiation of the neural network with respect to the 8 inputs-are the stress increment, Ao, and the dissipation rate, D'Ar, 7 which we denote as","Chunk 79: In this paper, we derive a set of outputs from a set of inputs, each of which consists of 8 variables."
80,"O2 = VIOi = (Ao, D'+A). The class of neural networks we propose differs from the previous ones by the fact that the quantity of main interest, i.e., the stress increment, is obtained as a derived one, which intrinsically satisfies the first principle of thermodynamics (and, as we shall see, the second principle, as well). In the following, we briefly recall the basic concepts of artificial neural","Chunk 80: In this paper, we present a new class of artificial neural networks, in which the main interest of the network is the stress increment."
81,"networks (paragraph 3.1), we then focus on the issue of the second-order vanishing gradients that may afflict the training and the performance of an ANN model (paragraph 3.2). In particular, it is shown that, in the framework of Thermodynamic-based Artificial Neural Networks, particular attention has to be paid to the selection of activation functions. Finally, we present in detail the architecture of our model (paragraph 3.3).","Chunk 81: In the first part of this paper, we show how to train an artificial neural network (ANN) in the presence of a perturbation (paragraph 2.8)."
82,"At AE Aa 9 Figure 2: Schematic architecture of TANN. Inputs are highlighted in gray (C); outputs in black, (e) and","Chunk 82: Figure 1: At AE Aa 9a, the"
83,"and intermediate quantities in white (O). Dashed lines represent definitions, while arrows are used to denote neural network links. 3.1. Artificial neural networks overview We give herein a brief overview of the basic concepts of ANNs and in particular FFNNS. For more details, we refer to Hu and Hwang (2002) and Géron (2019). ANNs can be",Chunk 83: The following table shows the basic concepts of artificial neural networks (ANNs).
84,"regarded as non-linear operators, composed of an assembly of mutually connected processing units-nodes-, which take an input signal I and return the output 0, namely O = ANN@I. (14) ANNs consist of at least three types of layers: input, output and hidden layers, with equal",Chunk 84: An ANN is a signal processing unit.
85,"or different number of nodes. Figure 3 depicts a network composed of one hidden layer, with 3 nodes, an input layer with 2 inputs, and an output layer with 1 node. When an ANN has two or more hidden layers, it is called a deep neural network (Géron, 2019). Denoting the input array with I = (i), with t = 1,2...,n1 (nr is the number of inputs), and the outputs with O = 0;) with j = 1,2.. - ,no (no is the number of outputs), the signal flows from layer",Chunk 85: An ANN is a network composed of several hidden layers.
86,(1- 1) to layer (I) according to (I-1) PP A) Z) with k wip (-1) + (15),Chunk 86: To layer (I) according to (I-1)
87,"9 where PP are the outputs of node k, at layer (I); S is the activation function of layer (); ne"" is the number of neurons in layer (1 - 1); wp are the weights between the s-th node in layer (1-1 1) and the k-th node in layer (); and b) are the biases of layer (I). With reference to Figure 3, the output is given by","Chunk 87: Figure 9 shows the relationship between the outputs of node k, at layer (I), and the activation function of layer."
88,"0= (0) z(o)) with z(o) = Zr w/2) (1) + b(2) p with Zr (1) Xt + where the activation function of the output layer, A(out), is a linear function, as in most of","Chunk 88: The functions of the output layer, A(out), and the activation function"
89,"the cases for regression problems. The weights and biases of interconnections are adjusted, in an iterative procedure (gradient descent algorithm Géron, 2019), to minimize the error between the benchmark, 0, and prediction, O, that is measured by a loss function, L. In the following, the Mean (over a set of N samples) Absolute Error (MAE) is used as loss function, i.e.,","Chunk 89: In this paper, we present a new method for the estimation of the mean error between a benchmark, 0, and a prediction, O,"
90,"EAI 1Oi-O: L= (16) N where i = 1,2,...N. The errors related to each node of the output layer are hence back-",Chunk 90: The following table shows the errors that can be caused
91,"propagated to the nodes in the hidden layers and used to calculate the gradients of the loss function, namely af a1-me aplm aL awkm aw/"" at-m) apl-m) I-m+I)",Chunk 91: propagated to the nodes in the hidden layers and used to calculate
92,"(17) aL az-mD) ap-meD) aL apl-m) j=1 apl-m) atmD ap-mep which are then used to update weights and biases, and force the minimization of the loss","Chunk 92: Researchers at the University of California, Los Angeles, have"
93,"function values, i.e. af wP-Rw = wp E (18) àw/p'",Chunk 93: Function af wP-R-w =
94,"where E is the so-called learning rate. The weights and biases updating, the so-called training process, is performed on a subset of the input-output data-set, defined as training set, known from experimental tests or numerical simulations of the phenomenon investigated. The ANN is trained. The training process is stopped as the loss function is below a specific tolerance. Then a test set, a subset of the input-output data-set different to the training set, is used",Chunk 94: The ANN is trained on a subset of the input-output data-set where E is the so-called learning rate.
95,"to check the error of the network predictions. Once the ANN is trained, it is used in recall mode to obtain the output of the problem at hand. Due to their rich interpolation space, ANNs have proved to be universal approximators, see e.g. Cybenko (1989); Chen and Chen (1995), although the choice of hyper-parameters, such as the number of neurons, the network topology, the weights, etc. are problem-dependent.","Chunk 95: An adjoint neural network (ANN) is a machine-learning algorithm that trains an approximator of a problem, e.g."
96,"The same stands for the activation functions, which may be chosen to have some desirable properties of non-linearity, differentiation, monotonicity, etc. Most of these properties stem 10 P 1",Chunk 96: The periodic table of the periodic table contains the properties of the
97,output input layer layer hidden,Chunk 97: BBC News NI takes a look at some of the
98,"layer (1) Figure 3: Graph illustration of an ANN structure with two inputs, one output, and one hidden layer with three nodes. from issues related to the gradient descent algorithm and the so-called (first-order) vanishing gradient problem. As it follows, we briefly present this well-known issue and we further give","Chunk 98: In our paper, we present a solution to a well-known problem in artificial neural networks (ANNs)."
99,"insights in a variation of it: the second-order vanishing gradient. 3.2. First- and second-order vanishing gradients During the training process, if the gradient of the loss function with respect to a certain weight tends to zero-that is, see Eq. (18), when  = ap? /az & 0 (with A' the first- derivative of the activation function with respect to its arguments)-the update operation can",Chunk 99: The first-order vanishing gradient is a function of the loss function with respect to a certain weight.
100,"fail, and weights and biases are not updated. In this case, we have the so-called first-order vanishing gradient (Géron, 2019). Figure 4 displays some of the most common activation A() A() A()","Chunk 100: In this section, we are going to look at some of the most"
101,"A() tanh(:) max(0,-) A(: 0) A(: 20)","Chunk 101: A tanh(:) max(0,"
102,A'() A'() A'() A' (z) 5 cosk-(),Chunk 102: A' A' A' A' A'
103,"05 max(0,1) A'(x<0)=e (20)=1 Figure 4: Some of the most common activation functions and their first-order gradient. From left to right: the",Chunk 103: Some of the most common activation functions and their first
104,"logistic (sigmoid) function, the hyperbolic tangent, the Rectified Linear Unit (ReLU), and the Exponential Linear Unit (ELU). functions and their derivatives-that is, the logistic (sigmoid) function, the hyperbolic tangent, the Rectified Linear Unit (ReLU), and the Exponential Linear Unit (ELU). The sigmoid function is S-shaped, continuous, differentiable, its output values range from 0 to 1, and its","Chunk 104: functions and their derivatives-that is, the logistic (sigmoid) function, the hyperbolic tangent, the Rectified Linear"
105,"first-order gradient (derivative) assumes values much smaller than 1. When inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close 11 to 0. Thus when backpropagation kicks in, it has virtually no gradient to propagate back through the network, which is problematic for training. The hyperbolic tangent activation","Chunk 105: In this paper, we show how the first-order gradient function can be used to train neural networks."
106,"function is very similar to the sigmoid, but it is centered at zero allowing to maintain the output values within a normalized range (between -1 and 1). Nevertheless, it suffers from saturated gradients (at Z = 0, for Z <K -1 and Z >> 1). ReLU is continuous but not differentiable at Z = 0. Nevertheless it is an unsaturated activation function for positive values of Z (its gradient has no maximum) and, therefore, it allows to avoid vanishing gradient",Chunk 106: The coefficients of the activation functions Z and ReLU are given by the coefficients of the sigmoid.
107,"issues for Z > 0. Nevertheless, it suffers from a problem known as the dying ReLUs: during training, some neurons are effectively deactivated, meaning they stop outputting anything other than 0 (for Z < 0). To this purpose many variants exist. The ELU activation, for instance, takes on negative values when Z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradient problem, as discussed earlier.","Chunk 107: In this paper, I present a solution to the vanishing gradient problem of the ZLU."
108,"Second, it has a nonzero gradient for Z < 0, which avoids the dying units issue. Finally, the function is smooth everywhere, including Z = 0, which helps speed up gradient descent. When dealing with TANNS, second-order vanishing gradients can appear. This is a new concept and, in order to illustrate it, we will use a simple example. Assume an ANN which takes as input some I = x and returns (a) O1 = x2 and (b) its derivative with respect to the","Chunk 108: In this paper, we are going to look at a function called TANNS, which has three main features: first, it has a first-order vanishing gradient for Z  0,"
109,"input, i.e., O2 = VIOi = 2x (see Figure 5). Let us consider one hidden layer, with activation function A and N, nodes. The activation function of the single output layer, which returns 3?, is assumed to be linear. In this case, the output (a) is given by 01 =plo) = S(0) e)) 01","Chunk 109: In this paper, we will consider a hidden layer, with activation function A and N, nodes."
110,o) + b(o) (19) 01 + 50,Chunk 110: The winning numbers in Saturday evening's drawing of
111,"The derivatives of the outputs with respect to the inputs can be easily computed, in this simple example, by taking advantage of the automatic (numerical) differentiation (Baydin et al., 2017). Output (b) is hence computed by the ANN as a01 àp(o) az(o) ap azP","Chunk 111: In this paper, we show how to use the ANN to compute the outputs of a set of"
112,"02 = V,Oi 5 aI àz(0) ap"" az"" aI (20)",Chunk 112: The winning numbers in Saturday evening's drawing of
113,"801 aI Consider the following loss function L= WoLo + Wviolv,0, where Lo and Lv,o are the loss functions corresponding to output Oi and O2 = Vy01,",Chunk 113: Consider the following loss function L= WoLo + W
114,"respectively. Regularized weights, Wo and WVy can be used to obtain comparable order 12 2r A() Figure 5: ANN which takes as input x and returns (a) O1 = x2 and (b) its derivative with respect to the",Chunk 114: Figure 5: ANN which takes as input x and returns O1 = x2 and
115,"input, i.e., ViOi = 2x, with one hidden layer whose activation function is A. of magnitude of the two loss functions. During training, weights and biases are updated according to Eq. (18) where the computed gradients are aL AL +","Chunk 115: In this paper, we present a new method for the estimation of the weights and biases of"
116,"Ev,o (2la) awlo) aL =","Chunk 116: To License This Clip, Click Here: http://"
117,LVio (21b) aw aL L,Chunk 117: BBC Wales Sport takes a look at some of the
118,"(21c) àb(o) aL - Ev,o (21d)",Chunk 118: BBC Sport takes a look back at some of the
119,"ab"" It follows, from relations (21b) and (21d), that the gradient descent algorithm needs the computation of both first- and second-order gradients of the activation function A. This particular result is a direct consequence of the minimization of the error between the gradient of the outputs with respect to the inputs, i.e. O2 = V/01, and the corresponding benchmark",Chunk 119: A gradient descent algorithm for the activation function A is presented.
120,"values, 2x. This is what we call second-order vanishing gradient problem. It is tantamount to the first-order variant, but it involves the second derivatives (and not only the first) of the activation functions in an ANN. With reference to Figure 4, none of the depicted, classical activation functions is suitable for such class of problems. Consequently, care must be taken in selecting activation functions that do not have second-order vanishing gradients. To this","Chunk 120: In this article, I will show you how to select activation functions that do not have second-order vanishing gradients."
121,"purpose, Appendix A presents an example illustrating the issue of second-order vanishing gradients and proper solutions are given to this problem. 3.3. Architecture of Thermodynamic-based Artificial Neural Networks Herein we detail the architecture and the internal steps/definitions TANNS are relying on. The architecture is detailed in Figure 6. The input vector is I = (,A6,0,0.4,A), the",Chunk 121: In this paper we discuss the architecture of Thermodynamic-based Artificial Neural Networks (TANNS).
122,"primary and secondary outputs are O = (AKi, AB, FHA) and VIo = (Ac, D'+A), respectively. TANN involves the following steps: 1. computation of the updated strain (definition): g'+Ar  g' + As 2. prediction of the kinematic variables and temperature increments with two sub-ANNs: AE = SNN@ (W,AE,d,8.)",Chunk 122: The tensor adversarial network (TANN) is a set of equations that can be used to predict the behaviour of
123,"and AB = SNNA@ (gAr A6,c,8,5) 13 3. computation of (a) the updated kinematic variables rates (backward finite difference approximation):",Chunk 123: This paper presents the results of (a) a
124,+1 AL At (b) the updated kinematic variables (definition): $+1:=5+ AE (c) the updated temperature (definition): 0+1 e' + AB 4. prediction of the updated energy potential:,"Chunk 124: Key words: kinematics, temperature, energy potential,"
125,"FAr = SNNP@(g+Ar S'Ar 8'A) 5. computation of the updated dissipation rate (definition, Eq. (13)): D'+Ar : aFA agAr k+Ar 6. computation of (a) the updated stress (definition, Eq. (13)): o1+Ar aFAr ag'HAT (b) the stress increment (definition): Ao : o+Ar a",Chunk 125: The following table shows the results for the following equations:
126,At At t+At +A) AE,Chunk 126: BBC Sport takes a look back at some of the
127,tA 6+At At at 4 SNNF,Chunk 127: Northern Ireland's Deputy First Minister Martin McGuinness has
128,"Aa AE SNN, Ot 1+At) SNNF","Chunk 128: All photographs  AFP, EPA, Getty Images"
129,"Aa AB Q++A) SNNS SNN, $","Chunk 129: All photographs courtesy of AFP, EPA, Getty Images"
130,"(a) non-isothermal processes. (b) isothermal processes. Figure 6: Architecture of TANNS: general case (a) and for isothermal processes (b). Inputs are highlighted in gray (0); outputs in black, C for direct ANN predictions and for derived outputs; and intermediate quantities (definitions) are in white (0) and (0). Relationships obtained from definitions are represented",Chunk 130: Figure 6: Architecture of TANNS: general case (a) and for isothermal processes.
131,"with dashed lines, while arrows denote ANNS. TANNS are thus composed of three sub-ANNs; SNNy predicts the internal variables increment, SNNe predicts the temperature increment (note that in case of the isothermal conditions, this component can be removed from the architecture, see Fig. 6b), and SNNF predicts the Helmholtz free-energy. The main output, the increment in stress, is computed","Chunk 131: The ANNs of the TANNS architecture are given as follows: SNNy, SNNe, and SNNF."
132,"according to expression (13), which stems from thermodynamic requirements. By virtue of the fact that the entire constitutive response of a material can be derived from definition of only two pseudo-)potential functions, the model is able to predict the stress increment from the knowledge of the energy potential (and the internal variables 5i). It is worth noticing that, differently from common approaches (cf. Sect. 3), the sub-network SNNF is required to",Chunk 132: This paper presents a novel model for the prediction of the energy potential of a material.
133,"learn a scalar quantity-that is, the Helmholtz free-energy potential. This offer compelling advantages. When dealing with ANNS, the curse of dimensions (increasing effort in training 14 and large amount of training data required) is an important issue when the studied problem passes to higher dimensions, see e.g. Bessa et al. (2017). Passing from 1D to 3D, for instance,",Chunk 133: The Helmholtz-Zentrum fr Naturkunde (Zentrum fr Naturkunde) is one of the
134,"increases the number of variables the ANNs need to learn. For stresses, from one single scalar value, in 1D, we pass to a vector with six-components, in 3D. The computational effort is thus not trivial. Nevertheless, TANNS are, in principle, less affected by these issues as the two pseude-)potentials, on which the entire set of predictions relies on, are scalar functions. The computation of dissipation, from expression (13), plays a double role. First, it assures","Chunk 134: In this paper, we show how dissipation, from expression (13), affects the learning of artificial neural networks (ANNs)."
135,"thermodynamic consistency of the predictions of TANNS (first law). Second, it brings the information to distinguish between reversible and irreversible processes, e.g. elasticity from plasticity/damage, etc., and it is trained to be positive or zero (second law). It is worth noticing that further improvements of the performance of TANNS may be obtained, as suggested in the work of Karpatne et al. (2017), by adding a physical inconsistency","Chunk 135: This paper presents a new approach to the study of elastic forces, one that brings two important benefits: first, it brings the information to distinguish between reversible and"
136,"term to the loss functions (e.g., with respect to dissipation). 4. Generation of data We present the procedures used to generate material data TANNS are trained with in the following applications (see Sect. 5). We distinguish two different strategies. The first one, based on the numerical integration of an incremental form of the constitutive","Chunk 136: In this paper, we describe the techniques used to generate material data TANNS."
137,"relations, is used to generate data for an hyper-plastic von Mises constitutive model with kinematic hardening (Houlsby and Puzrin, 2000, 2007). A different procedure is instead used to generate data for von Mises hypo-plasticity (Einav, 2012). In the case of hyper-plasticity models, we assume the Ziegler's orthogonality condition (see paragraph 4.1 and Ziegler, 2012; Houlsby and Puzrin, 2000, 2007), which, in general,","Chunk 137: The following procedure is used to generate data for hyper-plastic von Mises models with kinematic hardening (Houlsby and Puzrin, 2000,"
138,"it is not a strict requirement. Nevertheless, it is worth noticing that this restriction applies only on the generated data, and not on the ANN class here proposed. More precisely, TANN architecture still holds even for materials for which the Ziegler's normality condition does not apply. We shall recall that the aim is to demonstrate the advantages of thermodynamics- based neural networks with respect to classical approaches. Hence the restrictions, imposed",Chunk 138: We have imposed the following restrictions on the TANN architecture:
139,"by the orthogonality hypothesis for the generation of data, are expected not to affect the comparisons presented in Section 5. Hypo-plasticity is here used to show that the framework of thermodynamics encoded in TANNs is general and does not depend on restrictive assumptions such as the Ziegler's orthogonality condition afflicting hyper-plasticity. Furthermore, we consider the hypo-plastic","Chunk 139: In this paper, we consider the Hypo-plasticity hypothesis for the generation of data encoded in TANNs."
140,"material case to test TANNS against materials with a smooth response, which is more representative of realistic materials. 4.1. Incremental formulation 4.1.1. Hyper-plasticity Following the hyper-plasticity framework proposed in Einav et al. (2007), the thermo-",Chunk 140: This paper presents a case study to test TANNS against materials with a
141,"mechanical, non-linear, incremental constitutive relation for strain-rate independent materials, 15 undergoing infinitesimal strains, is here derived in the framework of isothermal processes (0 = cost). By differentiating the energy expressions (13) and rearranging the terms, we obtain the following non-linear incremental relations","Chunk 141: The non-linear incremental strain-rate relation for strain-rate independent materials, undergoing 15 infinite"
142,d=0eF-ét ZaF-5 (22a) -Xi = OyeF.ê+ > Ou4F. (22b) where the following notation is adopted,Chunk 142: The Latin phrase ZaF-5 has been changed to
143,aF aF a?F OEsF = deF =,Chunk 143:     
144,"OuAF = OEiyOEu dEiyask a4a5 Further, introducing the thermodynamic dissipative stresses xt = (X1, .. XN) and assuming","Chunk 144: In this paper, we present a new method for"
145,"the Ziegler's orthogonality condition (Ziegler, 2012), the following non-linear, incremental constitutive relation can be found Mly-o . & ify=0 E = (23)",Chunk 145: This paper presents the results of the Zieglerss
146,Mlyso . & else with OesF 2k ELF.(9 y à axk dy,Chunk 146:     
147,OgeF ZKOLAF. e OEsF] aXk E = Mly-0 =,Chunk 147:     
148,CE and Mlyco = OdeF (24) 0 B,Chunk 148: Match reports from the weekend's Premier League games
149,"Vg 0 B and . denotes the contraction of adjacent indices. In the above relations (23-24), whose derivation is presented in Appendix B, y = J(E, Z,X) is the yield function, 0 denotes a",Chunk 149: The following table shows the relationship between the following indices:
150,"quantity (scalar or tensorial, depending on the dimensionality of the internal variable set) equal to zero, and dy dy CE","Chunk 150: quantity (scalar or tensorial, depending"
151,"OyeF, ds ax; i=1 y",Chunk 151: BBC Wales Sport takes a look at some of the
152,y dy B = OyeF i=1 a5i ax; i=1 aX; -,Chunk 152:     
153,"aXk 4.1.2. Hypo-plasticity The theoretical framework used here to generate the hypo-plastic data can be found in Einav (2012). Einav (2012) proposed a new theoretical model, called hplastic, unifying hypo- and hyper-plasticity models. In particular, compared to standard hypo-plasticity, the",Chunk 153: This paper presents the results of the aXk 4.1.2 (hypo-plasticity
154,"incremental material formulation can be derived from (pseudo-) potentials. The h'plastic 16 model allows ease integration of the incremental constitutive equations, i.e., Eq. (5.15a) in Einav (2012). Here we use the following incremental equations (Eq.s 7.14 and 7.15 in Einav, 2012) for","Chunk 154: In this paper we present a h'plastic model, which can be used to derive material formulations"
155,"the relaxation strain rate (z) and stress increment (d), according to von Mises model:  a - z =",Chunk 155: The relaxation strain rate (z) and stress increment
156,(25a) 2k2 k 2 5- -,Chunk 156: BBC Sport takes a look back at some of the
157,- à = Kép + 2G (25b) k 2,Chunk 157:     
158,"where k represents the elastic limit in simple shear; S is a material parameter (s > 0); K and G are, respectively, the bulk and shear moduli; Ep is the mean strain rate; à and z are, respectively, the deviatoric total and relaxation strain rate tensors; and a' is the deviatoric stress. 4.2. Data generation",Chunk 158: The strain rate tensors for simple shear are given as the following:
159,"Data are generated in a Python environment, where SymPy and SciPy libraries are used for symbolic calculations and numerical integration. The accuracy of the generation process is 10-6 for strains and 10-4 MPa for stresses. For the case of hyer-plasticity, data are generated by identifying an initial state for the material at time t,","Chunk 159: Researchers at the University of Bristol have developed a method for the generation of strain and stress data for a variety of polymers, including"
160,"state at time t : E' = and g', Si 0","Chunk 160: , , , "
161,"and a given strain increment é, assuming constant and unitary time increment At = 1 (8 = Ag'). Numerical integration of the ordinary differential equations (23) is performed with an explicit solver (Bogacki and Shampine, 1989) to obtain the state at the new time t + At, i.e., o1+Ar","Chunk 161: The state of a strain at a new time t + At, i.e., o1+"
162,-XHAr state at time t + At : E'+Ar = 4HAr 2+At,Chunk 162:     
163,"For hypo-plasticity, data are generated similarly but only internal variables Si, deformation 8, and stress a are used to represent the material state at time t and t+ At, through numerical resolution of Eq.s (25a) and (25b). The training data play a crucial role for both the accuracy of the predictions and the generalization with respect to the ANN state variables, e.g., strain increments. The","Chunk 163: In the case of martensitic steel, data are generated in the form of internal variables Si, deformation 8, and stress to represent the material state at"
164,"generalization capability of a network is here defined as the ability to make predictions for loading paths different from those used in the training operation. Nevertheless, a significant 17 dependency on the ANN state variables is usually observed. This may result in a poor network generalization. In Lefik and Schrefler (2003), an improvement of the generalization","Chunk 164: In this paper, we report on the state-of-the-art of artificial neural networks (ANNs)."
165,"capability of ANNs is proposed. Artificial sub-sets of data, with zero strain increments, are added in the set of training data to force the network in learning that to zero input increments correspond zero output increments. In the available literature, strain-stress loading paths are commonly used in training. If recursive neural networks are used, feeding them with history variables (loading paths) is",Chunk 165: strain-stress loading paths for artificial neural networks (ANNs) are proposed.
166,"the only possible solution (see e.g. Mozaffar et al., 2019). Nevertheless, ANNs do not necessary need the data-sets to be (historical) paths. Herein, we generate data randomly. Conversely, this allows us to (1) improve the representativeness: of the material data and (2) improve the generalization of the network on the strain increments. For the hyper-plastic material model, the initial state, E and g, and","Chunk 166: In this paper, we present a novel artificial neural network (ANN) for the modelling of plastic waste."
167,"the strain increment, AE, are randomly generated from standard distributions with mean value equal to zero and standard deviation equal to Emax, Emaxi and AEmax respectively. The Cauchy and thermodynamic stresses, a' and X, as well as the internal variables 5i are then calculated to satisfy the constraint y' S 0. This incremental procedure is repeated for Nsamples, resulting in a set of Nsamples ordered pairs (E,8,Ae; EA), from which the corresponding","Chunk 167: In this paper we derive the strain increment for a set of Nsamples ordered pairs (E,8,Ae; EA), from which the corresponding strain increment is generated"
168,"energy potential and dissipation rate at time t + At are evaluated. For the case of hypo- plasticity, data are generated by random loading paths as the procedure aforementioned for hyper-plasticity is not applicable to the theoretical framework in Einav (2012), as no definition of yield surface is needed for the derivation of the incremental material constitutive law. Figure 7 depicts the sampling for one of the investigated applications (see paragraph",Chunk 168: In this work we investigate the properties of three-dimensional (3-D) amorphous silicon nitrides in terms of yield surface and hypo- plasticity.
169,5.1). 18 1500 - Perain 100001 = derain,"Chunk 169: All photographs courtesy of AFP, EPA, Getty Images"
170,1250 - Fval 8000 - dval 1000 D Perain,Chunk 170: BBC Sport takes a look at some of the key
171,D dtrain 6000 750 J Poa,"Chunk 171: All photographs courtesy of AFP, EPA, Getty Images"
172,D 4vr J 500 4000 250,Chunk 172: The world's most powerful 4x4 vehicle
173,2000 0 0 -3000-2000-1000 0 1000 2000 3000 0 25 50 75 100 125 150 175 200,Chunk 173: BBC Sport takes a look back at some of the
174,"p.pAr (MPa) q.q""ar (MPa) 800 - Strain Arain 600-",Chunk 174: Strain Arain 600 - Strain p.pAr
175,- Sval Kval 600 D Str 400,Chunk 175: BBC Sport takes a look back at some of the
176,D Arain J 400 p.val J Va,Chunk 176: The winners of this year's Isle of Man
177,200 200 0 -0.02 -0.01 0.00 0.01 0.02 0.00 0.01 0.02 0.03 0.04,Chunk 177: The winning numbers in Saturday evening's drawing of
178,"6,.6,a () d,ear (-) 800 = Fptrain - rain 600-","Chunk 178: The BBC's weather forecast for the weekend,"
179,- fpval - Eval 600 J D FAte ain 400-,"Chunk 179: All photographs courtesy of AFP, EPA, Getty Images"
180,- rain 400 E J fpval D LVay,Chunk 180: A selection of photos from around the world this week
181,200 200 0- -0.02 -0.01 0.00 0.01 0.02 0.00 0.01 0.02 0.03 0.04,Chunk 181: The winning numbers in Saturday evening's drawing of
182,"5-4ar () z',""A(-) - Flrain 4000 - Dn","Chunk 182: ,,,,"
183,6000 - FVy 3000 DVy 4000,Chunk 183: BBC Sport takes a look at some of the best
184,€ 2000 2000 1000 20 25 35,Chunk 184:     
185,0.0 0.5 1.0 1.5 2.0 2.5 prar (N-mm) DAr (N-mm/s) Figure 7: Sampling for material case H-1 (cf. Table 1). From top to bottom: mean and deviatoric stress (p and q); mean and deviatoric total deformation (Ep and e); mean and deviatoric plastic deformation (Sp and,Chunk 185: Figure 7: Sampling for material case H-1 (cf.
186,"z); energy (F) and dissipation rate (D). Training and validation data-sets are also distinguished. 19 5. Applications Herein we use TANNS to the modeling of multi-dimensional elasto-plastic materials and demonstrate their wide applicability and effectiveness. It is worth noticing that, even though",Chunk 186: The results of this study are presented in terms of the following:
187,"the applications here investigated consist of elasto-plastic materials, the proposed class of neural networks can be successfully applied (without any modification) to materials with different or more complex behavior, accounting e.g. for damage and/or other non-linearities (in the framework of strain-rate independent processes). In paragraph 5.1, von Mises hyper- plasticity is accounted for, considering perfect-plasticity, hardening and softening behaviors.",Chunk 187: A new class of neural networks has been proposed for the study of plasticity in materials.
188,"In paragraph 5.1, we further investigate hypo-plastic material models. In the examples presented herein, reference dependent variables, such as the total plastic strain, were considered. However, the internal state variables set Z, see Eq. (7), and, consequently, our approach are not limited to this kind of state variables. As it follows, the hyper-parameters (i.e., number of hidden layers, neurons, activation","Chunk 188: In paragraph 4 of this paper, we introduce a new class of material models, called hypo-plastic material models."
189,"functions, etc.) of the networks are selected to give the best predictions, while requiring minimum number of hidden layers and nodes per layer. This is accomplished by comparing the learning error on the set of test patterns, per each trial choice of the hyper-parameters. In each training process, we use early-stopping. In other words, training is stopped as the error of a validation set starts to increase while the learning error still decreases (Géron,","Chunk 189: In this paper, we show how to train networks on a set of hyper-parameters (i.e."
190,"2019). The validation set is used to avoid over-fitting of the training data. Throughout this Section relatively simple deep feed-forward neural networks architectures are used (with, at maximum, two hidden layers) and no additional regularization techniques are employed (e.g., L1/L2 penalties, dropout, etc.). Each numerical example is accompanied with a detailed discussion about the network architecture.","Chunk 190: In this Section, we present a validation set for a deep feed-forward neural network that is trained on a coarse-grained dataset"
191,"5.1. von Mises hyper-plasticity In order to illustrate the performance of TANNs, we use the simple von Mises elasto- plastic model with kinematic hardening and softening. The model can be derived from the following expressions of the energy potential and dissipation rate 9K","Chunk 191: In this paper, we study the properties of topological insulators (TANNs) in terms of"
192,F= p)-(Ep-5p)+ 2 H + G(e- ) (e +,Chunk 192: The winning numbers in Saturday evening's drawing of
193,"Z, 2 D=kV2 Vz-:, where k represents the elastic limit in simple shear; K and G are the bulk and shear moduli; H the hardening (softening) parameter; Ep and 5p are, respectively, the mean total and","Chunk 193: The coefficients of the shear parameters Z, 2 D, K, G, Ep"
194,"plastic deformation; and e and Z are, respectively, the total and plastic deviatoric strain tensors. The yield surface can be derived as shown in Appendix B (Houlsby and Puzrin, 2007) and is defined as y=D-X'-z= VX'.X - V2k S 0, (26)",Chunk 194: The coefficients of the strain tensors e and Z are used to derive the yield surface of
195,with Xij = 2G eij Zij) + Hzij- 20 Table 1: Material parameters for 3D elasto-plastic von Mises material. case K G,Chunk 195: Material parameters of 3D elasto-
196,k H (GPa) (GPa) (MPa) (GPa) H-1 167 77 140,"Chunk 196: All photographs  AFP, EPA, Getty Images"
197,0 H-2 167 77 140 -10 H-3 167 77 140,"Chunk 197: All photographs courtesy of AFP, EPA, Getty Images"
198,"10 5.1.1. Training Data are generated as detailed in Section 4. A total of 6000 data with random increments of deformation are generated. In order to improve the performance of the network in recall mode, additional sampling with random uni-axial and bi-axial loading paths are also used.",Chunk 198: This paper presents the results of a training scheme for the estimation of the deformation rate of a deformable object
199,"The samples are split into training (50%), validation (25%), and test (25%) sets. The sampling in terms of the mean and deviatoric stresses, P and 4, and deformations, Ep and e, is presented in Figure 7 for material case H-1 (perfect plasticity). We distinguish between training and validation sets. For the sake of simplicity, stress and deformation are converted in the principal axes frame of reference. Table 2 shows the mean, standard deviation, and",Chunk 199: In this paper we present the results of a large-scale numerical study of the properties of two-dimensional (2-D) microstructures.
200,"maximum values of the training data-sets. Adam optimizer with Nesterov's acceleration gradient (Dozat, 2016) is selected and a batch size of 10 samples is used. Data are normalized between -1 and 1. We use the Mean Absolute Error (MAE), and not the Mean Square Error (MSE), as loss function for each output in order to assure the same precision between data of low and high","Chunk 200: In this paper, we show how to train a Markov chain Monte Carlo Monte Carlo model."
201,"numerical values. Regularized weights are used to have consistent order of magnitude of different quantities involved in the loss functions. The network architecture is adapted to the size of the inputs and outputs, with respect to the mono-dimensional case. In particular, the sub-network SNN, consists of two hidden layers, with 48 neurons (leaky ReLU activation function), and three output layers, one per each","Chunk 201: In this paper, we present a novel spatio-temporal network architecture (SNN) for the analysis of loss functions in the brain."
202,"(principal) component of (increment of) 5. The sub-network S-NNF has one hidden layer with 36 neurons (activation ELU2). The output layers for both sub-networks have linear activation functions and biases set to zero. The resulting number of hyper-parameters is R 3000 (cf. Ghaboussi and Sidarta, 1998; ?; Lefik et al., 2009; Mozaffar et al., 2019).. Figure 8 displays the loss functions of each output as the training is performed, for material case","Chunk 202: In this paper, we train two sub-networks of neurons in the ventral striatum."
203,"H-1 (perfect plasticity). The early stopping rule assures convergence, after approximately 1000 epochs, with MAEs of the same order of magnitude for the 4 outputs, AK, F'+Ar, 1 Ac, and D'+A The adimensional MAE is approximately equal to 1 X 10-4 for all outputs at the end of the training. Similar behaviors are also recovered for cases H-2 (softening), H-3 (hardening).","Chunk 203: In this paper, we show how to recover plasticity-like phenomena in a Petri dish."
204,"5.1.2. Predictions in recall mode Once the network has been trained, it is used, in recall mode, to make predictions. We briefly present the performance of TANNs in predicting the material response for a random loading path. Figure 9 depicts the comparison with the target material model for material case H-1. The network displays extremely good performance and the ability to predict","Chunk 204: In this paper, we train a topological artificial intelligence network (TANN) to predict the material response to a random loading path."
205,"21 Table 2: Mean (y), standard deviation (st), and maximum values of the training data-sets. data H st",Chunk 205: The following table shows the mean and standard deviation of
206,max Ei () 8x 10-5 0.010 0.041 Asi () 2x 10-5 0.003 0.014,Chunk 206: BBC Sport takes a look back at some of the
207,5i () 8x 10-5 0.010 0.041 AG () 0 1x: 10-4 0.0011 ai (MPa),Chunk 207: The winning numbers in Saturday evening's drawing of
208,"-1.4 143 544 Ac, (MPa) 123",Chunk 208: A look back at some of the most memorable moments
209,7577 36744 F'Ar (N-mm) 1.82 2.72 37.9 D'+Ar (N-mm/s) 0.41 0.38 2.61,Chunk 209: Nasa's Deep Space Observatory (DSO)
210,le-1 Afitrain Afival le-2 Ftrain,Chunk 210: Le 1 Afitrain Afival le-2
211,Fval - le-3 Aditrain Adival le-4,Chunk 211: France's President Francois Hollande has said he will
212,Dtrain Dval 10 100 1000,"Chunk 212: All photographs courtesy of AFP, EPA, Getty Images"
213,"epochs Figure 8: Errors in terms of the adimensional Mean Absolute Error (MAE) of the predictions of TANN (loss functions), as the training is being performed, evaluated with respect to the training (train) and validation (val) sets. Weights and biases update are computed only on the training set. random loading path.",Chunk 213: Figure 8: Errors in terms of the adimensional Mean Absolute Error (MAE) of the predictions of TANN (
214,"5.1.3. TANN VS standard ANN. Generalization of the network Herein we investigate the performance of TANNS with respect to the classical approach of ANNs (Ghaboussi et al., 1991; Lefik and Schrefler, 2003), as well as the sensitivity with respect to the input variables range. Figure 10 displays the architecture of the network, ANN, with inputs I = (E, AEi,C, 5) and output O = (ALi, Aci), with i = 1,2,3 denoting the","Chunk 214: In this paper we introduce a new type of artificial intelligence (ANN), the topological adversarial network (TANNS)."
215,"principal components. The architecture is selected to give the best performance, preserving the same number of hyper-parameters between TANN and standard ANN. The network, ANN, consists of the two sub-networks, aNNy and aNNa, with two hidden layers, each one, leaky ReLU activation functions, and number of neurons per layer equal to 48. As for SNNy and SNNG, in aNNy and aNN three output layers (1 neuron each) are used, with linear",Chunk 215: A novel artificial neural network (ANN) has been developed to study the activity of neurons in the brain.
216,"activation functions and zero biases. In Figure 11 we present the comparison of the MAE of the network predictions with respect to the target values (training and validation data-sets). It is worth emphasizing that both ANNs and TANNs are dependent on the choice of the user, concerning, for instance, the hyper-parameters. Moreover, the actual configurations",Chunk 216: In this paper we present the results of our work on the prediction of user-generated content (UGC) using artificial neural
217,"of both networks may benefit of ltemative/extensions, such as RNNs. Nevertheless, the 22 le-2 0.5 0.0","Chunk 217: In this paper, we examine the impact of the"
218,3 A a -0.5 -1.0 -1.5,Chunk 218: The winning numbers in Saturday evening's drawing of
219,20 40 60 80 Increments () (a) random loading path. le3 5.0 le-3,Chunk 219: The following table lists the most common loading paths used
220,model model TANN 2.5 TANN € 0,Chunk 220: TANN is the world's best-selling
221,0.0 -1 a 6 G-5.0,Chunk 221: BBC Sport takes a look at some of the key
222,-2 -7.5 -3 -1.5 -1.0 -0.5 0.0 0.5 -1.5 -10 -0.5 0.0 0.5 E1 (),Chunk 222: BBC Sport takes a look at some of the key
223,le-2 61 (-) le-2 (b) stress and internal variable. lel,Chunk 223: The effect of stress on the brain's ability
224,1.5 model TANN 8. model TANN 6 1.0,Chunk 224: TANN has launched a new range of women'
225,0.5. A .0 5p (-) le-3,"Chunk 225: All photographs courtesy of AFP, EPA, Getty Images"
226,"Luky () le-3 (c) energy and dissipation rate. Figure 9: Predictions of TANN for a uni-axial random loading path, compared with the target constitutive model, case H-1, perfect plasticity: (a) loading path; (b) principal stress, 01, and internal variable, 51,","Chunk 226: Figure 9: Predictions of TANN for a uniaxial loading path, compared with the target model"
227,"predictions; (c) energy and dissipation rate predictions. following comparisons show the added value of our approach compared to standard ones that do not explicitly contain physics, as TANNS. We first compare the performance of both networks, TANNS and standard ANNS, in predicting the material response for cyclic isotropic loading paths (material case H-1, cf.","Chunk 227: In this paper we present a new approach for predicting the material response for cyclic isotropic loading paths, based on (a)"
228,"Table 1). A linear elastic material response is expected and retrieved. Figure 12 displays the stress predictions of TANNS and ANNs, compared with the target values, for different strain increments. It is worth mentioning that the standard approach of ANNs does not succeed in accurately predicting the elastic deformation range. Moreover, contrary to TANNS, the stress predictions of standard ANNS, depend strongly on the cyclic loading. As the network","Chunk 228: In this paper, we compare the stress predictions of TANNS and ANNs with the stress predictions of standard ANNS."
229,"is used recursively, in recall mode, the stress predictions rapidly become less and less precise, due to error accumulation. The performance of both networks is further compared for the following tri-axial loading 23 1+At","Chunk 229: In this paper, we compare the performance of two networks, one based on"
230,"Ae Ae Ac aNN, Aa a'","Chunk 230: All photographs  Getty Images, AFP, EPA"
231,ANN. +At $ - (a) ANN scheme.,Chunk 231: The scheme described in clause (a) of the
232,"(b) ANN architecture. Figure 10: Schematic (a) and full architecture (b) of the network, not based on thermodynamics, standard ANNS. Inputs are highlighted in gray (C), outputs in black (e) le-1 le-1",Chunk 232: Figure 10: (a) ANN architecture.
233,"TANN Ak,train TANN Abival ANN Aftrain le-2 le-2","Chunk 233: TANN Ak,train TANN Abival "
234,ANN Abival 14 le-3 TANN Acitrain le-34 TANN Adiyal,Chunk 234: France's women's rugby sevens team
235,ANN Acitrain le-4 ANN Adival 10 100 1000 10 100 1000,Chunk 235: ANN Acitrain le-4 ANN Ad
236,epochs epochs (a) mean absolute error of AE prediction (b) mean absolute error of Aci prediction Figure 11: Training of ANNs compared with TANNS evaluated with respect to the training (train) and,Chunk 236: Training of ANNs compared with TANNS evaluated with respect to
237,validation (val) sets. path nT ni Agi = As sgn cos,Chunk 237: path nT ni Agi = Assgn
238,"AE2 - AE3 = As sgn sin (27) 2N 2N with N = Emax/Ae, Emax = 2 X 10-3 + 1, and As = 1 X 10-5 + 1 X 10-1.",Chunk 238: Find out more at www.bbc.co.uk/bbc
239,"Figures 13 and 14 display the material response in terms of the principal stresses, 01 and 03, and inelastic strains, 51 and 53, respectively. We show in Figure 15 the energy and dissipation rate predicted by TANNS with those computed, with standard ANNs, directly using the corresponding definitions for the free-energy and dissipation rate, Eq. (5.1). The predictions of TANNS are in good agreement with the constitutive model, independently",Chunk 239: We show that TANNS predicts the free-energy and dissipation rate of the material response to a variety of stresses and inelastic strains.
240,"from the strain increment, which exceeds considerably the training range. Nevertheless, the performance of ANNs is found to be strongly affected by the values of As. For strain increments well inside the training range, i.e., As = 1 X 10-3, standard ANNs are well predict the material response. In particular, computing, through Eq. (5.1, the dissipation rate and energy from the ANNs' predictions reveals that ANNs can successfully predict","Chunk 240: In this paper, we study the performance of artificial neural networks (ANNs) on predicting the material response to a strain."
241,"output respecting the requirements of the thermodynamics. The first and second principles of thermodynamics are indirectly learned during training (on thermodynamic consistent data). However, standard ANNs perform poorly for strain increments smaller and larger than the ones at which it was trained (As = 1 X 103, cf. Table 2). And in these cases, 24","Chunk 241: In this paper, we show how standard ANNs can be used to train experimental systems."
242,"standard ANNs predict thermodynamically inconsistent outputs. The predictions of TANNS are, instead, always thermodynamically consistent. Moreover, the quantities of primary interest, such as the stress, the internal state variable, and the energy are in extremely good agreement with the reference model. The same stands also for the dissipation rate. We notice, once more, that its values are always positive, even when","Chunk 242: In this paper, we show that a new class of artificial neural networks (ANNs), called TANNS, can be used to predict"
243,"the network is used for predictions beyond the training range. Figure $3 displays the predictions for very small strain increments, i.e. As = 1x10-5. TANNS successfully still predict the response in this limiting case, while ANNs do not. Indeed, the training data were generated guaranteeing an accuracy of the order of 10-6 in terms of strains and such small strain increments are at the margin of the computing precision.","Chunk 243: In this paper we present a novel model for predicting the response of a strain to a given chemical reaction, called TANNS."
244,"In the Supplementary Material, we present the results of a uni-axial loading scenario, in Figures S1 and S2, for material case H-1 (perfect plasticity). Kinematic hardening and softening material cases and the predictions of TANNS and ANNs are shown in Figures $4-S9. It is worth noticing that in all the cases, even for very large strain increments-for which the predictions of the network in terms of dissipation rate differ from the target values-,","Chunk 244: In this paper, we present the results of a uni-axial loading scenario, in Figures S1 and S2, for material case H-1 (perfect plasticity)."
245,"TANNS successfully predict the Jacobian, i.e., aci dej (i,j = 1,2,3), in very good agreement with the reference model. This is of particular importance for numerical simulations with implicit algorithms. Therefore, TANNS can successfully replace complicated constitutive models or multiscale approaches, but considerably and safely decreasing the calculation cost, even when the requested increments are outside the training range.","Chunk 245: In this paper, we present TANNS, a new approach for the prediction of the Jacobian equation."
246,"We emphasize that the performance of TANNs and standard ANNs can be improved by increasing the dimension of the training data-sets, the number of the hyper-parameters (e.g. numbers of hidden layers, etc.). Nevertheless, the fundamental gap between the two approaches in assuring thermodynamically consistent quantities still persist. 25","Chunk 246: In this paper, we compare the performance of tensor-ANNs (TANNs) and standardANNs"
247,le3 e-3 1.0 model 3.0 model,Chunk 247: le3 e--3 1.0 model 3.0 model
248,TANN TANN ANN 2.5 ANN,Chunk 248: A look back at some of the most memorable moments
249,0.5 2.0 € 0.0 1.5 5.0.5,Chunk 249:     
250,1.0 d 0.5 model A -1.0,Chunk 250: BBC Sport takes a look back at some of the
251,0.0 TANN ANN 51 () le-3 Ep 6p (-),Chunk 251: TANN ANN 51 () le-3 Ep
252,le-3 Suku (-) le-5 (a) strain increment As = 1 X 10-3. le4,"Chunk 252: All photographs courtesy of AFP, EPA, Getty Images"
253,le3 el model TANN ANN,Chunk 253: le3 el model TANN ANN fue
254,3 0- 4 -2 model,Chunk 254: BBC Sport takes a look at some of the best
255,A - model -4 TANN 5 TANN,Chunk 255: A-model -4 TANN 5 TANN
256,0 ANN ANN -1.0 -0.5 0.0 0.5 1e1 1.0 -0.5 0.0 0.5 1.0 -6,Chunk 256: The winning numbers in Sunday evening's drawing of
257,E1 (-) Ep- 5p (-) le-1 Liku () le-3,Chunk 257: E1 (-) Ep. 5p -
258,(b) strain increment As = 1 X 10-2. le5 le5 le4 model,Chunk 258: The following table lists the most common strains of strain
259,0.00 4 TANN ANN -0.25 E 2,Chunk 259: A look at some of the top stories of the
260,0 -0.50 6 2-0.75 model,Chunk 260: BBC Sport takes a look back at some of the
261,- model -4 A-1.00 0 TANN TANN,Chunk 261: BBC Sport takes a look at some of the best
262,-6 ANN -1.25 ANN -1.0 -0.5 0.0 0.5 1.0 1.0 -0.5 0.0 0.5 1.0,Chunk 262:     
263,() Ep 6p (-) Sysu (-) le-2 (c) strain increment As = 1 X 10-1.,Chunk 263: BBC Sport takes a look back at some of the
264,"Figure 12: Comparison of the stress, energy, and dissipation predictions of TANNS and standard ANNs. Energy and dissipation for ANNS are computed according to Eq. (5.1), for the cyclic, isotropic loading path AEi = AE, = AE = As sgn (cos EN) -with N = Emax/AE, Emax = 2 X 10-3 (a), Emax = x10-1 (b), and Emax = 1 (c), for material case H-1 (perfect plasticity). Each row represents the prediction at different As increments. 26","Chunk 264: The stress, energy, and dissipation predictions of TANNS and standard ANNs are compared in Figure 12."
265,le3 le3 1.25 model 1.5 model TANN,"Chunk 265: The world's smallest car, the le3"
266,TANN 1.00 ANN ANN 1.0 *,"Chunk 266: ,,,,"
267,0.5 E 50.25 0.00 0.0,Chunk 267: The winning numbers in Saturday evening's drawing of
268,E1 () le-3 63 (-) le-3 (a) strain increment As = 1 X 10-4.,Chunk 268: E1 () le-3 63 (-)
269,le3 le3 1.25 model 1.5 model 1.00 TANN,"Chunk 269: The world's smallest car, the le3"
270,TANN ANN 1.0 ANN Ea 0.5,Chunk 270: A selection of photographs from around the world this week
271,50.25 6 0.00 0.0 -1 0,Chunk 271: The winning numbers in Saturday evening's drawing of
272,le-3 le-3 E1 () 53 (-) (b) strain increment As = 1 X 10-3.,Chunk 272: BBC Sport takes a look back at some of the
273,le4 le4 model model 6 TANN,Chunk 273: BBC Sport takes a look at some of the best
274,6. TANN ANN ANN a 4,Chunk 274: A look back at some of the most memorable moments
275,41  6 2- 6 0,Chunk 275: BBC Sport takes a look back at some of the
276,-1.0 -0.5 0.0 0.5 1e1 0.0 0.5 1.0 1.5 E1 (-) (-) 2e1,Chunk 276: BBC Sport looks at some of the key statistics from
277,E3 (c) strain increment As = 1 X 10-2. le5 le5 model,Chunk 277: E3 (c) strain increment As = 1
278,model 6 TANN 6- TANN ANN ANN,Chunk 278: All photographs courtesy of Getty Images and AFP.
279,4 a 6 2 6 -1.0 -0.5 0.0 0.5 1.0,Chunk 279: BBC Sport takes a look back at some of the
280,0.0 0.5 1.0 1.5 2.0 E1 () 3 (-) (a) strain increment As = 1 X 10-1. Figure 13: Comparison of the stress predictions of TANNS and standard ANNs with respect to the target,Chunk 280: Figure 13: Comparison of the stress predictions of TANNS and standard 
281,"values, for the tri-axial cyclic loading path, Eq. (27), for material case H-1 (perfect plasticity). Each row represents the prediction at different As increments. 27 le-3 1.0 le-3",Chunk 281: This table shows the predicted values for the cyclic loading path of
282,0.01 model TANN 0.8 -0.5 ANN 3,Chunk 282: BBC Sport takes a look at some of the best
283,3 0.6 a1.01 G0.4 model TANN,Chunk 283: BBC Sport takes a look at some of the best
284,-1.54 0.2 ANN USUUEU 0.0,Chunk 284: The winning numbers in Saturday evening's drawing of
285,-2.0- le-3 3 E1 (-) 63(),Chunk 285: BBC Sport takes a look at some of the key
286,le-3 (a) strain increment As = 1 X 10-4. le-3 1.0 le-3 0.01 model,Chunk 286: le-3 (a) strain increment As = 1
287,TANN 0.8 -0.5 ANN 0.6 3,Chunk 287: BBC Sport takes a look back at some of the
288,C 1.01 0.4 model TANN,Chunk 288: BBC Sport takes a look back at some of the
289,-1.5 0.2 ANN 0.0 -2.0 -2 -1 0,Chunk 289:     
290,2 3 51 () le-3 (-),Chunk 290: BBC Sport takes a look at some of the key
291,le-3 (b) strain increment As = 1 X 10-3. le-1 le-2 model,Chunk 291: le-3 (a) strain increment As = 1
292,model 0.04 TANN TANN ANN ANN,"Chunk 292: All photographs courtesy of AFP, EPA, Getty Images"
293,3o.5 C a  2 -1.0,Chunk 293: BBC Sport takes a look back at some of the
294,0 -1.5 2 -1.0 -0.5 0.0 0.5 1e1 0.0 0.5 1.0 1.5 251,Chunk 294: BBC Sport takes a look back at some of the
295,EI () 63 () (c) strain increment As = 1 X 10-2. le-1 model,Chunk 295: The BBC's science programme takes a look at
296,model 0.0 * TANN TANN ANN ANN,"Chunk 296: The world's biggest retailer, Amazon, has"
297,30.5 C 2  3 -1.0,Chunk 297: The winning numbers in Saturday evening's drawing of
298,0 -2 -1.5 -1.0 -0.5 0.0 0.5 1.0 0.0 0.5 1.0 1.5 2.0,Chunk 298: BBC Sport looks at some of the key statistics from
299,"E1 (-) 53 (-) (a) strain increment As = 1 X 10-1. Figure 14: Comparison of the internal variable predictions of TANNS and standard ANNs with respect to the target values, for the tri-axial cyclic loading path, Eq. (27), for material case H-1 (perfect plasticity).",Chunk 299: Figure 14: Comparison of the internal variable predictions of TANNS and standard ANNs with respect
300,Each row represents the prediction at different AE increments. 28 le-2 6 model model,Chunk 300: This table shows the predicted changes in the odds of
301,TANN 3 TANN & ANN ANN,Chunk 301: TANN 3 TANN & ANN ANN
302,I 3 2 2 & A,Chunk 302: BBC Sport takes a look at some of the key
303,0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 Ep 5p () 3 Susu (),Chunk 303: Match report from the Premier League match between Arsenal and
304,le-4 (a) strain increment As = 1 X 10-4. le-1 model model,Chunk 304: le-4 (a) strain increment As = 1
305,5 TANN 3 TANN ANN a ANN,"Chunk 305: All photographs courtesy of AFP, EPA, Getty Images"
306,3 2 2 A A,Chunk 306: The winning numbers in Saturday evening's drawing of
307,0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 Ep-5p (-) le-3,Chunk 307: BBC Sport takes a look at some of the key
308,byky (-) le-3 (b) strain increment As = 1 X 10-3. 1.50 le4 lel,Chunk 308: BBC News NI takes a look at some of the
309,model L model 1.25 TANN TANN 1.00 ANN,Chunk 309: TANN has announced the launch of TANN T
310,0 ANN  I 0.75 0.50 A 0.25,Chunk 310: The winning numbers in Saturday evening's drawing of
311,A 0.00 0.00 0.25 0.50 0.75 1.00 0.0 0.5 1.0 1.5 Ep- -6p (-) 155150,Chunk 311: A look back at some of the most memorable moments
312,Liky () le-2 (c) strain increment As = 1 X 10-2. le6 le3,Chunk 312: Here's a look at some of the more
313,1.50 model 1.25 TANN  a 1.00 ANN 5 0.75,Chunk 313: A selection of the best-selling children's
314,-2 50.50 & 0.25 A model,Chunk 314: BBC Sport takes a look at some of the best
315,TANN 0.00 -6 ANN 0.00 0.25 0.50 0.75 1.00 1.25 1.50 0.0 0.5 1.0 1.5,Chunk 315: The winning numbers in Sunday evening's drawing of
316,Ep 6p (-) hibuy (-) le-1 (a) strain increment As = 1 x 10-1. Figure 15: Comparison of the energy and dissipation rate predictions of TANNS and computation according,Chunk 316: Figure 15: Comparison of the energy and dissipation rate predictions of
317,"to Eq. (5.1) for standard ANNS with respect to the target values, for the tri-axial cyclic loading path, Eq. (27), for material case H-1 (perfect plasticity). Each rOW represents the prediction at different As increments. 29 5.2. von Mises hypo-plasticity We illustrate the performance of TANNS in predicting smooth material behaviors as well,","Chunk 317: In this paper, we present a new method for predicting smooth material behavior, TANNS, with respect to the properties"
318,"modeled here using the hypo-plasticity model, presented in Einav (2012) and paragraph 4.1.2. The energy potential and dissipation rate are given by 9K F (Ep- Sp)+",Chunk 318: The energy potential and dissipation rate of a low-cost
319,"2 +Gle-2)-e-2, D=d.: with z being defined in Eq. (25a). We consider K = 167 GPa, G = 77 GPa, and S = 1, see Eq.s (25a) and (25b).",Chunk 319: We consider the following equation for the differential equation 2 +G
320,"Data are generated as detailed in Section 4. 8000 are data generated through random loading paths. As in the case of hyper-plasticity, additional sampling with random uni-axial and bi-axial random loading paths are also used. The samples are split into training (50%), validation (25%), and test (25%) sets. The sampling in terms of the mean and deviatoric stresses, p and 9, and deformations, Ep and e, is presented in Figure $10.","Chunk 320: The results of this study are presented in terms of the mean and deviatoric stresses, p and 9, and deformations, Ep and e."
321,"The architecture and hyper-parameters of TANNS are maintained equal to the hyper- plastic case (see paragraph 5.1). The internal variables Si are selected to coincide with the inelastic strain. We emphasize that this particular choice does not affect the results of TANNS. As extensively discussed in Einav (2012), an alternative choice to the selection of the inelastic strain as internal variable can be the material porosity.","Chunk 321: In this paper, we present a new method for the determination of the inelastic strain (TANNS) in a hyper- plastic case."
322,"The early stopping rule assures convergence, after approximately 1000 epochs, with MAEs of the same order of magnitude for the 4 outputs, AK, F'+Ar, Ac, and D'+Ar The (adimensional) MAE is approximately equal to 1 X 10-4 for all outputs at the end of the training. 5.2.1. TANN VS standard ANN. Generalization of the network",Chunk 322: The following is a description of the TANN network:
323,"As for the hyper-plastic cases, we investigate the performance of TANNS with respect to standard ANNs through illustrative examples. The architecture and hyper-parameters of ANNs are maintained equal to the hyper-plastic case (see paragraph 5.1.3). Figure 16 shows the predictions of both networks for the following bi-axial loading path ni","Chunk 323: In this paper, we compare the performance of two types of artificial neural networks (ANNs): the hyper-plastic"
324,"AEi = -AE2 = As sgn COs AE3 = 0, 2N with N = Emax/AE, Emax = 2 X 10-3 + 1, and As = 2 X 10-4,2x 10-3. TANNs' predictions are in excellent agreement with the target model. The smoother material response, with",Chunk 324: The following table shows the results of the TANNs' predictions for the response of the Higgs boson
325,"respect to the hyper-plastic scenario, is well captured by the networks. Standard ANNs clearly underperform. 30 le2 3 le-3","Chunk 325: In our series of letters from African journalists, filmmaker"
326,model model TANN 2 TANN ANN,"Chunk 326: All photographs courtesy of AFP, EPA, Getty Images"
327,ANN  0 € 6 -1  -1,Chunk 327:     
328,d & -2 -2 2 -3 2,Chunk 328: Match reports from the weekend's Premier League games
329,E1 (-) le-3 (-) le-3 (a) strain increment As = 2 X 10-4.,Chunk 329: E1 (-) le-3 (-)
330,le2 le-2 model 2 ANN TANN ,Chunk 330: le2 le-2 model 2 ANN TANN
331,model € € 01 TANN 0,Chunk 331:     
332,6 ANN 4 1 2,Chunk 332: BBC Sport takes a look back at some of the
333,oota 2 61 (-) le-2 E1 (-),Chunk 333: BBC Sport takes a look back at some of the
334,"le-2 (b) strain increment As = 2 X 10-3. Figure 16: Comparison of the stress and internal variable predictions of TANNS and standard ANNs with respect to the target values, for a bi-axial cyclic loading path, AE2 = -AE1, with AEi as in Eq. (5.2.1), for a perfect hypo-plastic material. Each row represents the predictions at different As increments.",Chunk 334: le-2 (a) strain increment As = 2 X 10-3.
335,"Additional demonstration of the performance of TANNS is given in Figure 17, for a bi- axial loading path with strain-controlled ratcheting. Ratcheting is a well known phenomenon shown by many materials during cyclic loading, which has been modeled here with the h?plasticity framework (Einav, 2012). In particular, we show that TANNS, contrary to ANNs, successfully predict principal stresses, inelastic strains, energy potential, and dissipation","Chunk 335: In this paper, we report the results of a novel finite element method (TANNS) for the prediction of stresses during cyclic loading."
336,rate. 31 le2 le2 model,Chunk 336: BBC Sport takes a look at some of the best
337,TANN ANN / 0 01,Chunk 337: BBC Sport takes a look back at some of the
338,- - € € -1,Chunk 338:     
339,6 -1 model 6 TANN -2,Chunk 339: BBC Sport takes a look at some of the best
340,-2 ANN -3 -6 51 (-),Chunk 340: The winning numbers in Saturday evening's drawing of
341,le-3 2 (-) le-3 (a) principal stresses. le-3,Chunk 341: le-3 2 (-) le-3 a principal
342,le-3 model model 61 TANN 2 TANN,Chunk 342: A selection of the best photographs from around the world
343,ANN ANN 0 E € -2,Chunk 343:     
344,a U S 4 -2 -6 8,Chunk 344: A look back at some of the more memorable moments
345,6 -6 4 E1 (-) le-3 2 (),Chunk 345: BBC Sport takes a look at some of the key
346,le-3 (b) principal internal variables. le-1 le-2 2.0,Chunk 346: le-3 (a) principal external variables.
347,- model 6 TANN 1.5 ANN a,Chunk 347: BBC Sport takes a look at some of the best
348,1.0  & 0.5 model A 2 0.0 TANN,Chunk 348: BBC Sport takes a look back at some of the
349,a ANN 3 -2 Ep-6p () le-4,Chunk 349: BBC Sport takes a look back at some of the
350,"Syku (-) le-4 (c) energy and dissipation rate. Figure 17: Comparison of the predictions of TANNS and standard ANNs with respect to the target values, for the bi-axial loading path with strain-controlled ratcheting, for a perfect hypo-plastic material.",Chunk 350: Figure 17: Comparison of the predictions of TANNS and standard ANNs with
351,"32 6. Noise in training data and robustness of predictions After having demonstrated the performance of TANNS and their superiority to standard approaches in modeling path-dependent material behaviors, we investigate the effect of noise in the measurements of the data used to train artificial neural networks. This is achieved","Chunk 351: In this paper, we present a novel approach to the training of artificial neural networks (TANNS) based on"
352,"by training TANNS (and ANNs) using the previously generated data and adding, in the training and validation sets, artificial noise. For sake of clarity, we consider a perfectly plastic material (case H-1, cf. Tab. 1). The additive noise, ns, is based on a normal distribution with standard deviation (sd) equal to 10% of the mean value of the clean data. In particular, we consider the following scenarios, independently:","Chunk 352: In this paper, we consider the use of artificial noise to train TANNS and ANNs."
353,"(1) noise in 4Ar, 7 i.e., nsg, with sd = 10% of the mean value of 5A, (2) noise in CAr, 7 i.e., nso, with sd = 10% of the mean value of CAr, (3) noise in F+Ar 1 i.e., nsF, with sd = 10% of the mean value of Fl+Ar, and (4) noise in D'+Ar, 1 i.e., nsF, with sd = 10% of the mean value of D'+A We emphasize that the aforementioned noise levels were chosen to demonstrate the performance",Chunk 353: In this paper we present the results of a series of experiments on the noise levels of the Earth's atmosphere.
354,"of TANNS and generally lower levels of noise are expected in practical applications. However, we examine each scenario independently in order to explore better the effect of noise on training and on the accuracy of the predictions. In cases (1) and (2), once the noised quantities are computed (denoted with aAr and Z+A), the increments, i.e., Adi and ASi are re-evaluated as Adi = a#Ar ai and ASi = gear - 5, respectively.","Chunk 354: In this paper, we study the effect of noise on the training of fighter pilots in two different scenarios, i.e."
355,"The architecture and hyper-parameters of the neural networks, both TANNs and ANNs, designated in this study are the same as those used in Section 5. It should be noticed that, for each case (1-4), the data used to train the networks are not respecting the thermodynamics requirements due to the added noise, i.e., Eq. (13). The addition of noise can have an impact on the training of the networks and their",Chunk 355: The aim of this study is to investigate the impact of added noise on the training of neural networks.
356,"predictions. We first focus on the former. Figure 18 displays the loss functions of each output as the training of TANNs is performed, for noise added in stresses, case (2). The MAE is evaluated between the TANNs' predictions and the (noised) training and validation data- sets. Table 3 shows the MAEs of the predictions of TANNS with respect to the validation data-sets, for each level of noise, at the end of the training. Although the earlystopping","Chunk 356: In this paper, we evaluate the predictions of TANNS with respect to the training and validation data-sets."
357,"technique is used, training is accomplished, in all cases of noise, after approximately 1000 epochs. By comparing the training using the original, un-noised data (Fig. 8) and that using the noised ones (Fig. 18), we can observe that TANNS are unable to learn the noised signal, i.e., Ad,. This is a direct consequence of the fact that the network evaluates the stress increments","Chunk 357: In this paper we show that TANNS are unable to learn the noised signal, i.e., Ad."
358,"from the knowledge of the stress state at time t and the energy potential predictions. When noise is added, the first law of thermodynamics is violated and the training operation with noised data is unsuccessful, with respect to the noised training and validation data-sets. However this is not a drawback of our approach. On the contrary, it is an indication of the 33",Chunk 358: In this paper we present a new method for training and validation of experimental data-sets.
359,le-1 Afitrain Alival le-2 Ftrain,Chunk 359: le 1 Afitrain Alival le-2 F
360,Fyal 3 le-3 Aditrain Adival le-4,Chunk 360:     
361,Dtrain Dval 10 100 1000,"Chunk 361: All photographs courtesy of AFP, EPA, Getty Images"
362,"epochs Figure 18: Errors of the predictions of TANN, as the training is being performed, evaluated with respect to the training (train) and validation (val) sets. Noise is added in stress to both training and validation data-sets, case (2). Table 3: MAEs of the predictions of TANNS with respect to the validation data-sets, for the original, un-","Chunk 362: The results of this study are presented in the following tables: Figure 18: Errors of the predictions of TANN, as the training"
363,"noised data and each level of noise, at the end of the training, Early-stopping is used and the training is always completed at approximately 1000 epochs. Mean Absolute Error (le-4) ALi Aci Fl+Ar ADI+A:",Chunk 363: The training is carried out over a long period of time and is carried out
364,un-noised3.2 4.1 0.8 7.5 nsy 324 2.5 0.9 5.9 nsa 3.3 39 1.0 5.4,Chunk 364: BBC Wales Sport takes a look at some of the
365,nsF 3.3 3.1 19 7.3 nsp 3.4 4.9 0.9,Chunk 365: The winning numbers in Saturday evening's drawing of
366,"57 quality of the data, which in this case they don't respect the laws of thermodynamics due to measurement noise. Notice that the values of the training error is consistent with Eq. (13), the expression given in Section 3.4 and the magnitude of the noise. The implementation of the laws of thermodynamics in the network's architecture shields the learning process and","Chunk 366: In this paper, we show how to train a network to learn the laws of thermodynamics."
367,"prohibits learning of inconsistent data. For instance, with reference to Table 3, we can see that for case (2), nsa, the MAEs in the predictions of the inelastic strains, energy and dissipation rate approximately coincide with those obtained with the un-noised data. The aforementioned behavior is not observed in standard ANNS. As an example, we show","Chunk 367: In this paper, we present a new approach to the analysis of inelastic strain data, which is based on a new approach to"
368,"in Table 4 the MAEs of the predictions of standard ANNs with respect to the validation data-sets, for noised stresses. In this case, we can see that the network, unaware of the requirements of the thermodynamics, learns successfully the noised outputs. This means that, once standard ANNs are asked to make predictions, in recall mode, the outputs will be affected by the noisy training in an unpredicted way. For the levels of noise cases (1),","Chunk 368: In this paper, we show how standard ANNs can be trained to make predictions in noisy data-sets."
369,"(3), and (4), similar results are obtained. 34 Table 4: MAES of the predictions of standard ANNs with respect to the validation data-sets, for the original, un-noised data and noise on stresses, at the end of the training (approximately 1000 epochs). Mean Absolute Error",Chunk 369: This paper presents the results of a coarse-grained training of standard ANNs with respect to the
370,(le-4) ALi Aci un-noised5.8 4.2 nsa 5.9 4.2,Chunk 370: BBC Sport takes a look at some of the key
371,"In Figure 19 compared the predictions in recall mode of both networks based on noise data. The predictions of the training with clean (un-noised) data is also presented for helping the comparison. We notice that TANNS, whilst trained on data with relatively large levels of noise, successfully predict the material response and perform more or less as when trained on data free of noise. On the contrary, standard ANNs are strongly affected by",Chunk 371: We have compared the performance of two artificial neural networks (ANNs): TANNS and standard ANNs.
372,"the large levels of noise of the data used to train the network. Similar results are found in presence of noise in the training and validation data of the internal variable, Si, see Figure S11, in the Supplementary Material. It should be noticed that, in this case, ANNs do not manage to successfully minimize the loss function of ALi, with the selected number of hyper- parameters. This is the consequence of the ANN architecture which have been chosen to","Chunk 372: In this paper, we study the effect of noise on the performance of ANNs."
373,"achieve the best performance with thermodynamic consistent (clean of noise) data. However, we emphasize that, if the number of hyper-parameters of the ANN model were increased to achieve convergence with respect to the noised data, then ANNs would learn the noised material response, resulting to be highly affected by noise measurements. Consequently, we can state that TANNs show high degree of robustness to noise, when","Chunk 373: In this paper, we study the robustness of tensor learning networks (TANNs) to noise."
374,"compared to ANNs. 7. Concluding remarks A new class of artificial neural networks models to replace constitutive laws and predict the material response at the material point level was proposed. The two basic laws of thermodynamics were directly encoded in the architecture of the model, which we refer to",Chunk 374: A new class of artificial neural networks were proposed to replace laws and predict the material response at the material point level
375,"as Thermodynamic-lased Neural Network (TANN). Our approach was inspired by the SO- called Physics-Informed Neural Networks (PINNS) (Raissi et al., 2019), where the automatic differentiation was used to perform the numerical calculation of the derivative of a neural network with respect to its inputs. Feed-Forward Neural Networks were used herein, but the approach is general and can be applied to Recurrent Neural Networks (RNNs) or other","Chunk 375: In this paper, we present a novel method for the derivation of the derivative of a neural network with respect to its inputs, i.e."
376,"types of ANNs as well. The numerical requirements regarding the mathematical class of appropriate activation functions to be used together with automatic differentiation were investigated. More specifically, the internal restrictions, derived from the first law of thermodynamics, require activation functions whose second gradient does not vanish. This new problem and its remedy was","Chunk 376: In this paper, a new problem has been solved in the field of artificial neural networks (ANNs)."
377,extensively explored and discussed in the manuscript. 35 le-2 le3 le-3 model,Chunk 377: This article is part of a series of papers on
378,model 1.5 model 0.0 ANN ANN 0 ANN,"Chunk 378: The world's most powerful car, the Tesla"
379,noised -0.5 1 noised ANN € 1.0 31.0 ANN,Chunk 379:     
380,2 5 0.5 51.5 C0xxxc0c0xxx006 A -2.0,Chunk 380: C0xxxc0xxx006 A -2.0
381,0.0 noised ANN -2.5 0.0 0.5 1.0 1.5 () le-3,Chunk 381:     
382,() le-3 usuy () 324 le3,"Chunk 382: (Nos  jours, "
383,le-3 le-2 1.25 model 0.0 model - model,Chunk 383: le-3 le-2 1.25 model 0.0 model -
384,1.00 TANN 0 TANN 3 TANN noised noised,Chunk 384: A look back at some of the most memorable moments
385,3 0.75 -0.5 TANN TANN 3 0.50 C,Chunk 385: The winning numbers in Saturday evening's drawing of
386,2 -1.0. 5 0.25 0.00 noised TANN -1.5,Chunk 386: A look at some of the key stories of the
387,51 () le-3 () le-3 0.0 0.5 1.0 1.5,Chunk 387: The winning numbers in Saturday evening's drawing of
388,(a) strain increment As = 1 X (b) strain increment As = 1 X Ayki () le-4 10-4. 10-3.,Chunk 388: Turkey's President Recep Tayyip Erdogan and his
389,"(c) strain increment As = 1 X 10-2. Figure 19: Influence of noise in the stress, Oi, for the predictions of the stress, internal variable, and dissipation rate of TANNS and of standard ANNS with respect to the target values, for the tri-axial cyclic loading path for material case H-1 (perfect plasticity). Noise strongly affect the predictions of standard",Chunk 389: (a) strain increment As = 1 X 10-2.
390,"ANNS, see Fig.s 13-15. TANN, relying on an incremental formulation and on the theoretical developments in Houlsby and Puzrin (2007), posses the special feature that the entire constitutive response of a material can be derived from definition of only two scalar functions: the free-energy and the dissipation rate. This assures thermodynamically consistent predictions both for",Chunk 390: TANN is a new approach to the theory ofStokes-Stokes-Stokes-Stokes reactions.
391,"seen and unseen data. Differently from the standard ANN approaches, TANN does not have to identify, through learning, the underlying thermodynamic laws. Indeed, predictions of standard ANNs may be thermodynamicaly inconsistent, even though the training of the network has been performed on consistent material data. Being aware of physics, TANNs are found to be a robust approach with the presence of noise measurements in the training","Chunk 391: In this paper, we present a new approach to the training of artificial neural networks (ANNs), called tensor annealing networks (TANNs)."
392,"data, contrary to the standard ANN approach. For the cases here investigated, we showed that TANNS are characterized by high accuracy of the predictions, higher than those of standard approaches. The integration of thermodynamic principles inside the network renders TANN's ability of generalization (i.e., make predictions for loading paths different from those used in the training operation)","Chunk 392: In this paper, we report the results of a new approach to the prediction of loading paths based on tensor annealing neural networks (TAN"
393,"remarkably good. Consequently, TANN is an excellent candidate for replacing constitutive calculations at Finite Element incremental formulations. Moreover, thanks to the implementation of the free-energy in the network predictions and its thermodynamical relation with the stresses, the Jacobian AE at the material point level is better predicted even for increments far beyond the training data-set range. As a result quadratic convergence in implicit","Chunk 393: In this paper we present a new model for the prediction of stresses at the material point level, the tensor angular momentum network (TANN)."
394,"formulations can be preserved, reducing the calculation cost. Finally, we investigated the presence of noise in data and the effect on the training process 36 and predictions in recall mode. The thermodynamic framework of TANNS shields the training operation and prohibits learning of inconsistent data. As a result, TANNS posses","Chunk 394: In this paper, we report the development of TANNS, a novel method for the prediction of the stability of"
395,"high degrees of robustness to noise, compared to standard ANNS. Further extensions of TANN in a wide range of applications, for complex materials, are straightforwards, as the thermodynamics principles hold true for any known class of material, at any length (micro- and macro-scale). Acknowledgments",Chunk 395: TANN is a novel approach to the study of solid-state heat transfer.
396,The authors would like to acknowledge the anonymous reviewers whose feed-backs helped improve this work. The author I.S. would like to acknowledge the support of the European Research Council (ERC) under the European Union Horizon 2020 research and innovation program (Grant agreement ID 757848 CoQuake).,Chunk 396: The authors would like to acknowledge the support of the European Research Council (ERC) under the European Union
397,"37 7.1. Appendix A. Understanding second-order vanishing gradient In the following, we investigate the performance and influence of different activation functions on the computational time to train an ANN with input I, primary output O1,and secondary output O2 = VIO1. Consider the above discussed example with I = x, O1 = x?,","Chunk 397: In this paper, we investigate the performance and influence of different activation functions on the computational time to train an ANN with"
398,"and O2 = 2x. The ANN has one hidden layer, with N, = 6 nodes, and activation functions as reported in Table 5. The output layer has linear activation and null bias. The absolute error is selected as loss function for both Oi and O2. Training is performed on 1000 samples, normalized between -1 and 1. A very small value for the learning rate is selected, i.e., E = 10-5 in order to facilitate the gradient descent algorithm in reaching small values of the","Chunk 398: An artificial neural network (ANN) is used to train a gradient descent algorithm on a set of samples, i.e."
399,"loss function. We use early-stopping. In other words, training is stopped as the error of a validation set (500 samples) starts to increase while the learning error still decreases (Géron, 2019). The validation set is used to avoid over-fitting of the training data. Table 5: Set of activation functions considered to investigate the performance of the network with outputs 0= x and VIo = 2x, with I = x, in the framework of first- and second-order vanishing gradients.","Chunk 399: Table 5: Set of activation functions considered to investigate the performance of the network with outputs 0= x and VIo = 2x, with I = x, in the framework of first and"
400,"Function Z range S(z) '(z) '""(z) z<0",Chunk 400: Function Z range S(z) '(z
401,0 0 0 ReLUz z20,Chunk 401: BBC Sport takes a look at some of the key
402,Z 1 0 z<0 0,Chunk 402: BBC Sport takes a look at some of the key
403,0 0 ReLUoS24z z20 0.5:2+z Z+ 1 1,Chunk 403: ReLUoS24z z20 
404,z<0 0 0 0 ReLU,Chunk 404: A look back at some of the most memorable moments
405,z20 22 2z 2 ELUe,Chunk 405:     
406,Vz ez-1 ez ez z<0,Chunk 406:     
407,"e-1 ez ez ELU, z20",Chunk 407:     
408,z 1 0 z<0 e2-1,Chunk 408: BBC Sport takes a look back at some of the
409,ez ez ELUOSz z20 0.52?+z Z+ 1 1,Chunk 409:     
410,"z<0 e-1 ez ez ELU,",Chunk 410:     
411,z20 z2 2z 2 z<0,"Chunk 411: , , , "
412,e-1 ez ez ELU z20,Chunk 412:     
413,2* 4z3 12z2 z<0 ez-1,Chunk 413:     
414,"ez ez ELU40522 z20 24 + 0.522+z 423+z+1 12:2+1 For each tested activation function, Table 6 shows the adimensional Mean Absolute Error (MAE) calculated using a set of new, unseen data (500 samples) of input-output predictions","Chunk 414: For each tested activation function, Table 6 shows the adimensional Mean Absolute Error ("
415,"for x2 and 2x. The advancement of training is quantified herein as the number of epochs, i.e., the number with which the training algorithm works with the training data-set Géron (2019). Activation functions with quadratic terms, or of higher degree, perform very well, compared to their linear equivalents. RELU2, ELU2 outperform as their shape is very similar to the input-output regression they are trained to learn. Nevertheless, it is worth","Chunk 415: In this paper, we show that activation functions with terms of higher degree, i.e."
416,"38 noticing that training fails when activation functions with vanishing second gradient are used (e.g. RELUZ and ELUz). Figure 20 compares the ANN predictions for a selection of activation functions with the analytical (exact) results. Whilst RELUZ is clearly inadequate, ELUZ predictions overall agree with the analytical values. This is due to the fact that the",Chunk 416: This paper presents the results of the ANN-ELUZ training scheme.
417,"ANN takes advantage of the exponential term, for negative Z and thus successfully manage to satisfy both O and Vro. Additional hidden layers may improve the performance of the network. It can be further noticed that activation function of high degree, e.g. ELUe, ELU, and ELUA405242 even if successful, require a large number of epochs. Table 6: Activation functions and performance with unseen data.",Chunk 417: In this paper we show that ANN is able to perform the activation function of O and Vro with unseen data.
418,"Activation function SA L Lo Lv,o no. epochs (10-4) (10-4) (10-4) () ReLUz",Chunk 418: The following table shows the number of epochs in
419,1521.2 205.98 1315.18 920 ReLU0.524z 762.4 93.58 668.85 8054,Chunk 419: BBC Sport takes a look back at some of the
420,ReLU2 0.061 0.0241 0.0371 148 ELUe 127.2 26.83 100.38 19477,Chunk 420: ReLU2 0.061 0.0241 0.0371 148
421,"ELU, 108.56 12.12 96.44 17280 ELUOSB4z 65.5 10.91 54.63",Chunk 421: BBC Sport takes a look at some of the best
422,"12178 ELU,2 0.13 0.067 0.067 88 ELUA","Chunk 422: All photographs courtesy of AFP, EPA, Getty Images"
423,65.36 33.75 31.61 20051 ELU40S 12.94 1.81 11.13 9683,Chunk 423: BBC Sport takes a look at some of the best
424,"39 1.2 2z 1.0 ReLU,",Chunk 424: BBC Sport takes a look at some of the best
425,"Z ReLU, 0.8 ELU, ELU,",Chunk 425: BBC Sport takes a look at some of the key
426,0.61 o 0.4 0.2 01 Z,Chunk 426: The winning numbers in Saturday evening's drawing of
427,-1.0 -0.5 0 0.5 1.0 -1.0 -0.5 0 0.5 1.0 z() 2 (-) 0.25,Chunk 427: The winning numbers in Saturday evening's drawing of
428,"1.0 2 0.75 2z 0.20 ReLU,",Chunk 428: BBC Sport takes a look at some of the best
429,"ReLU, ELU, 0.50 ELU, 20.15","Chunk 429: ,,,,"
430,0.25 0 0.14 C' 0 -0.25 0.05,Chunk 430: The winning numbers in Saturday evening's drawing of
431,0.50 01 -0.75 -0.4 -0.2 0 0.2 0.4 -1.0 -0.4 -0.2 0 0.2 0.4,Chunk 431: The winning numbers in Saturday evening's drawing of
432,"z() 2 (-) (a) x2 predictions, O1, using ReLU and ELUz- (b) 2x predictions, O2, using ReLU and ELUz- le-2",Chunk 432: BBC Sport takes a look at some of the best
433,"le-1 65  4 2z ELU, ELU,",Chunk 433: BBC Sport takes a look at some of the best
434,ELUAsAs 2 C E EV,Chunk 434: BBC Sport takes a look at some of the best
435,"ELU. ELU, d 0 ELU, -2",Chunk 435: Match reports from the Champions League quarter-final between
436,-4 0 2 0 2,"Chunk 436: All photographs  AFP, EPA, Getty Images"
437,(-) le-1 (-) le-1 le-2,Chunk 437:     
438,le-2 1.0 2 3 2z 0.8,Chunk 438: BBC Sport takes a look back at some of the
439,"ELU, 2 ELU, ELUasAs ELUAsA","Chunk 439: ELU, 2 ELU, ELUas"
440,0.6 ELU. C ELV 0.4,Chunk 440: BBC Sport takes a look at some of the key
441,"E, d 0 ELU, 0.2 -1","Chunk 441: E, d 0 ELU, 0.2 -1"
442,-2 U -0.5 0.5 -3 -1.5 -1.0 -0.5 0 0.5 1.0 1.5,"Chunk 442: Match details, team news and stats for Saturday'"
443,"(-) le-1 z() le-1 (c) 2 predictions, O1, using ELUZ, ELU0522+2 (a) 2x predictions, 02, using ELUz, ELUOS2",Chunk 443: BBC Sport takes a look back at some of the
444,"ELU,, ELUe, and ELU. ELU2, ELUe, and ELU. Figure 20: Comparison of different activation functions for the prediction of the primary output, x2 (a), and secondary output, 2x (b). From top to bottom the range of Z decreases from larger to smaller values, to observe the behavior at z & 0.","Chunk 444: Figure 20: Comparison of different activation functions for the prediction of the primary output, x2 (a), and secondary"
445,"40 Appendix B. Derivation of the incremental material formulation By differentiating the energy expressions (13) and rearranging the terms, we obtain the following non-linear incremental relations à = OsF-g+ > OEAF-5k + OFê",Chunk 445: We derive the following non-linear incremental relations  = OsF
446,"(28a) -Ki = OyF.E+ OLE F + agoFè (28b) -S = OF.E+ > dog F . 5k + OgFè, (28c)",Chunk 446: BBC Sport takes a look at some of the key
447,where the following notation is adopted 8F 8PF OgsF = deyF=,Chunk 447: The following is a list of the most commonly used
448,dEioEu dEiO5 8F 8?F OEoF =,Chunk 448: dEioEu dEiO5 8
449,"O0F = dEij00 a02 We introduce the thermodynamic dissipative stresses X+ = (X1,. ...,XN) with aD",Chunk 449: In this paper we present a new theory of the
450,"X,= ViE[1,N). (29) ab For a rate-independent material, the dissipation is a homogeneous first-order function in the",Chunk 450: The coefficients of the first-order dissipation of a
451,"internal variable rates 5i (Houlsby and Puzrin, 2007). This homogeneity can be expressed by the Euler's relation OD D = x-5",Chunk 451: The homogeneity of the Euler's relation OD D
452,"(30) i=1 a5i which, together with (11), implies x-x)-4=0 (31)",Chunk 452: This table shows the number of letters in the word
453,"i-1 Ziegler's orthogonality condition (Ziegler, 2012) is further assumed, i.e., X; = Xi Vie [1,N]. Being D homogeneous first-order function in 5i the Legendre transform, conjugate to Xi, is degenerate, that is equal to zero, and represents the yield function y = 5(6,6,Z,X), i.e. ly = 2x-4-D=0,","Chunk 453: The Legend, conjugate to Xi, is assumed to be D homogeneous first-order function in 5i"
454,"(32) where 2 is a non-negative multiplier. From the properties of Legendre transform, the following flow rules must hold dy 5 = A",Chunk 454: The coefficients of the coefficients of Legendre transform are
455,"Vie[ [1,N]. (33) ax; 41 Since 2 N 0 and ly = 0,<0. If y=0, the following consistency equation is met",Chunk 455: The consistency equation is given as the following:
456,Oy dy dy y - + B=0.,Chunk 456: Oy dy dy y - + B=0
457,(34) ds i=1 asi i-1 ax; 00,Chunk 457: BBC Sport takes a look back at some of the
458,"By further using the flow rules (33) and Ziegler's normality condition, we obtain CE Co A = -",Chunk 458: We derive the coefficients of flow (CE Co A
459,"e, (35) B B with","Chunk 459: All photographs courtesy of AFP, EPA, Getty Images"
460,"dy dy C= OyeF, Os",Chunk 460:     
461,ax; i-1 dy dy Ce,Chunk 461:     
462,"OgoF, 00 ax; i=1 and",Chunk 462: BBC Sport takes a look back at some of the
463,dy dy dy ày B = OueF,Chunk 463: dy dy y B = O
464,"= a5i ax; - aX; aXk Finally, we arrive to the following, incremental non-linear formulation, for y = 0, MgE Mgo","Chunk 464: In this week's Mathematica, we look at"
465,o Mje Mxe & Mos,Chunk 465: BBC Sport takes a look back at some of the
466,"Moo E = Ml-0 5, with É= -S 0 Ml-0 = CE.B Co dy (36) ax; B ax;","Chunk 466: Moo E = Ml-0 5, with "
467,A Bc. Ce B B,Chunk 467: A Bc a Bc a Bc a
468,"and MgE = OgsF- Zk OF-(% ax, Mge = OseF - Zk deF-( axk Mxe = agsF- ZOuF-(. axk y Mxo = OyoF - ZOuF-(. ax;",Chunk 468: The following table lists the most common denominators in the
469,"Mes = OBsF = Zk OF-( axe) J Moe = O00F - Zk Oes F axk In case of y < 0, relation (36) becomes OEsF OEoF]",Chunk 469: The following table shows the results of a test carried out
470,"ageF OyoF E = Mlyco 5, with Mlyso = OBsF O0oF (37) 0 0",Chunk 470: Match reports from the weekend's matches in the
471,"0 0 42 References O. Lloberas Valls, M. Raschi Schaw, A. E. Huespe, X. Oliver Olivella, Reduced finite element square",Chunk 471: In: Proceedings of the 11th International Conference on
472,"techniques (rfe2): towards industrial multiscale fe software, in: COMPLAS 2019: XV International Conference on Computational Plasticity: Fundamentals and Applications, International Centre for Numerical Methods in Engineering (CIMNE), 2019, pp. 157-169. M. Nitka, G. Combe, C. Dascalu, J. Desrues, Two-scale modeling of granular materials: a DEM-FEM approach, Granular Matter 13 (2011) 277-281. doi:10.1007/s10035-011-0255-6.","Chunk 472: Nitka, M., Combe, G., Dascalu, C., Desrues, J."
473,"F. Feyel, A multilevel finite element method (FE2) to describe the response of highly non-linear structures using generalized continua, Computer Methods in Applied Mechanics and Engineering 192 (2003) 3233- 3244. doi10.1016/S0045-7825(03/00348-7. N. Bakhvalov, G. Panasenko, Homogenisation: Averaging Processes in Periodic Media: Mathematical Problems in the Mechanics of Composite Materials, 1989.",Chunk 473: Mathematical problems in the Mechanics of Composite Materials.
474,"G. Houlsby, A. Puzrin, A thermomechanical framework for constitutive models for rate-independent dissipative materials, International journal of Plasticity 16 (2000) 1017-1047. I. Einav, G. Houlsby, G. Nguyen, Coupled damage and plasticity models derived from energy and dissipation potentials, International Journal of Solids and Structures 44 (2007) 2487-2508. G. T. Houlsby, A. M. Puzrin, Principles of hyperplasticity: an approach to plasticity theory based on","Chunk 474: In: Houlsby, G. T., Puzrin, A., and Nguyen, G."
475,"thermodynamic principles, Springer Science & Business Media, 2007. I. Einav, The unification of hypo-plastic and elasto-plastic theories, International Journal of Solids and Structures 49 (2012) 1305-1315. F. Masi, I. Stefanou, V. Maffi-Berthier, P. Vannucci, A discrete element method based-approach for arched masonry structures under blast loads, Engineering Structures 216 (2020) 110721. dothtps//dolor/1.",Chunk 475: The following papers have been published in refereed journals over the past 10 years.
476,"lolpempinet2P.IUTAL. F. Masi, I. Stefanou, P. Vannucci, A study on the effects of an explosion in the Pantheon of Rome, Engineering Structures 164 (2018) 259-273. doi10.1016/jengstruct.2018.02.082- H. Rattez, I. Stefanou, J. Sulem, The importance of thermelydromechanical couplings and microstructure to strain localization in 3d continua with application to seismic faults. part i: Theory and linear stability",Chunk 476: A study on the effects of an explosion in the Pantheon of Rome.
477,"analysis, Journal of the Mechanics and Physics of Solids 115 (2018a) 54 = 76. domhtps/dolorg/l. 1016/3mps2018.03.00. H. Rattez, I. Stefanou, J. Sulem, M. Veveakis, T. Poulet, The importance of hemmoelydromechanica. couplings and microstructure to strain localization in 3d continua with application to seismic faults. part ii: Numerical implementation and post-bifurcation analysis, Journal of the Mechanics and Physics of",Chunk 477: The importance of hemmoelydromechanica couplings and microstructure to strain in 3da with application to seismic faults.
478,"Solids 115 (2018b) 1 29. doihttps://doi.or/10.1016/mps:.2018.03.003. N. A. Collins-Craft, I. Stefanou, J. Sulem, I. Einav, A cosserat breakage mechanics model for brittle, granular media, Journal of the Mechanics and Physics of Solids (2020) 103975. dolhttps//dol.org/10.1016/mps. 2020.103975. A. P. V. D. Eijnden, P. Bésuelle, F. Collin, R. Chambon, J. Desrues, Modeling the strain localization around","Chunk 478: A cosserat breakage mechanics model for brittle, granular media, Journal of the Mechanics and Physics of Solids."
479,"an underground gallery with a hydro-mechanical double scale model ; effect of anisotropy, Computers and Geotechnics (2016). do10.1016/.compge0.2016.08.06. A. Geron, Hands-on MachineLearning with Scikit-Learn & Tensorflow, volume 1, O'Reilly Media, 2015. dol10.1017/CB0978107415324.00.. MAarXIIOILIGRN4. T. M. Mitchell, et al., Machine learning. 1997, Burr Ridge, IL: McGraw Hill 45 (1997) 870-877.","Chunk 479: Geron, A., and Mitchell, T."
480,"G. Cybenko, Approximation by superpositions of a sigmoidal function, Mathematics of control, signals and systems 2 (1989) 303-314. T. Chen, H. Chen, Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems, IEEE Transactions on Neural Networks 6 (1995) 911-917.","Chunk 480: In: Cybenko, G., and Chen, T."
481,"J. Ghaboussi, J. H. Garrett, X. Wu, Koowlatga200hased modeling of material behavior with neural networks, Journal of Engineering Mechanics 117 (1991) 132-153. doi10.1061/(ASCE)733-9390991) 117:1(132). 43 J. Ghaboussi, D. Sidarta, New nested adaptive neural networks (nann) for constitutive modeling, Computers",Chunk 481: Neural networks have been used to model the behaviour of materials for more than 20 years.
482,"and Geotechnics 22 (1998) 29-52. M. Lefik, B. A. Schrefler, Artificial neural network as an incremental non-linear constitutive model for a finite element code, Computer methods in applied mechanics and engineering 192 (2003) 3265-3283. S. Jung, J. Ghaboussi, Neural network constitutive model for rate-dependent materials, Computers & Structures 84 (2006) 955-963.",Chunk 482: Neural network models for rate-dependent materials.
483,"C. Settgast, M. Abendroth, M. Kuna, Constitutive modeling of plastic deformation behavior of open-cell foam structures using neural networks, Mechanics of Materials 131 (2019) 1-10. Z. Liu, C. Wu, Exploring the 3d architectures of deep material network in data-driven multiscale mechanics, Journal of the Mechanics and Physics of Solids 127 (2019) 20-46. X. Lu, D. G. Giovanis, J. Yvonnet, V. Papadopoulos, F. Detrez, J. Bai, A data-driven computational","Chunk 483: Researchers at the Massachusetts Institute of Technology (MIT) and the University of California, Los Angeles (UCLA) have been working on new ways to study the properties of"
484,"homogenization method based on neural networks for the nonlinear anisotropic electrical response of graphene/Polymer nanocomposites, Computational Mechanics 64 (2019) 307-321. K. Xu, D. Z. Huang, E. Darve, Learning constitutive relations using symmetric positive definite neural networks, arXiv preprint arXiv:2004.00265 (2020). D. Z. Huang, K. Xu, C. Farhat, E. Darve, Learning constitutive relations from indirect observations using","Chunk 484: Zhang, D., Xu, K., Huang, D., Darve, E."
485,"deep neural networks, Journal of Computational Physics (2020) 109491. S. Gajek, M. Schneider, T. Bohlke, On the micromechanics of deep material networks, Journal of the Mechanics and Physics of Solids (2020) 103984. M. B. Gorji, M. Mozaffar, J. N. Heidenreich, J. Cao, D. Mohr, On the potential of recurrent neural networks for modeling path dependent plasticity, Journal of the Mechanics and Physics of Solids (2020) 103972.","Chunk 485: The potential of recurrent neural networks for model path dependent plasticity, Journal of the Mechanics and Physics of Solids (2020) 103972."
486,"Y. Heider, K. Wang, W. Sun, S0(3)-invariance of informed-graph-based deep neural network for anisotropic elastoplastic materials, Computer Methods in Applied Mechanics and Engineering 363 (2020) 112875. doihttps://doi.org/10.1016/.cma.cma.2020.112875. F. Ghavamian, A. Simone, Accelerating multiscale finite element simulations of history-dependent materials using a recurrent neural network, Computer Methods in Applied Mechanics and Engineering 357 (2019)","Chunk 486: Researchers at the University of California, Berkeley, have developed a novel deep neural network-based model for anisotropic "
487,"112594. M. Mozaffar, R. Bostanabad, W. Chen, K. Ehmann, J. Cao, M. Bessa, Deep learning predicts path- dependent plasticity, Proceedings of the National Academy of Sciences 116 (2019) 26414-26420. A. L. Frankel, R. E. Jones, C. Alleman, J. A. Templeton, Predicting the mechanical response of oligocrystals with deep learning, Computational Materials Science 169 (2019) 109099.",Chunk 487: Researchers from the Massachusetts Institute of Technology (MIT) and the Massachusetts Institute of Technology (MIT) are working together to develop new
488,"D. Gonzalez, F. Chinesta, E. Cueto, Learning corrections for hyperelastic models from data, Frontiers in Materials 6 (2019) 14. M. Lefik, D. Boso, B. Schrefler, Artificial neural networks in numerical modelling of composites, Computer Methods in Applied Mechanics and Engineering 198 (2009) 1785-1804. T. Kirchdoerfer, M. Ortiz, Data-driven computational mechanics, Computer Methods in Applied Mechanics","Chunk 488: Data-driven computational mechanics, Computer Methods in Applied Mechanics"
489,"and Engineering 304 (2016) 81-101. R. Ibanez, D. Borzacchiello, J. V. Aguado, E. Abisset-Chavanne, E. Cueto, P. Ladevèze, F. Chinesta, Data-driven non-linear elasticity: constitutive manifold construction and problem discretization, Computational Mechanics 60 (2017) 813-826. T. Kirchdoerfer, M. Ortiz, Data-driven computing in dynamics, International Journal for Numerical Methods","Chunk 489: In: Ibanez, R., Borzacchiello, D., Aguado, J.,"
490,"in Engineering 113 (2018) 1697-1710. R. Ibanez, E. Abisset-Chavanne, J. V. Aguado, D. Gonzalez, E. Cueto, F. Chinesta, A manifold learning approach to data-driven computational elasticity and inelasticity, Archives of Computational Methods in Engineering 25 (2018) 47-57. R. Eggersmann, T. Kirchdoerfer, S. Reese, L. Stainier, M. Ortiz, Model-free data-driven inelasticity,","Chunk 490: A manifold learning approach to data-driven computational elasticity and inelasticity, Archives of Computational Methods in Engineering 25 (2018)"
491,"Computer Methods in Applied Mechanics and Engineering 350 (2019) 81-99. M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, Journal of Computational Physics 378 (2019) 686-707. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, J. M. Siskind, Automatic differentiation in machine learning:","Chunk 491: In: Raissi, M., Perdikaris, P., and Karniadakis, G."
492,"44 a survey, The Journal of Machine Learning Research 18 (2017) 5595-5637. G. A. Maugin, W. Muschik, Thermodynamics with internal variables. Part I. General concepts, 1994. P. M. Mariano, L. Galano, Fundamentals of the Mechanics of Solids, Springer, 2015. L. Anand, O. Aslan, S. A. Chester, A large-deformation gradient theory for elastic-plastic materials: Strain","Chunk 492: In: Maugin, G. A., and Muschik, W."
493,"softening and regularization of shear bands, International Journal of Plasticity 30-31 (2012) 116 - 143. doi.https//doi.or/10.016/jplas.2011.10.002. Y. H. Hu, J.-N. Hwang, Handbook of neural network signal processing, 2002. A. Géron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, O'Reilly Media, 2019.","Chunk 493: Y. H Hu, J.-N. Hwang, Handbook of neural network processing, Handbook of Machine Learning"
494,"M. Bessa, R. Bostanabad, Z. Liu, A. Hu, D. W. Apley, C. Brinson, W. Chen, W. K. Liu, A framework for data-driven analysis of materials under uncertainty: Countering the curse of dimensionality, Computer Methods in Applied Mechanics and Engineering 320 (2017) 633-667. A. Karpatne, W. Watkins, J. Read, V. Kumar, Physics-guided neural networks (pgnn): An application in lake temperature modeling, arXiv preprint arXiv:1710.11431 (2017).",Chunk 494: A framework for data-driven analysis of materials under uncertainty.
495,"H. Ziegler, An introduction to thermomechanics, Elsevier, 2012. P. Bogacki, L. F. Shampine, A 3 (2) pair of Runge-Kutta formulas, Applied Mathematics Letters 2 (1989) 321-325. T. Dozat, Incorporating Nesterov momentum into Adam (2016). 45","Chunk 495: In: Ziegler H., Bogacki P., Shampine L."
