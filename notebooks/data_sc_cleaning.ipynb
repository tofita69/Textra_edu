{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In order to clean and preprocess the data, I created a data cleaning and preprocessing function with the following capabilities: \n",
    "\n",
    "- Lemmatization\n",
    "- Stopword removal\n",
    "- Lowercase\n",
    "- Punctuation cleaning\n",
    "- Emoji cleaning\n",
    "- Number cleaning\n",
    "- Weblinks cleaning\n",
    "- Unnecessary spaces removal\n",
    "\n",
    "I gave the user the freedom to choose which cleaning to apply by creating a unified function where every cleaning step is a boolean. For the purpose of this project, I do not lemmatize, remove stopwords, lowercase, and remove punctuations so that the summarization will still have its semantic context in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yassine/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/yassine/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/yassine/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import string \n",
    "import re \n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual cleaning functions\n",
    "def remove_web_links(text):\n",
    "  text = re.sub(r'http://www.\\w+.org/','', text)\n",
    "  text = re.sub(r'http://www.\\w+.org/','', text)\n",
    "  text = re.sub(r'http://www.([\\w\\S]+).org/\\w+\\W\\w+','',text)\n",
    "  text = re.sub(r'https://www.\\w+.org/','', text)\n",
    "  text = re.sub(r'https://www.([\\w\\S]+).org/\\w+\\W\\w+','',text)\n",
    "  text = re.sub(r'https://\\w+.\\w+/\\d+.\\d+/\\w\\d+\\W\\w+','',text)\n",
    "  text = re.sub(r'https://\\w+.\\w+/\\d+.\\d+/\\w\\d+\\W\\w+','',text)\n",
    "  text = re.sub(r'Figure\\s\\d:','', text)\n",
    "  text = re.sub(r'\\Wwww.\\w+\\W\\w+\\W','',text)\n",
    "  text = re.sub(\"@[A-Za-z0-9]+\", \"\", text)\n",
    "  text = re.sub(r'www.\\w+','',text)\n",
    "\n",
    "  return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "  regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "  text = regrex_pattern.sub('', text)\n",
    "\n",
    "  return text\n",
    "\n",
    "def remove_spaces(text):\n",
    "  text = re.sub(r'\\n',\"\",text)\n",
    "\n",
    "  return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  stop_words = set(stop.words('english'))\n",
    "  words = word_tokenize(text)\n",
    "  sentence = [w for w in words if w not in stop_words]\n",
    "\n",
    "  return \" \".join(sentence)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "  wordlist = []\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  sentences = sent_tokenize(text)\n",
    "  for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    for word in words: \n",
    "      wordlist.append(lemmatizer.lemmatize(word))\n",
    "  return    ' '.join(wordlist)\n",
    "\n",
    "def lowercase_text(text):\n",
    "  return text.lower()\n",
    "\n",
    "def remove_punctuations(text):\n",
    "  additional_punctuations = ['’', '…'] # punctuations not in string.punctuation  \n",
    "  for punctuation in string.punctuation:\n",
    "    text = text.replace(punctuation, '')\n",
    "  \n",
    "  for punctuation in additional_punctuations:\n",
    "    text = text.replace(punctuation, '')\n",
    "    \n",
    "  return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "  if text is not None:\n",
    "    text = text.replace(r'^\\d+\\.\\s+','')\n",
    "  \n",
    "  text = re.sub(\"[0-9]\", '', text)\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified boolean controlled cleaning function \n",
    "def clean_and_preprocess_data(text, lowercase=True, clean_stopwords=True, clean_punctuations=True, clean_links=True, \n",
    "                              clean_emojis=True, clean_spaces=True, clean_numbers=True,  lemmatize=True):\n",
    "  \n",
    "  if clean_stopwords == True:\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "  if clean_punctuations == True:\n",
    "    text = remove_punctuations(text)\n",
    "  \n",
    "  if clean_links == True:\n",
    "    text = remove_web_links(text)\n",
    "  \n",
    "  if clean_emojis == True:\n",
    "    text = remove_emojis(text)\n",
    "  \n",
    "  if clean_spaces == True:\n",
    "    text = remove_spaces(text)\n",
    "  \n",
    "  if clean_numbers == True:\n",
    "    text = remove_numbers(text)\n",
    "  \n",
    "  if lemmatize == True:\n",
    "    text = lemmatize_text(text)\n",
    "  \n",
    "  if lowercase == True:\n",
    "    return text.lower()\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and cleaning the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv(\"top1000.csv\")\n",
    "\n",
    "\n",
    "text_df['abstract'] = text_df['abstract'].fillna(\"\").astype(str)\n",
    "text_df['full_text'] = text_df['full_text'].fillna(\"\").astype(str)\n",
    "text_df['conclusion'] = text_df['conclusion'].fillna(\"\").astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                           abstract  \\\n",
      "0           0  We apply statistical machine translation (SMT)...   \n",
      "1           1  Parallel corpora have become an essential reso...   \n",
      "2           2  The concept of maximum entropy can be traced b...   \n",
      "3           3  We apply the hypothesis of “One Sense Per Disc...   \n",
      "4           4  Transformation-based learning has been success...   \n",
      "\n",
      "                                           full_text  \\\n",
      "0  We apply statistical machine translation (SMT)...   \n",
      "1  Parallel corpora have become an essential reso...   \n",
      "2  The concept of maximum entropy can be traced b...   \n",
      "3  We apply the hypothesis of “One Sense Per Disc...   \n",
      "4  Transformation-based learning has been success...   \n",
      "\n",
      "                                          conclusion  \n",
      "0  We presented a novel approach to the problem o...  \n",
      "1  For each item, participants were instructed to...  \n",
      "2  We began by introducing the building blocks of...  \n",
      "3  The trigger labeling task described in this pa...  \n",
      "4  We have presented in this paper a new and impr...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_df['abstract'] = text_df['abstract'].apply(lambda x: clean_and_preprocess_data(x, lemmatize=False, clean_numbers=False, clean_stopwords=False, clean_punctuations=False, lowercase=False))\n",
    "text_df['full_text'] = text_df['full_text'].apply(lambda x: clean_and_preprocess_data(x, lemmatize=False, clean_numbers=False, clean_stopwords=False, clean_punctuations=False, lowercase=False))\n",
    "text_df['conclusion'] = text_df['conclusion'].apply(lambda x: clean_and_preprocess_data(x, lemmatize=False, clean_numbers=False, clean_stopwords=False, clean_punctuations=False, lowercase=False))\n",
    "\n",
    "print(text_df.head())\n",
    "\n",
    "# Saving the processed data into a .csv file \n",
    "\n",
    "text_df.to_csv(\"top1000_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
