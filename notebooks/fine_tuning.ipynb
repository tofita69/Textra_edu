{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/yassine/miniconda3/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: datasets in /home/yassine/miniconda3/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: pandas in /home/yassine/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: torch in /home/yassine/miniconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /home/yassine/miniconda3/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yassine/miniconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yassine/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yassine/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/yassine/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/yassine/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/yassine/miniconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yassine/miniconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/yassine/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/yassine/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/yassine/miniconda3/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yassine/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yassine/miniconda3/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yassine/miniconda3/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/yassine/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yassine/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yassine/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yassine/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/yassine/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yassine/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yassine/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yassine/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yassine/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/yassine/miniconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets pandas torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:50:52.142069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736283052.287816   32255 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736283052.320066   32255 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-07 21:50:52.545552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare the Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 1009 rows.\n",
      "Filtered dataset has 944 rows after removing NaNs.\n",
      "Filtered dataset has 944 rows after removing long summaries.\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset from Hugging Face library\n",
    "# This allows us to apply tokenization and other operations more effectively\n",
    "#  within the Hugging Face framework\n",
    "from datasets import Dataset  \n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. Load your dataset\n",
    "    df = pd.read_csv(\"/home/yassine/Textra-edu/outputs/extractive_summarized_dataframe_final.csv\")  \n",
    "    print(f\"Dataset loaded successfully with {len(df)} rows.\")\n",
    "\n",
    "    # Filter and drop NaN values\n",
    "    df = df[[\"full_text\", \"extractive_summarized_text\"]].dropna()  # Remove NaN values to avoid errors.\n",
    "    print(f\"Filtered dataset has {len(df)} rows after removing NaNs.\")\n",
    "\n",
    "    # Filter out overly long summaries\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")  # Initialize tokenizer\n",
    "    df['summary_token_length'] = df['extractive_summarized_text'].apply(\n",
    "        lambda x: len(tokenizer(x)[\"input_ids\"])\n",
    "    )\n",
    "    df_filtered = df[df['summary_token_length'] <= 1024].drop(columns=['summary_token_length'])\n",
    "    print(f\"Filtered dataset has {len(df_filtered)} rows after removing long summaries.\")\n",
    "\n",
    "    if len(df_filtered) == 0:\n",
    "        raise ValueError(\"No valid summaries left after filtering. Check your data.\")\n",
    "\n",
    "    # Convert the filtered DataFrame to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(df_filtered)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File 'extractive_summarized_dataframe_final.csv' not found. Check the file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenize_function processes the dataset by converting text into numerical representations (tokens) that the model can understand. Here's what it does step by step:\n",
    "\n",
    "1. Tokenize Input Text (full_text):\n",
    "Converts the input text (examples[\"full_text\"]) into tokens using the model's tokenizer. It truncates or pads the text to a fixed length (max_length=1024) to ensure all inputs are uniform in size.\n",
    "\n",
    "2. Tokenize Target Text (extractive_summarized_text):\n",
    "Similarly, converts the summary (examples[\"extractive_summarized_text\"]) into tokens. It truncates or pads to a shorter fixed length (max_length=128), since summaries are typically much shorter than input text.\n",
    "\n",
    "3. Create Model Inputs:\n",
    "\n",
    "The tokenized full_text is saved as model_inputs.\n",
    "The tokenized extractive_summarized_text is added as labels to model_inputs. These labels will guide the model during fine-tuning by showing the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e283a661e61e457f801163e1e1cabc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"full_text\"], max_length=1024, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"extractive_summarized_text\"],\n",
    "        max_length=128,  # Keep summaries concise\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Output:\n",
    "The function returns a dictionary for each data point containing:\n",
    "\n",
    "    . Input tokens (model_inputs): The tokenized full text.\n",
    "    . Labels: The tokenized summary that the model should generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model and Training Args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yassine/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bart-distilled-from-scibert\",  # Directory to save the model\n",
    "    per_device_train_batch_size=2,  # Adjust based on memory availability\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients to simulate larger batch size\n",
    "    num_train_epochs=2,  # Number of epochs to train\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",  # Save model after each epoch\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision (FP16) if GPU is available\n",
    "    push_to_hub=False,\n",
    "    logging_steps=50,  # Log progress every 50 steps\n",
    "    evaluation_strategy=\"no\"  # No evaluation during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  # Your fine-tuned BART model\n",
    "    args=training_args,  # TrainingArguments object\n",
    "    train_dataset=tokenized_datasets,  # Tokenized dataset to train on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='118' max='118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [118/118 3:13:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>19.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>15.717900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yassine/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"outputs/bart-fine-tuned\")  # Save the model after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"outputs/bart-fine-tuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart-distilled-from-scibert  bart-fine-tuned\n"
     ]
    }
   ],
   "source": [
    "!ls TunedModels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  bart-distilled-from-scibert  bart-fine-tuned\n"
     ]
    }
   ],
   "source": [
    "!ls -a outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the fine-tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  generation_config.json  model.safetensors\ttraining_args.bin\n"
     ]
    }
   ],
   "source": [
    "!ls TunedModels/bart-fine-tuned/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model and the tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "# Re-initialize the tokenizer from the original pre-trained model\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Save the tokenizer to your fine-tuned model directory\n",
    "tokenizer.save_pretrained(\"TunedModels/bart-fine-tuned\")\n",
    "\n",
    "print(\"Tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model_dir = \"TunedModels/bart-fine-tuned\"\n",
    "\n",
    "# Load the fine-tuned model \n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "# Load the tokenizer \n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, tokenizer, text):\n",
    "    \"\"\"\n",
    "    Generate a summary for the given input text using the fine-tuned BART model.\n",
    "    \n",
    "    Args:\n",
    "    - model: The fine-tuned BART model.\n",
    "    - tokenizer: The tokenizer corresponding to the BART model.\n",
    "    - text: The input text to summarize.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The generated summary.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=1024,  # Ensure it matches the model's input size\n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    # Generate the summary\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=300,  # Maximum length of the generated summary\n",
    "        num_beams=5,     # Use beam search for diversity\n",
    "        early_stopping=True,  # Stop generation upon reaching end token\n",
    "        repetition_penalty=5.0,  # Penalize repetitions\n",
    "        no_repeat_ngram_size=3,  # Prevent repeating n-grams\n",
    "        temperature=2.0,  # Sampling temperature for randomness\n",
    "        top_k=50,  # Limit to top-k tokens\n",
    "        top_p=0.9  # Nucleus sampling for diversity\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated summary\n",
    "    summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yassine/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `2.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      " \n",
      "   Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512.\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position i can depend only on the known outputs at positions less than i.\n",
      "    \n",
      "\n",
      "Generated Summary:\n",
      " centric   Encoder: The encoder is composed of a stack of N = 6 identical layers. We employ a residual connection [11] around each of                the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is produced as follows: where Sublayer(x) is the function implemented by the self-attention mechanism and dmodel = 512.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input text for testing\n",
    "    input_text = \"\"\"\n",
    "   Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
    "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
    "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
    "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
    "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
    "layers, produce outputs of dimension dmodel = 512.\n",
    "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
    "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
    "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
    "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
    "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
    "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
    "predictions for position i can depend only on the known outputs at positions less than i.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate and print the summary\n",
    "    summary = generate_summary(model, tokenizer, input_text)\n",
    "    print(\"Input Text:\\n\", input_text)\n",
    "    print(\"\\nGenerated Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
